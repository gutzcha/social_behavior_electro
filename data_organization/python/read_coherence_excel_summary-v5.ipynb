{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2cd4625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af63149",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477dacf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Project- Electro\\\\social_behavior_electro\\\\data_organization\\\\python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b69cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = osp.join('..','..','analysis')\n",
    "path_to_aversive_enc_pre = osp.join(folder_path,\n",
    "                            'Population analysis results for LFP Coherence - Encounter-PreEncounter_4-12_30-80_aversive.xlsx')\n",
    "path_to_affiliative_enc_pre = osp.join(folder_path,\n",
    "                               'Population analysis results for LFP Coherence - Encounter-PreEncounter_4-12_30-80_affiliative.xlsx')\n",
    "\n",
    "path_to_aversive_post_pre = osp.join(folder_path,\n",
    "                            'Population analysis results for LFP Coherence - PostEncounter-PreEncounter_4-12_30-80_aversive.xlsx')\n",
    "path_to_affiliative_post_pre = osp.join(folder_path,\n",
    "                               'Population analysis results for LFP Coherence - PostEncounter-PreEncounter_4-12_30-80_affiliative.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "path_to_affiliative_lfp = osp.join(folder_path,\n",
    "                            'lfp_rawdata_affiliative.xlsx') \n",
    "\n",
    "path_to_aversive_lfp = osp.join(folder_path,\n",
    "                            'lfp_rawdata_aversive.xlsx')\n",
    "\n",
    "path_to_affiliative_spikes = osp.join(folder_path,\n",
    "                            'spikes_rawdata_affiliative.xlsx') \n",
    "\n",
    "path_to_aversive_spikes = osp.join(folder_path,\n",
    "                            'spikes_rawdata_aversive.xlsx')\n",
    "\n",
    "\n",
    "path_to_common_areas = osp.join(folder_path, 'common_area_pairs_w5_sessions.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d60e186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_N_SESSIONS = 5\n",
    "METRIC = 'kappa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b00eb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aff_lfp_raw = pd.read_excel(path_to_affiliative_lfp ,header=[0,1,2], index_col=[0])\n",
    "df_avv_lfp_raw = pd.read_excel(path_to_aversive_lfp ,header=[0,1,2], index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb9d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aff_spikes_raw = pd.read_excel(path_to_affiliative_spikes ,header=[0,1], index_col=[0])\n",
    "df_avv_spikes_raw = pd.read_excel(path_to_aversive_spikes ,header=[0,1], index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d9c94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_aff_spikes_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a4fabc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aff_enc_pre = pd.read_excel(path_to_affiliative_enc_pre,None)\n",
    "df_avv_enc_pre = pd.read_excel(path_to_aversive_enc_pre,None)\n",
    "\n",
    "df_aff_post_pre = pd.read_excel(path_to_affiliative_post_pre,None)\n",
    "df_avv_post_pre = pd.read_excel(path_to_aversive_post_pre,None)\n",
    "\n",
    "\n",
    "df_files_avv_bad = pd.read_excel(path_to_aversive_enc_pre,'Uploaded files', header=None)\n",
    "df_files_aff_bad = pd.read_excel(path_to_affiliative_enc_pre,'Uploaded files', header=None)\n",
    "\n",
    "df_areas = pd.read_excel(path_to_common_areas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba353773",
   "metadata": {},
   "source": [
    "## Clean and combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc6e2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def fix_file_paths(df_files):\n",
    "    \n",
    "    df_files = df_files.rename(columns={df_files.columns[1]:'timestamps', df_files.columns[0]:'lfp', })\n",
    "    \n",
    "    return df_files\n",
    "df_files_avv = fix_file_paths(df_files_avv_bad)['lfp']\n",
    "df_files_aff = fix_file_paths(df_files_aff_bad)['lfp']\n",
    "\n",
    "\n",
    "def extract_ratnum_from_file_name(filename):\n",
    "    filename = filename.replace(' ','')\n",
    "    match = re.search(r\"rat(\\d{1,2})\", filename.lower())\n",
    "\n",
    "    if match:\n",
    "        number = int(match.group(1))       \n",
    "    else:\n",
    "        number = -1\n",
    "    return number\n",
    "\n",
    "def extract_daynum_from_file_name(filename):\n",
    "    filename = filename.replace(' ','')\n",
    "    match = re.search(r\"day(\\d{1,2})\", filename.lower())\n",
    "    if match:\n",
    "        number = int(match.group(1))       \n",
    "    else:\n",
    "        number = -1\n",
    "    return number\n",
    "# rat_numbers = [extract_ratnum_from_file_name(filename) for filename in filenames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "616a1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_num_to_name_map = {\n",
    "    '111': 'MeD',\n",
    "    '2': 'MePV',\n",
    "    '13':'CeA',\n",
    "    '112': 'BMA',\n",
    "    '14': 'AA',\n",
    "    '16': 'EA',\n",
    "    '12': 'STIA',\n",
    "    '15': 'VP'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def reformat_columns(df):\n",
    "    # combine column names and remove levels\n",
    "    df_new = pd.DataFrame()\n",
    "    areas = df.columns.get_level_values('area').unique()\n",
    "    freqs = df.columns.get_level_values('freq').unique()\n",
    "    stages = df.columns.get_level_values('stage').unique()\n",
    "\n",
    "    df_new.index = df.index\n",
    "    for area in areas:\n",
    "        for freq in freqs:\n",
    "            for stage in stages:\n",
    "\n",
    "                col_name = (area, freq, stage)\n",
    "                \n",
    "                area_name = area_num_to_name_map[area]\n",
    "                if \"diffduring\" in stage.lower():\n",
    "                    new_stage = 'enc_pre_lfp'\n",
    "                elif\"diffafter\" in stage.lower():\n",
    "                    new_stage = 'post_pre_lfp'\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                new_freq = freq.replace('-','_') + 'Hz'\n",
    "                \n",
    "                \n",
    "                new_col_name = '_'.join((area_name, new_freq, new_stage))\n",
    "\n",
    "                df_new[new_col_name] = df[col_name]\n",
    "    return df_new\n",
    "\n",
    "\n",
    "SPIKE_DATA_TYPE_SRC = 'diff'\n",
    "SPIKE_DATA_TYPE_TARGET = 'spike_diff'\n",
    "\n",
    "# SPIKE_DATA_TYPE_SRC = 'ratio'\n",
    "# SPIKE_DATA_TYPE_TARGET = 'spike_ratio'\n",
    "\n",
    "# SPIKE_DATA_TYPE_SRC = 'spikes_'\n",
    "# SPIKE_DATA_TYPE_TARGET = 'spike_raw'\n",
    "\n",
    "def reformat_columns_spikes(df):\n",
    "    # combine column names and remove levels\n",
    "    df_new = pd.DataFrame()\n",
    "    areas = df.columns.get_level_values('area').unique()\n",
    "    stages = df.columns.get_level_values('stage').unique()\n",
    "\n",
    "    df_new.index = df.index\n",
    "    add_pre_sufix = 'ratio' in SPIKE_DATA_TYPE_TARGET or 'diff' in SPIKE_DATA_TYPE_TARGET\n",
    "    suffix = 'pre_' if add_pre_sufix else ''\n",
    "    for area in areas:\n",
    "        for stage in stages:\n",
    "\n",
    "            col_name = (area, stage)\n",
    "\n",
    "            area_name = area_num_to_name_map[area]\n",
    "            if SPIKE_DATA_TYPE_SRC+\"during\" in stage.lower():\n",
    "                new_stage = 'enc_'+suffix+ SPIKE_DATA_TYPE_TARGET\n",
    "            elif SPIKE_DATA_TYPE_SRC+\"after\" in stage.lower():\n",
    "                new_stage = 'post_'+suffix+ SPIKE_DATA_TYPE_TARGET\n",
    "            elif SPIKE_DATA_TYPE_SRC+\"before\" in stage.lower():\n",
    "                new_stage = 'pre_'+ SPIKE_DATA_TYPE_TARGET\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            \n",
    "\n",
    "            new_col_name = '_'.join((area_name, new_stage))\n",
    "\n",
    "            df_new[new_col_name] = df[col_name]\n",
    "    return df_new\n",
    "\n",
    "def reformat_columns_spikes_v2(df):\n",
    "    # combine column names and remove levels\n",
    "    df_new = pd.DataFrame()\n",
    "    areas = df.columns.get_level_values('area').unique()\n",
    "    stages = df.columns.get_level_values('stage').unique()\n",
    "\n",
    "    df_new.index = df.index\n",
    "    add_pre_sufix = 'ratio' in SPIKE_DATA_TYPE_TARGET or 'diff' in SPIKE_DATA_TYPE_TARGET\n",
    "    suffix = 'enc' if add_pre_sufix else ''\n",
    "    for area in areas:\n",
    "        for stage in stages:\n",
    "\n",
    "            col_name = (area, stage)\n",
    "\n",
    "            area_name = area_num_to_name_map[area]\n",
    "            if SPIKE_DATA_TYPE_SRC+\"during\" in stage.lower():\n",
    "                during_data = df[col_name].dropna().values\n",
    "            elif SPIKE_DATA_TYPE_SRC+\"after\" in stage.lower():\n",
    "                after_data = df[col_name].dropna().values\n",
    "            elif SPIKE_DATA_TYPE_SRC+\"before\" in stage.lower():\n",
    "                before_data = df[col_name].dropna().values\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            \n",
    "        # create new column with calculation\n",
    "        log_during_change = np.log(during_data) - np.log(before_data)\n",
    "        \n",
    "        \n",
    "        new_stage = 'enc_pre_log_'+ SPIKE_DATA_TYPE_TARGET\n",
    "        new_col_name = '_'.join((area_name, new_stage))\n",
    "        df_new[new_col_name] = df[col_name]\n",
    "        \n",
    "        log_after_change = np.log(after_data) - np.log(before_data)\n",
    "        new_stage = 'post_pre_log'+ SPIKE_DATA_TYPE_TARGET\n",
    "        df_new[new_col_name] = df[col_name]\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff2369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6a38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4ff9154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_avv_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6f02ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aff_lfp = reformat_columns(df_aff_lfp_raw)\n",
    "df_avv_lfp = reformat_columns(df_avv_lfp_raw)\n",
    "df_aff_spikes = reformat_columns_spikes(df_aff_spikes_raw)\n",
    "df_avv_spikes = reformat_columns_spikes(df_avv_spikes_raw)\n",
    "# df_aff_spikes = reformat_columns_spikes_v2(df_aff_spikes_raw)\n",
    "# df_avv_spikes = reformat_columns_spikes_v2(df_avv_spikes_raw)\n",
    "\n",
    "# Add variables befor concatination\n",
    "df_aff_enc_pre = pd.read_excel(path_to_affiliative_enc_pre,None)\n",
    "# df_aff_enc_pre['sociability'] = 'affiliative'\n",
    "# df_aff_enc_pre['stage'] = 'enc_pre'\n",
    "\n",
    "df_aff_post_pre = pd.read_excel(path_to_affiliative_post_pre,None)\n",
    "# df_aff_post_pre['sociability'] = 'affiliative'\n",
    "# df_aff_post_pre['stage'] = 'post_pre'\n",
    "\n",
    "\n",
    "df_avv_enc_pre = pd.read_excel(path_to_aversive_enc_pre,None)\n",
    "# df_avv_enc_pre['sociability'] = 'aversive'\n",
    "# df_avv_enc_pre['stage'] = 'enc_pre'\n",
    "\n",
    "\n",
    "df_avv_post_pre = pd.read_excel(path_to_aversive_post_pre,None)\n",
    "# df_avv_post_pre['sociability'] = 'aversive'\n",
    "# df_avv_post_pre['stage'] = 'post_pre'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6936c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_keywords = {'First': '4_18Hz', 'Second':'30_80Hz'}\n",
    "stage_keywords = {'During':'enc_pre', 'After':'post_pre'}\n",
    "substrings_to_remove = ['During', 'After','Before', 'First', 'Second']\n",
    "ignore_keyworks = ['Norm', 'files']\n",
    "\n",
    "def remove_columns_with_fewer_values(df, N=5):\n",
    "    # Get the count of non-null values in each column\n",
    "    column_counts = df.count()\n",
    "\n",
    "    # Filter columns based on count condition\n",
    "    columns_to_remove = column_counts[column_counts < N].index\n",
    "\n",
    "    # Drop the columns from the DataFrame\n",
    "    updated_df = df.drop(columns=columns_to_remove)\n",
    "\n",
    "    return updated_df\n",
    "\n",
    "def reformat_dict_to_table(df_dict, file_df):\n",
    "# def reformat_dict_to_table(df_dict, freq_keywords, stage_keywords, ignore_keyworks, substrings_to_remove):\n",
    "    ret_df_list = []\n",
    "    for sheet, df in df_dict.items():\n",
    "        df = df.copy()\n",
    "        # ignore first and last sheets that contain a summary and list of file names\n",
    "        if any(substring in sheet for substring in ignore_keyworks): \n",
    "            continue\n",
    "            \n",
    "        if isinstance(df, str):\n",
    "            print(df)\n",
    "            continue\n",
    "            \n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "        \n",
    "         # Remove all substrings to keep just the area name\n",
    "        area_name = sheet\n",
    "        for sub_string_to_remove in substrings_to_remove:\n",
    "            area_name = area_name.replace(sub_string_to_remove,'')\n",
    "        \n",
    "        for fk, freq in  freq_keywords.items():\n",
    "            if fk in sheet:\n",
    "                this_freq = freq_keywords[fk]\n",
    "            for sk in stage_keywords.keys():\n",
    "                if sk in sheet:\n",
    "                    this_stage = stage_keywords[sk]\n",
    "        \n",
    "                # Rename all column names to match\n",
    "        \n",
    "        for col in df.columns:\n",
    "            area_pair = [col, area_name]\n",
    "            area_pair.sort()\n",
    "            \n",
    "            df.rename(columns={col:f'{area_pair[0]}_{area_pair[1]}_{this_freq}_{this_stage}'}, inplace=True)\n",
    "        \n",
    "        \n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        df = remove_columns_with_fewer_values(df)\n",
    "        ret_df_list.append(df)\n",
    "        \n",
    "    df_ret = pd.concat(ret_df_list,axis=1)\n",
    "    df_ret['files'] = file_df.values\n",
    "    return df_ret\n",
    "                \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25ef5517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\3964125948.py:30: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  df_aff = df_aff[common_cols]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\3964125948.py:31: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  df_avv = df_avv[common_cols]\n"
     ]
    }
   ],
   "source": [
    "df_aff_enc_pre_rectified = reformat_dict_to_table(df_aff_enc_pre, df_files_aff)\n",
    "df_aff_enc_post_rectified = reformat_dict_to_table(df_aff_post_pre, df_files_aff)\n",
    "df_aff = pd.concat([\n",
    "    df_aff_enc_pre_rectified.set_index('files'),\n",
    "    df_aff_enc_post_rectified.set_index('files'),\n",
    "    df_aff_lfp,\n",
    "    df_aff_spikes\n",
    "],\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "df_aff['sociability'] = 'affiliative'\n",
    "\n",
    "\n",
    "df_avv_enc_pre_rectified = reformat_dict_to_table(df_avv_enc_pre, df_files_avv)\n",
    "df_avv_enc_post_rectified = reformat_dict_to_table(df_avv_post_pre, df_files_avv)\n",
    "df_avv = pd.concat([\n",
    "    df_avv_enc_pre_rectified.set_index('files'),\n",
    "    df_avv_enc_post_rectified.set_index('files'),\n",
    "    df_avv_lfp,\n",
    "    df_avv_spikes\n",
    "], axis=1)\n",
    "\n",
    "df_avv['sociability'] = 'aversive'\n",
    "\n",
    "\n",
    "df_avv_cols = df_avv.columns\n",
    "df_aff_cols = df_aff.columns\n",
    "common_cols = set(df_aff_cols).intersection(set(df_avv_cols))\n",
    "df_aff = df_aff[common_cols]\n",
    "df_avv = df_avv[common_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed7c68f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix column names\n",
    "# change 4-12 to 4-18\n",
    "def fix_range_name(s):\n",
    "    return s.replace('4_18','4_12')\n",
    "# if lfp is in the name, put it at the beggining\n",
    "# if spike_ratio is in the name, put it at the beginning\n",
    "def move_suffix_to_apx(s, sfx):\n",
    "    if sfx in s:\n",
    "        s = s.replace('_'+sfx, '')\n",
    "        s = sfx+'_'+s\n",
    "    return s\n",
    "\n",
    "# if there is not lfp or skipe ration in the name, add coherence at the befinning\n",
    "def add_cohherence(s):\n",
    "    if 'lfp' not in s and 'spike' not in s and ('enc' in s or 'post' in s):\n",
    "        s = 'coherence_' + s\n",
    "    return s\n",
    "\n",
    "# add number to catergorical variables to ensure that they are first\n",
    "def add_number(s):\n",
    "    if 'sociability' in s:\n",
    "        s = '0_' + s\n",
    "        \n",
    "    elif 'rat_number' in s:\n",
    "        s = '1_' + s\n",
    "        \n",
    "    elif 'day_number' in s:\n",
    "        s = '2_' + s\n",
    "    return s\n",
    "\n",
    "def add_all_fix_functions(s):\n",
    "    s = fix_range_name(s)\n",
    "    s = move_suffix_to_apx(s, 'lfp')\n",
    "    s = move_suffix_to_apx(s, SPIKE_DATA_TYPE_TARGET)\n",
    "    s = add_cohherence(s)\n",
    "#     s = add_number(s)\n",
    "    return s\n",
    "\n",
    "all_df = pd.concat([df_aff, df_avv])\n",
    "filenames = all_df.index.to_list()\n",
    "all_df['rat_number'] = pd.Categorical([extract_ratnum_from_file_name(r) for r in filenames])\n",
    "all_df['day_number'] = pd.Categorical([extract_daynum_from_file_name(r) for r in filenames])\n",
    "\n",
    "\n",
    "all_df.rename(columns={c:add_all_fix_functions(c) for c in all_df.columns} , inplace=True)\n",
    "all_df = all_df.reindex(sorted(all_df.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48fa6315",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars  = ['sociability','rat_number','day_number']\n",
    "\n",
    "df_spike_data = all_df[[c for c in all_df.columns if 'spike' in c or c in cat_vars]]\n",
    "df_lfp_data = all_df[[c for c in all_df.columns if 'lfp' in c or c in cat_vars]]\n",
    "df_coherence_data = all_df[[c for c in all_df.columns if 'coherence' in c or c in cat_vars]]\n",
    "df_coherence_lfp = all_df[[c for c in all_df.columns if 'spike' not in c]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b84959",
   "metadata": {},
   "source": [
    "## Preliminary feature selection via statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aedcd099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu, ttest_ind\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, StratifiedGroupKFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# ############################\n",
    "# import sklearn.neighbors._base\n",
    "# import sys\n",
    "# sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "# from missingpy import MissForest\n",
    "# ############################\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.special import logit\n",
    "\n",
    "\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sweetviz as sv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4362c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_variance(dataframe):\n",
    "    column_names = dataframe.columns\n",
    "    result = pd.DataFrame(columns=['Column', 'Mean', 'Variance'])\n",
    "\n",
    "    for column in column_names:\n",
    "        mean_value = dataframe[column].mean()\n",
    "        variance_value = dataframe[column].var()\n",
    "        result = result.append({'Column': column, 'Mean': mean_value, 'Variance': variance_value}, ignore_index=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "# result_df = calculate_mean_variance(df_spike_data[[c for c in df_spike_data if c not in cat_vars]])\n",
    "# print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1422da86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_correlation_heat_map(df):\n",
    "    corr_data = df.corr()\n",
    "    a4_dims = (11.7, 8.27)\n",
    "    fig, ax = plt.subplots(figsize=a4_dims)\n",
    "    g = sns.heatmap(corr_data, ax=ax)\n",
    "    return\n",
    "\n",
    "def mark_nans_with_x(correlation_matrix, ax):\n",
    "#     correlation_matrix = correlation_matrix.drop(cat_vars, axis = 1)\n",
    "    # Step 3: Extract variables with all NaN values\n",
    "    nan_variables = correlation_matrix.columns[correlation_matrix.isnull().all()]\n",
    "\n",
    "    # Step 4: Mark relevant squares with NaN values in the heat map\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(len(correlation_matrix.columns)):\n",
    "            if correlation_matrix.iloc[i, j] != 1.0 and pd.isnull(correlation_matrix.iloc[i, j]):\n",
    "                ax.text(j + 0.5, i + 0.5, 'X', ha='center', va='center', color='red', fontsize=16)\n",
    "    return\n",
    "    \n",
    "\n",
    "def plot_correlation_heat_map_sidebyside(df):\n",
    "    # Convert numeric columns to float if needed\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].astype(float)\n",
    "\n",
    "    # Step 1: Split the dataframe based on the value of \"sociability\"\n",
    "    df_low_sociability = df[df[\"sociability\"] == \"affiliative\"]\n",
    "    df_high_sociability = df[df[\"sociability\"] == \"aversive\"]\n",
    "\n",
    "    # Step 2: Extract correlation matrices\n",
    "    corr_low_sociability = df_low_sociability.corr()\n",
    "    corr_high_sociability = df_high_sociability.corr()\n",
    "\n",
    "    # Step 3: Create heat map plot side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    sns.heatmap(corr_low_sociability, annot=False, cmap='coolwarm', vmin=-1, vmax=1, ax=ax1)\n",
    "    sns.heatmap(corr_high_sociability, annot=False, cmap='coolwarm', vmin=-1, vmax=1, ax=ax2)\n",
    "\n",
    "    # Step 4: Mark relevant squares with NaN values in the heat maps\n",
    "    for ax, corr_matrix in zip([ax1, ax2], [corr_low_sociability, corr_high_sociability]):\n",
    "        nan_matrix = corr_matrix.isnull().astype(int)\n",
    "        for i in range(len(corr_matrix.index)):\n",
    "            for j in range(len(corr_matrix.columns)):\n",
    "                if nan_matrix.iloc[i, j] == 1:\n",
    "                    ax.text(j + 0.5, i + 0.5, 'X', ha='center', va='center', color='red', fontsize=16)\n",
    "\n",
    "    # Step 5: Extract and mark cells with significant difference\n",
    "    for i in range(len(corr_low_sociability.index)):\n",
    "        for j in range(len(corr_low_sociability.columns)):\n",
    "            if i != j:\n",
    "                \n",
    "                low_col = pd.to_numeric(df_low_sociability.iloc[:, i], errors='coerce')\n",
    "                high_col = pd.to_numeric(df_high_sociability.iloc[:, j], errors='coerce')\n",
    "                \n",
    "                len_high= np.sum(~np.isnan(high_col.values))\n",
    "                len_low= np.sum(~np.isnan(low_col.values))\n",
    "                \n",
    "                if len_high<MIN_N_SESSIONS or len_low<MIN_N_SESSIONS:\n",
    "                    p_value = 1\n",
    "#                     print(f'len_high: {len_high}, len_high: {len_low}')\n",
    "                else:\n",
    "#                     _, p_value = mannwhitneyu(low_col, high_col, alternative='two-sided')\n",
    "                    p_value = ttest_ind(low_col, high_col, nan_policy='omit').pvalue\n",
    "                if p_value < 0.05:\n",
    "                    ax1.text(j + 0.5, i + 0.5, 'O', ha='center', va='center', color='blue', fontsize=16)\n",
    "                    ax2.text(j + 0.5, i + 0.5, 'O', ha='center', va='center', color='blue', fontsize=16)\n",
    "#                 print(f'i,j:({i},{j})')\n",
    "    \n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.7, hspace=None)\n",
    "    \n",
    "\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "658fd124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_correlation_heat_map_sidebyside(df_spike_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a568e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09fcfc30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #EDA using Autoviz\n",
    "# df_vis = df_coherence_data\n",
    "# df_vis = df_vis[[c for c in df_vis.columns if c not in cat_vars]]\n",
    "\n",
    "# df_vis = df_vis.apply(pd.to_numeric)\n",
    "# df_vis['sociability'] = df_coherence_data['sociability']\n",
    "\n",
    "# # feature_config = sv.FeatureConfig(force_num=[c for c in df_vis.columns if c not in cat_vars])\n",
    "# sweet_report = sv.compare_intra(\n",
    "#     df_vis,df_vis['sociability']=='affiliative',\n",
    "#     ['affiliative', 'aversive'],\n",
    "# #     feat_cfg=feature_config\n",
    "# )\n",
    "\n",
    "# #Saving results to HTML file\n",
    "# sweet_report.show_html('df_coherence_data.html')\n",
    "# # sweet_report.show_notebook(filepath='sweet_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311da094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d613b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def transform_var(func_handle, X_train,X_test, method='reuse', func_parameters=None):\n",
    "    if func_parameters is None:\n",
    "        func_train = func_handle()\n",
    "    else:\n",
    "        func_train = func_handle(**func_parameters)\n",
    "            \n",
    "    X_train_trans = func_train.fit_transform(X_train)\n",
    "    \n",
    "    if method == 'reuse':\n",
    "        X_test_trans = func_train.transform(X_test)\n",
    "        func_test = func_train\n",
    "    # create new instance and refit on test data\n",
    "    elif method == 'new':\n",
    "        if func_parameters is None:\n",
    "            func_test = func_handle()\n",
    "        else:\n",
    "            func_test = func_handle(**func_parameters)\n",
    "        X_test_trans = func_test.fit_transform(X_test)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input value. The value must be either 'reuse' or 'new'.\")\n",
    "    return X_train_trans, X_test_trans, func_train, func_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "18344cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def eval_model(y_test, y_test_pred, y_prob=None):\n",
    "#     print(f'y:{y_test}')\n",
    "#     print(f'y_pred:{y_test_pred}')\n",
    "    # Calculating evaluation metrics on the testing set\n",
    "    ret_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    ret_precision = precision_score(y_test, y_test_pred, pos_label=\"affiliative\")\n",
    "    ret_recall = recall_score(y_test, y_test_pred,pos_label=\"affiliative\")\n",
    "    ret_f1 = f1_score(y_test, y_test_pred,pos_label=\"affiliative\")\n",
    "    \n",
    "    kappa = cohen_kappa_score(y_test, y_test_pred)\n",
    "#     print(y_test)\n",
    "#     print(y_test=='affiliative')\n",
    "    if y_prob is not None:\n",
    "        roc_auc = roc_auc_score(y_test=='affiliative', y_prob)\n",
    "    ret = {\n",
    "        'accuracy':np.round(ret_accuracy, 3),\n",
    "        'precision':np.round(ret_precision, 3),\n",
    "        'recall':np.round(ret_recall, 3),\n",
    "        'f1':np.round(ret_f1, 3),\n",
    "        'kappa': np.round(kappa, 3),\n",
    "        'roc_auc':roc_auc,\n",
    "    }\n",
    "    \n",
    "\n",
    "    return ret\n",
    "\n",
    "def sum_model_results(y, y_pred, \n",
    "                      confidence_levels, affiliative,\n",
    "                      filenames,\n",
    "                      dataset):\n",
    "    df_results = pd.DataFrame()\n",
    "    df_results['GT'] = y\n",
    "    df_results['predicted'] = y_pred\n",
    "    df_results['correct'] = df_results['GT'] == df_results['predicted']\n",
    "    df_results['confidence'] = confidence_levels\n",
    "    df_results['affiliative_level'] = affiliative\n",
    "    df_results['filenames'] = filenames\n",
    "\n",
    "    return df_results\n",
    "\n",
    "def sum_all_results(y_train, y_train_pred, y_test, y_test_pred, **kwargs):\n",
    "    \n",
    "    ret_train = eval_model(y_train,y_train_pred)\n",
    "    ret_test = eval_model(y_test,y_test_pred)\n",
    "    df_res_all = pd.DataFrame.from_dict([ret_test, ret_train])\n",
    "    df_res_all.index = ['test','train']\n",
    "    \n",
    "    for param, vals in kwargs.items():\n",
    "        try:\n",
    "            df_res_all[param] = [vals]*2\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "    return df_res_all\n",
    "\n",
    "\n",
    "def train_eval_model(config_dict,\n",
    "                     X_train, y_train, X_test, y_test,\n",
    "                     train_file_names, test_file_names,\n",
    "                     method_name, imputer_name, scaler_name, classifier_name,\n",
    "                     k_fold, ind, iteration):\n",
    "    \n",
    "#     print('I am here')\n",
    "    scaler = config_dict['scaler'][scaler_name]\n",
    "    imputer_class = config_dict['imputer'][imputer_name]\n",
    "    \n",
    "    if ('imputer_param' in config_dict) and (imputer_name in config_dict['imputer_param']):\n",
    "        imputer_param = config_dict['imputer_param'][imputer_name]\n",
    "    else:\n",
    "        imputer_param = None\n",
    "    \n",
    "    classifier_class = config_dict['model'][classifier_name]\n",
    "\n",
    "    \n",
    "    # impute\n",
    "    \n",
    "    X_train_imputed, X_test_imputed, imputer_train, imputer_test = transform_var(\n",
    "        imputer_class, X_train, X_test, method_name, imputer_param);\n",
    "\n",
    "    # ==================== impute each class seperatly =================\n",
    "#     X_train_aff = X_train[y_train=='affiliative']\n",
    "#     X_test_aff = X_test[y_test=='affiliative']\n",
    "#     X_train_avv = X_train[y_train=='aversive']\n",
    "#     X_test_avv = X_test[y_test=='aversive']\n",
    "    \n",
    "#     print(f'size of X_train_aff :{X_train_aff.shape}')\n",
    "#     print(f'size of X_test_aff :{X_test_aff.shape}')\n",
    "#     print(f'size of X_train_avv :{X_train_avv.shape}')\n",
    "#     print(f'size of X_test_avv :{X_test_avv.shape}')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     X_train_aff_imputed, X_test_aff_imputed, imputer_train, imputer_test = transform_var(\n",
    "#         imputer_class, X_train_aff, X_test_aff, method_name, imputer_param);\n",
    "    \n",
    "#     X_train_avv_imputed, X_test_avv_imputed, imputer_train, imputer_test = transform_var(\n",
    "#         imputer_class, X_train_avv, X_test_avv, method_name, imputer_param);\n",
    "    \n",
    "#     X_train_imputed = np.concatenate((X_train_aff_imputed,X_train_avv_imputed), axis=0)\n",
    "#     X_test_imputed = np.concatenate((X_test_aff_imputed,X_test_avv_imputed), axis=0)\n",
    "    # ==================== impute each class seperatly (end) =================\n",
    "\n",
    "\n",
    "    # scale\n",
    "    X_train_scaled, X_test_scaled, scaler_train, scaler_test= transform_var(\n",
    "        imputer_class, X_train_imputed, X_test_imputed, method_name)\n",
    "\n",
    "    # train classifier\n",
    "    if ('model_param' in config_dict) and (classifier_name in config_dict['model_param']):\n",
    "        model = classifier_class(**config_dict['model_param'][classifier_name])\n",
    "    else: \n",
    "        model = classifier_class()\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # predict\n",
    "\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    y_test_pred_prob = model.predict_proba(X_test_scaled)\n",
    "    y_train_pred_prob = model.predict_proba(X_train_scaled)\n",
    "\n",
    "    test_confidence_levels = y_test_pred_prob.max(axis=1)\n",
    "    train_confidence_levels = y_train_pred_prob.max(axis=1)\n",
    "\n",
    "    test_affiliative = y_test_pred_prob[:,0]\n",
    "    train_affiliative = y_train_pred_prob[:,0]\n",
    "\n",
    "    sum_results = sum_all_results(\n",
    "        y_train, y_train_pred, y_test, y_test_pred,\n",
    "        method = method_name,\n",
    "        imputer= imputer_name,\n",
    "        scaler = scaler_name, \n",
    "        model = classifier_name,\n",
    "        k_fold = k_fold,\n",
    "        ind = ind,\n",
    "        trained_model = model,\n",
    "        trained_imputer = imputer_train,\n",
    "        trained_scalar = scaler_train,\n",
    "        iteration = iteration\n",
    "        \n",
    "    )\n",
    "\n",
    "    df_results_train = sum_model_results(\n",
    "        y_train, y_train_pred,train_confidence_levels, train_affiliative,train_file_names ,dataset='train')\n",
    "    df_results_train['dataset'] = 'train'\n",
    "    df_results_test = sum_model_results(y_test, y_test_pred,test_confidence_levels, test_affiliative, test_file_names ,dataset='test')\n",
    "    df_results_test['dataset'] = 'test'\n",
    "    df_results_all = pd.concat([df_results_test,df_results_train])\n",
    "    df_results_all\n",
    "    \n",
    "    df_results_all['method'] = method_name\n",
    "    df_results_all['imputer'] = imputer_name\n",
    "    df_results_all['scaler'] = scaler_name\n",
    "    df_results_all['model'] = classifier_name\n",
    "    df_results_all['k_fold'] = k_fold\n",
    "    df_results_all['ind'] = ind\n",
    "    df_results_all['iteration'] = iteration\n",
    "    \n",
    "\n",
    "    return sum_results, df_results_all, model, imputer_train\n",
    "\n",
    "def model_selection(config_dict, X, y, subject_id, filenames, n_iterations,verbose=True):\n",
    "    all_results_summary = []\n",
    "    all_raw_results = []\n",
    "    errors_log = []\n",
    "    models= []\n",
    "    imputers = []\n",
    "    for iteration in tqdm(range(n_iterations), disable=n_iterations<10):          \n",
    "        for k_fold in config_dict['k_fold']:\n",
    "#             print(f'k_fold: {k_fold}')\n",
    "#             stratified_group_kfold = StratifiedGroupKFold(n_splits=k_fold, shuffle=True)\n",
    "#             for ind, (train_index, test_index) in enumerate(stratified_group_kfold.split(X, y, groups=subject_id)):\n",
    "            stratified_group_kfold = StratifiedKFold(n_splits=k_fold, shuffle=True)        \n",
    "            for ind, (train_index, test_index) in enumerate(stratified_group_kfold.split(X, y)):\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                train_file_names, test_file_names = filenames[train_index], filenames[test_index]\n",
    "                # mamke sure that there is nan columns:\n",
    "                X_train_nan_flag = np.any(np.all(np.isnan(X_train), axis=0))\n",
    "                X_test_nan_flag = np.any(np.all(np.isnan(X_test), axis=0))\n",
    "                if X_train_nan_flag or X_test_nan_flag:\n",
    "#                     if (X_train_nan_flag):\n",
    "#                         print(f'Detected nan columns: {np.isnan(X_train)}')\n",
    "#                     if (X_test_nan_flag):\n",
    "#                         print(f'Detected nan columns: {np.isnan(X_test)}')\n",
    "                                                       \n",
    "                                                       \n",
    "                    continue\n",
    "\n",
    "                for method_name in config_dict['method']:\n",
    "                    for imputer_name in config_dict['imputer'].keys():\n",
    "                        for scaler_name in config_dict['scaler'].keys():\n",
    "                            for classifier_name in config_dict['model'].keys():\n",
    "\n",
    "#                                 try:\n",
    "                                sum_results, raw_results, model, imputer = train_eval_model(\n",
    "                                    config_dict,\n",
    "                                    X_train, y_train, X_test, y_test,\n",
    "                                    train_file_names, test_file_names,\n",
    "                                    method_name,\n",
    "                                    imputer_name,\n",
    "                                    scaler_name,\n",
    "                                    classifier_name,\n",
    "                                    k_fold, ind, iteration)\n",
    "                                all_results_summary.append(sum_results)\n",
    "                                all_raw_results.append(raw_results)\n",
    "                                models.append(model)\n",
    "                                imputers.append(imputer)\n",
    "                                \n",
    "\n",
    "    return all_results_summary, all_raw_results, errors_log,models, imputers\n",
    "                        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad295ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7072\\640660092.py:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  significance_table = significance_table.append(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def variable_significance_table(df_in):\n",
    "    significance_table = pd.DataFrame(columns=[\"Variable\", \"Statistic\", \"p-value\"])\n",
    "    df = df_in[[c for c in df_in.columns if c not in cat_vars or 'sociability' in c]]\n",
    "    \n",
    "    predictor = df[\"sociability\"]\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column != \"sociability\":\n",
    "            \n",
    "            variable = df[column].values\n",
    "            \n",
    "            variable_aff = variable[np.where(predictor=='affiliative')]\n",
    "            variable_avv = variable[np.where(predictor=='aversive')]\n",
    "            \n",
    "            \n",
    "            variable_aff = variable_aff[~pd.isnull(variable_aff)]\n",
    "            variable_avv = variable_avv[~pd.isnull(variable_avv)]\n",
    "            \n",
    "            if len(variable_aff)<MIN_N_SESSIONS or len(variable_avv)< MIN_N_SESSIONS:\n",
    "                continue\n",
    "            \n",
    "            statistic, p_value = mannwhitneyu(variable_avv, variable_aff)\n",
    "#             p_value = ttest_ind(variable_avv, variable_aff).pvalue\n",
    "#             statistic = ttest_ind(variable_avv, variable_aff).statistic\n",
    "\n",
    "            significance_table = significance_table.append(\n",
    "                {\"Variable\": column, \"Statistic\": statistic, \"p-value\": p_value},\n",
    "                ignore_index=True\n",
    "            )\n",
    "    significance_table = significance_table.sort_values('p-value')\n",
    "    return significance_table\n",
    "\n",
    "\n",
    "sig_table_all = variable_significance_table(df_in=all_df)\n",
    "# sig_table_spike = variable_significance_table(df=df_spike_data[[c for c in df_spike_data.columns if c not in cat_vars or 'sociability' in c]])\n",
    "\n",
    "# sig_table_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8ea6163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Variable  Statistic   p-value\n",
      "44               lfp_EA_30_80Hz_enc_pre       12.0  0.011655\n",
      "32               lfp_AA_30_80Hz_enc_pre       41.0  0.017954\n",
      "17   coherence_CeA_MeD_30_80Hz_post_pre       61.0  0.018207\n",
      "6       coherence_AA_MeD_4_12Hz_enc_pre      110.0  0.020729\n",
      "34                lfp_AA_4_12Hz_enc_pre       43.0  0.023270\n",
      "18     coherence_CeA_MeD_4_12Hz_enc_pre       60.0  0.024456\n",
      "4      coherence_AA_MeD_30_80Hz_enc_pre      108.0  0.028108\n",
      "40              lfp_CeA_30_80Hz_enc_pre       20.0  0.031420\n",
      "72              spike_diff_STIA_enc_pre       66.0  0.044931\n",
      "19    coherence_CeA_MeD_4_12Hz_post_pre       57.0  0.052898\n",
      "20   coherence_CeA_STIA_30_80Hz_enc_pre       41.0  0.055278\n",
      "55             lfp_MePV_4_12Hz_post_pre       74.0  0.066684\n",
      "42               lfp_CeA_4_12Hz_enc_pre       24.0  0.069096\n",
      "48              lfp_MeD_30_80Hz_enc_pre      235.0  0.070985\n",
      "16    coherence_CeA_MeD_30_80Hz_enc_pre       55.0  0.083064\n",
      "5     coherence_AA_MeD_30_80Hz_post_pre       98.0  0.107347\n",
      "50               lfp_MeD_4_12Hz_enc_pre      249.0  0.121710\n",
      "2       coherence_AA_BMA_4_12Hz_enc_pre       23.0  0.177489\n",
      "58              lfp_STIA_4_12Hz_enc_pre       71.5  0.183444\n",
      "21  coherence_CeA_STIA_30_80Hz_post_pre       36.0  0.206460\n",
      "13  coherence_BMA_MePV_30_80Hz_post_pre       38.0  0.223776\n",
      "68               spike_diff_MeD_enc_pre      363.0  0.254264\n",
      "56             lfp_STIA_30_80Hz_enc_pre       78.5  0.307805\n",
      "14    coherence_BMA_MePV_4_12Hz_enc_pre       18.0  0.327672\n",
      "3      coherence_AA_BMA_4_12Hz_post_pre       21.0  0.329004\n",
      "1     coherence_AA_BMA_30_80Hz_post_pre       21.0  0.329004\n",
      "22    coherence_CeA_STIA_4_12Hz_enc_pre       33.0  0.370962\n",
      "33              lfp_AA_30_80Hz_post_pre      108.0  0.393153\n",
      "53            lfp_MePV_30_80Hz_post_pre       61.0  0.400258\n",
      "43              lfp_CeA_4_12Hz_post_pre       37.0  0.426864\n",
      "0      coherence_AA_BMA_30_80Hz_enc_pre       20.0  0.428571\n",
      "69              spike_diff_MeD_post_pre      344.0  0.443327\n",
      "39              lfp_BMA_4_12Hz_post_pre      123.5  0.446735\n",
      "38               lfp_BMA_4_12Hz_enc_pre       86.5  0.446735\n",
      "11    coherence_BMA_MeD_4_12Hz_post_pre       41.0  0.447474\n",
      "15   coherence_BMA_MePV_4_12Hz_post_pre       34.0  0.455944\n",
      "52             lfp_MePV_30_80Hz_enc_pre       39.0  0.487960\n",
      "67               spike_diff_EA_post_pre       53.0  0.492022\n",
      "59             lfp_STIA_4_12Hz_post_pre       86.5  0.505978\n",
      "60                spike_diff_AA_enc_pre      104.0  0.510068\n",
      "71             spike_diff_MePV_post_pre       58.0  0.535260\n",
      "7      coherence_AA_MeD_4_12Hz_post_pre       60.0  0.578031\n",
      "70              spike_diff_MePV_enc_pre       41.0  0.584572\n",
      "23   coherence_CeA_STIA_4_12Hz_post_pre       30.0  0.594073\n",
      "57            lfp_STIA_30_80Hz_post_pre       89.5  0.594664\n",
      "25    coherence_EA_MeD_30_80Hz_post_pre       51.0  0.599878\n",
      "10     coherence_BMA_MeD_4_12Hz_enc_pre       45.0  0.629796\n",
      "51              lfp_MeD_4_12Hz_post_pre      309.0  0.658358\n",
      "28   coherence_MeD_STIA_30_80Hz_enc_pre       23.0  0.661172\n",
      "27     coherence_EA_MeD_4_12Hz_post_pre       49.0  0.716784\n",
      "49             lfp_MeD_30_80Hz_post_pre      352.0  0.740164\n",
      "31   coherence_MeD_STIA_4_12Hz_post_pre       24.0  0.742674\n",
      "61               spike_diff_AA_post_pre       97.0  0.751116\n",
      "46                lfp_EA_4_12Hz_enc_pre       36.0  0.761826\n",
      "36              lfp_BMA_30_80Hz_enc_pre       97.5  0.767311\n",
      "35               lfp_AA_4_12Hz_post_pre       95.0  0.826200\n",
      "45              lfp_EA_30_80Hz_post_pre       37.0  0.828557\n",
      "37             lfp_BMA_30_80Hz_post_pre      110.5  0.832612\n",
      "9    coherence_BMA_MeD_30_80Hz_post_pre       49.0  0.836593\n",
      "62               spike_diff_BMA_enc_pre       82.0  0.900109\n",
      "63              spike_diff_BMA_post_pre       88.0  0.900109\n",
      "73             spike_diff_STIA_post_pre       44.0  0.901802\n",
      "66                spike_diff_EA_enc_pre       42.0  0.903892\n",
      "29  coherence_MeD_STIA_30_80Hz_post_pre       29.0  0.913004\n",
      "64               spike_diff_CeA_enc_pre       56.0  0.915106\n",
      "8     coherence_BMA_MeD_30_80Hz_enc_pre       51.0  0.945211\n",
      "12   coherence_BMA_MePV_30_80Hz_enc_pre       26.0  0.954645\n",
      "47               lfp_EA_4_12Hz_post_pre       41.0  0.965401\n",
      "26      coherence_EA_MeD_4_12Hz_enc_pre       43.0  0.967849\n",
      "24     coherence_EA_MeD_30_80Hz_enc_pre       43.0  0.967849\n",
      "41             lfp_CeA_30_80Hz_post_pre       49.0  0.969929\n",
      "54              lfp_MePV_4_12Hz_enc_pre       50.0  0.971018\n",
      "65              spike_diff_CeA_post_pre       55.0  0.971654\n",
      "30    coherence_MeD_STIA_4_12Hz_enc_pre       28.0  1.000000\n"
     ]
    }
   ],
   "source": [
    "print(sig_table_all.to_string())\n",
    "# print(sig_table_spike.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2fa0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "87c81f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_per_param(var_names, all_df,config_dict, n_iterations, verbose=True):\n",
    "\n",
    "    X = all_df[var_names].values\n",
    "\n",
    "    samples_to_remove = ~np.all(np.isnan(X),axis=1)\n",
    "    y = all_df.iloc[samples_to_remove]['sociability'].values\n",
    "    filenames = all_df.iloc[samples_to_remove].index.values\n",
    "    subject_id = all_df.iloc[samples_to_remove]['rat_number'].values\n",
    "    X = all_df.iloc[samples_to_remove][var_names].values\n",
    "    \n",
    "\n",
    "    ret, raw_results, errors_log, models, imputers = model_selection(\n",
    "        config_dict=config_dict,\n",
    "        X=X, y=y,\n",
    "        subject_id=subject_id,\n",
    "        filenames=filenames,\n",
    "        n_iterations=n_iterations,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "#     print(len(models))\n",
    "\n",
    "    raw_results_df = pd.concat(raw_results)\n",
    "    \n",
    "    ret_df = pd.concat(ret)\n",
    "    ret_df = ret_df.reset_index().rename(columns={'index':'dataset'})\n",
    "#     mean_df = ret_df.drop(['trained_model','trained_imputer','trained_scalar','ind'],axis=1).groupby(['dataset','method', 'imputer', 'scaler', 'model', 'k_fold']).mean().reset_index()\n",
    "    mean_df = ret_df.drop(['trained_model','trained_imputer','trained_scalar','ind','iteration'],axis=1).groupby(\n",
    "        ['dataset','method', 'imputer', 'scaler', 'model', 'k_fold']).mean().reset_index()\n",
    "\n",
    "    # Add a new column 'ind' that counts the number of rows for each group\n",
    "    mean_df['ind'] = ret_df.drop(\n",
    "        ['trained_model','trained_imputer','trained_scalar','ind','iteration'],axis=1).groupby(\n",
    "        ['dataset','method', 'imputer', 'scaler', 'model', 'k_fold']).size().reset_index(name='ind')['ind']\n",
    "\n",
    "    return mean_df, raw_results_df, ret_df, errors_log, models, imputers\n",
    "\n",
    "def select_parameters(config_dict, df,sig_table=None,sig_cut_off=0.1, verbose=False, n_iterations=1, metric='f1'):\n",
    "   \n",
    "    if sig_table is None:\n",
    "        sig_table = variable_significance_table(df)\n",
    "    \n",
    "    all_errors_log = []\n",
    "    best_metric = 0\n",
    "    best_sum_table = []\n",
    "    best_raw_rable = []\n",
    "    best_ret_df = []\n",
    "    best_model = []\n",
    "    best_imputer = []\n",
    "    best_params = dict()\n",
    "    n_var_start = 1\n",
    "    sig_table_filt = sig_table.loc[sig_table['p-value']<sig_cut_off]\n",
    "    \n",
    "    n_params = len(sig_table_filt)\n",
    "    all_vars = list(sig_table_filt.Variable.values)\n",
    "    \n",
    "    current_vars = all_vars[:n_var_start]\n",
    "#     current_vars = []\n",
    "    vars_to_test = all_vars[n_var_start:]\n",
    "    \n",
    "    for n_var, new_var in enumerate(vars_to_test):\n",
    "        var_names = current_vars + [new_var]\n",
    "        try:\n",
    "            if verbose:\n",
    "                print(f\"============ New param: {new_var} ({n_var+1} of {len(vars_to_test)}) ============\")\n",
    "            \n",
    "            mean_df, raw_results_df, ret_df, errors_log, models, imputers = run_per_param(\n",
    "                var_names,df,config_dict, n_iterations, verbose)\n",
    "            this_best_sum_df = mean_df.loc[mean_df['dataset']=='test'].sort_values(metric, ascending=False).iloc[0]\n",
    "            all_errors_log.append(errors_log)\n",
    "            this_best_metric = mean_df.loc[mean_df['dataset']=='test'][metric].max()\n",
    "            best_ind = mean_df.loc[mean_df['dataset']=='test'][metric].argmax()\n",
    "            print(best_metric)\n",
    "            if this_best_metric > best_metric:\n",
    "                current_vars.append(new_var)\n",
    "                if verbose:\n",
    "                    print(f'*********New best {metric} was found: {this_best_metric}************')\n",
    "                    print(f'Adding: {new_var}')\n",
    "                    print(f'Current vars: {current_vars}')\n",
    "                    print(f'method:{this_best_sum_df.method}')\n",
    "                    print(f'imputer:{this_best_sum_df.imputer}')\n",
    "                    print(f'model:{this_best_sum_df.model}')\n",
    "                    print(f'k_fold:{this_best_sum_df.k_fold}')\n",
    "\n",
    "                best_metric = this_best_metric\n",
    "                best_sum_table = mean_df\n",
    "                best_ret_df = ret_df\n",
    "                best_raw_rable = raw_results_df.loc[\n",
    "                    (raw_results_df['method'] == this_best_sum_df.method)&\n",
    "                    (raw_results_df['imputer'] == this_best_sum_df.imputer)&\n",
    "                    (raw_results_df['scaler'] == this_best_sum_df.scaler)&\n",
    "                    (raw_results_df['model'] == this_best_sum_df.model)&\n",
    "                    (raw_results_df['k_fold'] == this_best_sum_df.k_fold)\n",
    "                ]\n",
    "    #                 best_model = models[best_ind]\n",
    "    #                 best_imputer = imputers[best_ind]\n",
    "                best_params = {\n",
    "                    'method': this_best_sum_df.method,\n",
    "                    'imputer': this_best_sum_df.imputer,\n",
    "                    'scaler': this_best_sum_df.scaler,\n",
    "                    'model': this_best_sum_df.model,\n",
    "                    'k_fold': this_best_sum_df.k_fold\n",
    "                }\n",
    "            else:\n",
    "                print(f'Variable {new_var} did not improve the model')\n",
    "        except :\n",
    "            print('=================Error - start ==================')\n",
    "            traceback.print_exc()\n",
    "            print('=================Error - end ==================')\n",
    "            continue\n",
    "    print('*************=============ALL DONE============*******************')\n",
    "    return best_metric, best_sum_table,best_raw_rable, current_vars,best_ret_df, all_errors_log, best_model, best_imputer, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f2cf5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run again\n",
    "\n",
    "def get_config_dict(config_dict, best_params):\n",
    "    config_dict_best = config_dict.copy()\n",
    "    for pname, param in best_params.items():\n",
    "        if pname == 'method':\n",
    "            continue\n",
    "        if pname == 'k_fold':\n",
    "            config_dict_best[pname] = [param]\n",
    "        else:\n",
    "            config_dict_best[pname] = {param:config_dict[pname][param]}\n",
    "    return config_dict_best\n",
    "    \n",
    "def run_model_again(\n",
    "    df, list_of_variables,\n",
    "    best_params,\n",
    "    config_dict, \n",
    "    verbose=False,\n",
    "    n_total_iterations=100,\n",
    "    run_shuffled=False,\n",
    "    n_iterations=1,\n",
    "    reshuffle_each_iteration=True,\n",
    "    metric='f1',\n",
    "):\n",
    "    \n",
    "    def run_model_again_helper(df_in, shuffled=False):\n",
    "        all_metric = []\n",
    "        models = []\n",
    "        imputers = []\n",
    "        all_ret_df = []\n",
    "        df = df_in.copy()\n",
    "        \n",
    "        config_dict_best = get_config_dict(config_dict, best_params)\n",
    "        desc = 'Data' if not shuffled else 'Shuffled'\n",
    "        for n in tqdm(range(n_total_iterations),desc=desc):\n",
    "            if shuffled:\n",
    "                df['sociability'] = np.random.permutation(df['sociability'].values)\n",
    "            if reshuffle_each_iteration:\n",
    "                df = df.sample(frac=1)\n",
    "#             try:\n",
    "            mean_df, raw_results_df, ret_df, errors_log, models_sub, imputers_sub = run_per_param(\n",
    "                list_of_variables,\n",
    "                df,\n",
    "                config_dict=config_dict_best,\n",
    "                verbose=verbose,\n",
    "                n_iterations=n_iterations)\n",
    "\n",
    "            metric_val = mean_df.loc[mean_df['dataset'] == 'test'][metric].values\n",
    "            all_metric.append(metric_val)\n",
    "            models.append(models_sub)\n",
    "            imputers.append(imputers_sub)\n",
    "            all_ret_df.append(ret_df)\n",
    "#             except:\n",
    "#                 continue\n",
    "        all_metric = np.array(all_metric)\n",
    "        all_metric = all_metric.flatten()\n",
    "        \n",
    "        models = np.array(models)\n",
    "        models = models.flatten()\n",
    "        \n",
    "        \n",
    "#         print(imputers)\n",
    "#         models = list(itertools.chain(*models))\n",
    "#         imputers = list(itertools.chain(*imputers))\n",
    "\n",
    "\n",
    "        mean_metric_real_data = np.mean(all_metric)\n",
    "        std_error_metric_real_data = np.std(all_metric, ddof=1) / np.sqrt(np.size(all_metric))\n",
    "\n",
    "        print(f'{desc} {metric} Score:{mean_metric_real_data}\\u00B1{std_error_metric_real_data}')\n",
    "        return all_metric, models, imputers, all_ret_df\n",
    "\n",
    "\n",
    "    all_metric, models, imputers, all_ret_df = run_model_again_helper(df,shuffled=False)\n",
    "#     print(len(models))\n",
    "    all_metric_df = pd.DataFrame(all_metric)\n",
    "    all_metric_df['shuffled'] = False\n",
    "    \n",
    "    if run_shuffled:\n",
    "        all_metric_shuffled,models_shuffled, imputers_shuffled, _ = run_model_again_helper(df,shuffled=True)\n",
    "        all_metric_shuffled_df = pd.DataFrame(all_metric_shuffled)\n",
    "        all_metric_shuffled_df['shuffled'] = True\n",
    "        ret_df = pd.concat([all_metric_df,all_metric_shuffled_df])\n",
    "    else:\n",
    "        all_metric_shuffled = []\n",
    "        ret_df = all_metric_df\n",
    "\n",
    "    \n",
    "    \n",
    "    return all_metric, all_metric_shuffled, ret_df, models, imputers, all_ret_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5ded7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_imputer_model_pair_helper(X, y, imputer, model):\n",
    "    X_imp = imputer.transform(X)\n",
    "    y_pred = model.predict(X_imp)\n",
    "    y_prob = model.predict_proba(X_imp)\n",
    "    y_prob = y_prob[:,0]\n",
    "    ret_table = eval_model(y, y_pred, y_prob)\n",
    "    return ret_table\n",
    "    \n",
    "def eval_imputer_model_pair(all_df, var_names, n_iterations, imputer_list, model_list):\n",
    "    X = all_df[var_names].values\n",
    "\n",
    "    samples_to_remove = ~np.all(np.isnan(X),axis=1)\n",
    "    y = all_df.iloc[samples_to_remove]['sociability'].values\n",
    "    filenames = all_df.iloc[samples_to_remove].index.values\n",
    "    subject_id = all_df.iloc[samples_to_remove]['rat_number'].values\n",
    "    X = all_df.iloc[samples_to_remove][var_names].values\n",
    "    \n",
    "\n",
    "    all_res = []\n",
    "    for n, (imputer, model) in enumerate(zip(imputer_list, model_list)):\n",
    "        # split data\n",
    "        for n_iter in range(n_iterations):\n",
    "            temp_res = eval_imputer_model_pair_helper(X, y, imputer, model)\n",
    "            temp_res['pair_index'] = n\n",
    "            temp_res['model'] = model\n",
    "            temp_res['imputer'] = imputer\n",
    "            temp_res['iteration'] = n_iter\n",
    "            \n",
    "            all_res.append(temp_res)\n",
    "    return all_res\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "15de5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict_test = {\n",
    "    'method': ['new'],\n",
    "    'k_fold': [5],\n",
    "    'imputer': {\n",
    "        'iterative_imputer': IterativeImputer,\n",
    "    },\n",
    "    'imputer_param': {\n",
    "        'iterative_imputer': {'max_iter':1000},\n",
    "    },\n",
    "    'scaler':{\n",
    "        'standard_scaler': StandardScaler,\n",
    "    },\n",
    "   'model':{\n",
    "    'random_forest':RandomForestClassifier,\n",
    "#     'logistic_regression': LogisticRegression,\n",
    "#     'svm': svm.SVC,\n",
    "#     'naive_basian': GaussianNB,\n",
    "#     'knn_classifier': KNeighborsClassifier,\n",
    "   },\n",
    "    'model_param':{\n",
    "    'svm':{'probability': True},\n",
    "    'knn_classifier':{'n_neighbors': 5}\n",
    "    }\n",
    "}\n",
    "\n",
    "config_dict = {\n",
    "    'method': ['reuse'],\n",
    "#     'method': ['reuse', 'new'],\n",
    "#     'k_fold': [3,4,5],\n",
    "    'k_fold': [4],\n",
    "    'imputer': {\n",
    "#         'missforest':MissForest,\n",
    "        'iterative_imputer': IterativeImputer,\n",
    "#         'knn_imputer': KNNImputer        \n",
    "    },\n",
    "    'imputer_param': {\n",
    "        'iterative_imputer': {'max_iter':1000},\n",
    "    },\n",
    "    'scaler':{\n",
    "        'standard_scaler': StandardScaler,\n",
    "#         'no_scaler': lambda x: x,\n",
    "    },\n",
    "    'model':{\n",
    "        'random_forest':RandomForestClassifier,\n",
    "#         'logistic_regression': LogisticRegression,\n",
    "#         'svm': svm.SVC,\n",
    "#         'naive_basian': GaussianNB,\n",
    "#         'knn_classifier': KNeighborsClassifier,\n",
    "    },\n",
    "    'model_param':{\n",
    "        'svm':{'probability': True},\n",
    "        'knn_classifier':{'n_neighbors': 5}\n",
    "    }\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c3bd4e",
   "metadata": {},
   "source": [
    "### Create model using the best sig (pvalue<0.05) and the worst sig (pvalue>0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca6c3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_vars_best = [\n",
    "#     'lfp_EA_30_80Hz_enc_pre',\n",
    "#      'lfp_AA_30_80Hz_enc_pre',\n",
    "#      'coherence_CeA_MeD_30_80Hz_post_pre',\n",
    "#      'coherence_AA_MeD_4_18Hz_enc_pre',\n",
    "#      'coherence_CeA_MeD_4_18Hz_enc_pre',\n",
    "#      'coherence_AA_MeD_30_80Hz_enc_pre',\n",
    "#      'coherence_CeA_STIA_30_80Hz_enc_pre',\n",
    "#      'lfp_MePV_4_18Hz_post_pre'\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4df08f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_vars_best = sig_table_all.loc[sig_table_all['p-value']<=0.05]['Variable'].values\n",
    "# current_vars_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cbd19716",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# current_vars_best = sig_table_all.loc[sig_table_all['p-value']<=0.05]['Variable'].values\n",
    "# current_vars_worst = sig_table_all.loc[sig_table_all['p-value']>=0.95]['Variable'].values\n",
    "\n",
    "# best_params = {\n",
    "#     'k_fold':4,\n",
    "#     'model': 'logistic_regression'\n",
    "# }\n",
    "\n",
    "# # all_df_shuffled = all_df.sample(frac=1)\n",
    "# with warnings.catch_warnings(record=True):\n",
    "#     f1_score_best, f1_score_best_shuffled, ret_df_best,models_best, imputers_best = run_model_again(\n",
    "#         df=all_df,\n",
    "#         list_of_variables=current_vars_best,\n",
    "#         best_params = best_params,\n",
    "#         config_dict = config_dict,\n",
    "#         n_total_iterations=100,\n",
    "#         verbose=False,\n",
    "#         n_iterations=1,\n",
    "#         run_shuffled=True\n",
    "#     )\n",
    "#     ret_df_best['data'] = 'best'\n",
    "\n",
    "#     f1_score_worst, f1_score_worst_shuffled, ret_df_worst,models_worst, imputers_worst = run_model_again(\n",
    "#         df=all_df,\n",
    "#         list_of_variables=current_vars_worst,\n",
    "#         best_params = best_params,\n",
    "#         config_dict = config_dict,\n",
    "#         n_total_iterations=100,\n",
    "#         verbose=False,\n",
    "#         n_iterations=1,\n",
    "#         run_shuffled=True\n",
    "#     )\n",
    "#     ret_df_worst['data'] = 'worst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "77d03dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_f1_combined_results = pd.concat([ret_df_worst, ret_df_best])\n",
    "# sns.catplot(data= df_f1_combined_results, x='data', y=0, hue='shuffled', kind='violin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5245a54c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data: 100%|██████████████████████████████████████████████████████████████████████████| 100/100 [02:48<00:00,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data f1 Score:0.7487016666666666±0.004955238215707775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# current_vars = [\n",
    "#     'lfp_EA_30_80Hz_enc_pre',\n",
    "#      'lfp_AA_30_80Hz_enc_pre',\n",
    "#      'coherence_CeA_MeD_30_80Hz_post_pre',\n",
    "#      'coherence_AA_MeD_4_12Hz_enc_pre',\n",
    "#      'coherence_CeA_MeD_4_12Hz_enc_pre',\n",
    "#      'coherence_AA_MeD_30_80Hz_enc_pre',\n",
    "#      'coherence_CeA_STIA_30_80Hz_enc_pre',\n",
    "#      'coherence_CeA_MeD_30_80Hz_enc_pre'\n",
    "#     ]\n",
    "\n",
    "# best_vars = [\n",
    "#     'lfp_EA_30_80Hz_enc_pre',\n",
    "#      'lfp_AA_30_80Hz_enc_pre',\n",
    "#      'coherence_CeA_MeD_30_80Hz_post_pre',\n",
    "#      'coherence_AA_MeD_30_80Hz_enc_pre',\n",
    "#      'coherence_CeA_STIA_30_80Hz_enc_pre',\n",
    "# ]\n",
    "\n",
    "best_params = {\n",
    "    'k_fold':4,\n",
    "    'model': 'random_forest'\n",
    "}\n",
    "\n",
    "# all_df_shuffled = all_df.sample(frac=1)\n",
    "with warnings.catch_warnings(record=True):\n",
    "\n",
    "    metric_score_test, metric_score_test_shuffled, ret_df_test, models_test, imputers_test, ret_dt_all_test = run_model_again(\n",
    "        df=all_df,\n",
    "        list_of_variables=current_vars,\n",
    "        best_params = best_params,\n",
    "        config_dict = config_dict,\n",
    "        n_total_iterations=100,\n",
    "        verbose=False,\n",
    "        n_iterations=1,\n",
    "        run_shuffled=False,\n",
    "        metric = 'f1'\n",
    "\n",
    "    )\n",
    "    ret_df_test['data'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b4a2c25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_best = np.argmax(metric_score_test)\n",
    "best_models = models_test[ind_best]\n",
    "best_imputers = imputers_test[ind_best]\n",
    "\n",
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "model_paramas = {'model': best_models, 'imputer': best_imputers, 'params': current_vars}\n",
    "model_filename = osp.join(folder_path, 'best_model_top4_20230626.sav')\n",
    "pickle.dump(model_paramas, open(model_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "657ec7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ret_test = pd.concat(ret_dt_all_all)\n",
    "ret_test = pd.concat(ret_dt_all_test)\n",
    "ret_test = ret_test.loc[ret_test['dataset']=='test']\n",
    "ret_test\n",
    "\n",
    "var_names = current_vars\n",
    "\n",
    "model_list = ret_test['trained_model'].values\n",
    "imputer_list = ret_test['trained_imputer'].values\n",
    "\n",
    "ret = eval_imputer_model_pair(\n",
    "    all_df,\n",
    "    var_names,\n",
    "    n_iterations=1,\n",
    "    imputer_list=imputer_list,\n",
    "    model_list=model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5446f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cba14a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list = [pd.DataFrame(a,index=[a['pair_index']]) for a in ret]\n",
    "df_list = [pd.DataFrame(a) for a in ret]\n",
    "df_ret_all = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddeeb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "449265f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy                                                    1.0\n",
      "precision                                                   1.0\n",
      "recall                                                      1.0\n",
      "f1                                                          1.0\n",
      "kappa                                                       1.0\n",
      "roc_auc                                                     1.0\n",
      "pair_index                                                    0\n",
      "model         DecisionTreeClassifier(max_features='sqrt', ra...\n",
      "imputer                         IterativeImputer(max_iter=1000)\n",
      "iteration                                                     0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "metric = 'roc_auc'\n",
    "best_tab = df_ret_all.iloc[df_ret_all[metric].idxmax()]\n",
    "print(best_tab)\n",
    "best_model = best_tab.model\n",
    "best_imputer = best_tab.imputer\n",
    "# best_imputer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d9f81f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset                                                         test\n",
      "accuracy                                                       0.818\n",
      "precision                                                        0.8\n",
      "recall                                                           0.8\n",
      "f1                                                               0.8\n",
      "kappa                                                          0.633\n",
      "method                                                         reuse\n",
      "imputer                                            iterative_imputer\n",
      "scaler                                               standard_scaler\n",
      "model                                                  random_forest\n",
      "k_fold                                                             4\n",
      "ind                                                                1\n",
      "trained_model      (DecisionTreeClassifier(max_features='sqrt', r...\n",
      "trained_imputer                      IterativeImputer(max_iter=1000)\n",
      "trained_scalar                                    IterativeImputer()\n",
      "iteration                                                          0\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "best_tab = ret_test.iloc[1]\n",
    "# best_tab = df_ret_all.iloc[]\n",
    "print(best_tab)\n",
    "best_model = best_tab.trained_model      \n",
    "best_imputer = best_tab.trained_imputer                      \n",
    "# best_imputer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7d63caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "model_paramas = {'model': best_model, 'imputer': best_imputer, 'params': current_vars}\n",
    "# model_filename = osp.join(folder_path, 'best_model_20230626.sav')\n",
    "# pickle.dump(model_paramas, open(model_filename, 'wb'))\n",
    "\n",
    "# imputer_filename = osp.join(folder_path, 'best_imputer.sav')\n",
    "# pickle.dump(best_imputer, open(imputer_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(record=True):\n",
    "    best_metric, best_sum_table, best_raw_rable, current_vars, best_ret_df, all_errors_log, best_model, best_imputer, best_params = select_parameters(\n",
    "        config_dict = config_dict,\n",
    "        sig_table = None,\n",
    "        verbose = True,\n",
    "        df = df_spike_data,\n",
    "        sig_cut_off =0.6,\n",
    "        n_iterations=100)\n",
    "\n",
    "    metric_score_spike, metric_score_spike_shuffled, ret_df_spike = run_model_again(\n",
    "        df=df_spike_data,\n",
    "        list_of_variables=current_vars,\n",
    "        best_params = best_params,\n",
    "        config_dict = config_dict,\n",
    "        n_total_iterations=100,\n",
    "        verbose=False,\n",
    "        n_iterations=1,\n",
    "        run_shuffled=True,\n",
    "        reshuffle_each_iteration=True,\n",
    "    )\n",
    "    ret_df_spike['data'] = 'spikes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score_spike, f1_score_spike_shuffled, ret_df_spike = run_model_again(\n",
    "#     df=df_spike_data,\n",
    "#     list_of_variables=current_vars,\n",
    "#     best_params = best_params,\n",
    "#     config_dict = config_dict,\n",
    "#     n_total_iterations=100,\n",
    "#     verbose=False,\n",
    "#     n_iterations=1,\n",
    "#     run_shuffled=True\n",
    "# )\n",
    "# ret_df_spike['data'] = 'spikes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc40ad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with warnings.catch_warnings(record=True):\n",
    "    best_metric, best_sum_table, best_raw_rable, current_vars, best_ret_df, all_errors_log, best_model, best_imputer, best_params = select_parameters(\n",
    "        config_dict = config_dict,\n",
    "        sig_table = None,\n",
    "        verbose = True,\n",
    "        df = df_coherence_data,\n",
    "        sig_cut_off =0.1,\n",
    "        n_iterations=100)\n",
    "\n",
    "    metric_score_coherence,metric_score_coherence_shuffled,ret_df_coherence = run_model_again(\n",
    "        df=df_coherence_data,\n",
    "        list_of_variables=current_vars,\n",
    "        best_params = best_params,\n",
    "        config_dict = config_dict,\n",
    "        n_total_iterations=100,\n",
    "        verbose=False,\n",
    "        n_iterations=1,\n",
    "        run_shuffled=True,\n",
    "        reshuffle_each_iteration=True,\n",
    "\n",
    "    )\n",
    "ret_df_coherence['data'] = 'coherence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f754801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ret_df_coherence['data'] = 'coherence'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(record=True):\n",
    "    best_metric, best_sum_table, best_raw_rable, current_vars, best_ret_df, all_errors_log, best_model, best_imputer, best_params = select_parameters(\n",
    "        config_dict = config_dict,\n",
    "        sig_table = None,\n",
    "        verbose = True,\n",
    "        df = df_lfp_data,\n",
    "        sig_cut_off =0.1,\n",
    "        n_iterations=100)\n",
    "\n",
    "    metric_score_lfp,metric_score_lfp_shuffled,ret_df_lfp = run_model_again(\n",
    "        df=df_lfp_data,\n",
    "        list_of_variables=current_vars,\n",
    "        best_params = best_params,\n",
    "        config_dict = config_dict,\n",
    "        n_total_iterations=100,\n",
    "        verbose=False,\n",
    "        run_shuffled=True,\n",
    "        n_iterations=1,\n",
    "        reshuffle_each_iteration=True,\n",
    "    )\n",
    "    \n",
    "ret_df_lfp['data'] = 'lfp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df613d30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ New param: lfp_AA_30_80Hz_enc_pre (1 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:56<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "*********New best kappa was found: 0.14651************\n",
      "Adding: lfp_AA_30_80Hz_enc_pre\n",
      "Current vars: ['lfp_EA_30_80Hz_enc_pre', 'lfp_AA_30_80Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:4\n",
      "============ New param: coherence_CeA_MeD_30_80Hz_post_pre (2 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14651\n",
      "*********New best kappa was found: 0.3351152882205514************\n",
      "Adding: coherence_CeA_MeD_30_80Hz_post_pre\n",
      "Current vars: ['lfp_EA_30_80Hz_enc_pre', 'lfp_AA_30_80Hz_enc_pre', 'coherence_CeA_MeD_30_80Hz_post_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:4\n",
      "============ New param: coherence_AA_MeD_4_12Hz_enc_pre (3 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:55<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3351152882205514\n",
      "*********New best kappa was found: 0.49024060150375937************\n",
      "Adding: coherence_AA_MeD_4_12Hz_enc_pre\n",
      "Current vars: ['lfp_EA_30_80Hz_enc_pre', 'lfp_AA_30_80Hz_enc_pre', 'coherence_CeA_MeD_30_80Hz_post_pre', 'coherence_AA_MeD_4_12Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:4\n",
      "============ New param: lfp_AA_4_12Hz_enc_pre (4 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:51<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49024060150375937\n",
      "Variable lfp_AA_4_12Hz_enc_pre did not improve the model\n",
      "============ New param: coherence_CeA_MeD_4_12Hz_enc_pre (5 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [03:27<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49024060150375937\n",
      "*********New best kappa was found: 0.5090875************\n",
      "Adding: coherence_CeA_MeD_4_12Hz_enc_pre\n",
      "Current vars: ['lfp_EA_30_80Hz_enc_pre', 'lfp_AA_30_80Hz_enc_pre', 'coherence_CeA_MeD_30_80Hz_post_pre', 'coherence_AA_MeD_4_12Hz_enc_pre', 'coherence_CeA_MeD_4_12Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:4\n",
      "============ New param: coherence_AA_MeD_30_80Hz_enc_pre (6 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:15<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5090875\n",
      "*********New best kappa was found: 0.5301729323308271************\n",
      "Adding: coherence_AA_MeD_30_80Hz_enc_pre\n",
      "Current vars: ['lfp_EA_30_80Hz_enc_pre', 'lfp_AA_30_80Hz_enc_pre', 'coherence_CeA_MeD_30_80Hz_post_pre', 'coherence_AA_MeD_4_12Hz_enc_pre', 'coherence_CeA_MeD_4_12Hz_enc_pre', 'coherence_AA_MeD_30_80Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:4\n",
      "============ New param: lfp_CeA_30_80Hz_enc_pre (7 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5301729323308271\n",
      "Variable lfp_CeA_30_80Hz_enc_pre did not improve the model\n",
      "============ New param: spike_diff_STIA_enc_pre (8 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5301729323308271\n",
      "Variable spike_diff_STIA_enc_pre did not improve the model\n",
      "============ New param: coherence_CeA_MeD_4_12Hz_post_pre (9 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:21<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5301729323308271\n",
      "*********New best kappa was found: 0.5339348370927318************\n",
      "Adding: coherence_CeA_MeD_4_12Hz_post_pre\n",
      "Current vars: ['lfp_EA_30_80Hz_enc_pre', 'lfp_AA_30_80Hz_enc_pre', 'coherence_CeA_MeD_30_80Hz_post_pre', 'coherence_AA_MeD_4_12Hz_enc_pre', 'coherence_CeA_MeD_4_12Hz_enc_pre', 'coherence_AA_MeD_30_80Hz_enc_pre', 'coherence_CeA_MeD_4_12Hz_post_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:4\n",
      "============ New param: coherence_CeA_STIA_30_80Hz_enc_pre (10 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [03:10<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5339348370927318\n",
      "*********New best kappa was found: 0.5653208020050126************\n",
      "Adding: coherence_CeA_STIA_30_80Hz_enc_pre\n",
      "Current vars: ['lfp_EA_30_80Hz_enc_pre', 'lfp_AA_30_80Hz_enc_pre', 'coherence_CeA_MeD_30_80Hz_post_pre', 'coherence_AA_MeD_4_12Hz_enc_pre', 'coherence_CeA_MeD_4_12Hz_enc_pre', 'coherence_AA_MeD_30_80Hz_enc_pre', 'coherence_CeA_MeD_4_12Hz_post_pre', 'coherence_CeA_STIA_30_80Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:4\n",
      "============ New param: lfp_MePV_4_12Hz_post_pre (11 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:40<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5653208020050126\n",
      "Variable lfp_MePV_4_12Hz_post_pre did not improve the model\n",
      "============ New param: lfp_CeA_4_12Hz_enc_pre (12 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:46<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5653208020050126\n",
      "Variable lfp_CeA_4_12Hz_enc_pre did not improve the model\n",
      "============ New param: lfp_MeD_30_80Hz_enc_pre (13 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [08:31<00:00,  5.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5653208020050126\n",
      "Variable lfp_MeD_30_80Hz_enc_pre did not improve the model\n",
      "============ New param: coherence_CeA_MeD_30_80Hz_enc_pre (14 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [04:16<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5653208020050126\n",
      "Variable coherence_CeA_MeD_30_80Hz_enc_pre did not improve the model\n",
      "*************=============ALL DONE============*******************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data: 100%|██████████████████████████████████████████████████████████████████████████| 100/100 [03:00<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data kappa Score:0.560465±0.008353638156788062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffled: 100%|██████████████████████████████████████████████████████████████████████| 100/100 [03:25<00:00,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled kappa Score:-0.016472500000000004±0.018515659040574013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings(record=True):\n",
    "    best_metric, best_sum_table, best_raw_rable,\\\n",
    "    current_vars, best_ret_df, all_errors_log, best_model, best_imputer, best_params = select_parameters(\n",
    "        config_dict = config_dict,\n",
    "        sig_table = None,\n",
    "        verbose = True,\n",
    "        df = all_df,\n",
    "        sig_cut_off =0.1,\n",
    "        n_iterations=100,\n",
    "        metric=METRIC\n",
    "    )\n",
    "\n",
    "    metric_score_all, metric_score_all_shuffled, ret_df_all, models_all, imputers_all, ret_dt_all_all = run_model_again(\n",
    "        df=all_df,\n",
    "        list_of_variables=current_vars,\n",
    "        best_params = best_params,\n",
    "        config_dict = config_dict,\n",
    "        n_total_iterations=100,\n",
    "        verbose=False,\n",
    "        run_shuffled=True,\n",
    "        n_iterations=1,\n",
    "        reshuffle_each_iteration=True,\n",
    "        metric=METRIC\n",
    "    )\n",
    "    \n",
    "ret_df_all['data'] = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffbbcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ret_df_lfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df83984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric_combined_results = pd.concat([ret_df_all, ret_df_spike, ret_df_lfp, ret_df_coherence])\n",
    "sns.catplot(data= df_metric_combined_results, x='data', y=0, hue='shuffled', kind='violin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e02704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with warnings.catch_warnings(record=True):\n",
    "#     best_f1, best_sum_table, best_raw_rable, current_vars, best_ret_df, all_errors_log, best_model, best_imputer, best_params = select_parameters(\n",
    "#         config_dict = config_dict,\n",
    "#         sig_table = sig_table_all,\n",
    "#         all_df = all_df, n_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a695b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_ret_df.loc[\n",
    "#     (best_ret_df['imputer'] == 'iterative_imputer')&\n",
    "#     (best_ret_df['model'] == 'random_forest')&\n",
    "#     (best_ret_df['kfold'] == 5)&\n",
    "#     (best_ret_df['dataset'] == 'test')]\n",
    "\n",
    "# best_raw_rable.loc[\n",
    "#        (best_raw_rable['imputer'] == 'iterative_imputer')&\n",
    "#         (best_raw_rable['model'] == 'random_forest')&\n",
    "#         (best_raw_rable['kfold'] == 5)]\n",
    "# best_raw_rable.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f040fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_raw_rable.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_sum_table.to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5880134",
   "metadata": {},
   "source": [
    "### Best params selected:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff2134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e569f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4847ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict_best = config_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a08c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pname, param in best_params.items():\n",
    "#     if pname == 'method':\n",
    "#         continue\n",
    "#     if pname == 'k_fold':\n",
    "#         config_dict_best[pname] = [param]\n",
    "# #         config_dict_best[pname] = [3]\n",
    "#     else:\n",
    "#         config_dict_best[pname] = {param:config_dict[pname][param]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69952dbd",
   "metadata": {},
   "source": [
    "### Recreate sucsess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc956355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_dict_best = {'method': ['reuse'],\n",
    "#  'k_fold': [4],\n",
    "#  'imputer': {'imputer': sklearn.impute._iterative.IterativeImputer},\n",
    "#  'imputer_param': {'iterative_imputer': {'max_iter': 10000}},\n",
    "#  'scaler': {'scaler': sklearn.preprocessing._data.StandardScaler},\n",
    "#  'model': {'model': LogisticRegression},\n",
    "#  'model_param': {'svm': {'probability': True},\n",
    "#   'knn_classifier': {'n_neighbors': 5}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f5f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_df, raw_results_df, ret_df, errors_log, models, imputers = run_per_param(\n",
    "#     current_vars, all_df, config_dict=config_dict_best,n_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ba9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f277de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_results_df.loc[raw_results_df['dataset'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a901d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_shuffled = all_df.copy()\n",
    "all_df_shuffled['sociability'] = np.random.permutation(all_df_shuffled['sociability'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576d0a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_total_iterations = 100\n",
    "n_iterations = 1\n",
    "\n",
    "all_metric = []\n",
    "\n",
    "for n in range(n_total_iterations):\n",
    "    try:\n",
    "        mean_df, raw_results_df, ret_df, errors_log, models, imputers = run_per_param(\n",
    "            current_vars, all_df, config_dict=config_dict_best, n_iterations=n_iterations)\n",
    "        metric_val = mean_df.loc[mean_df['dataset'] == 'test'][metric].values\n",
    "        all_metric.append(metric_val)\n",
    "    except:\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef74acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de74639c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5497e3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_total_iterations = 100\n",
    "n_iterations = 1\n",
    "\n",
    "shuffled_all_metric = []\n",
    "\n",
    "for n in range(n_total_iterations):\n",
    "    all_df_shuffled = all_df.copy()\n",
    "    try:\n",
    "        all_df_shuffled['sociability'] = np.random.permutation(all_df_shuffled['sociability'].values)\n",
    "        shuffled_mean_df, shuffled_raw_results_df, shuffled_ret_df, shuffled_errors_log, shuffled_models, shuffled_imputers = run_per_param(\n",
    "            current_vars, all_df_shuffled, config_dict=config_dict_best, n_iterations=n_iterations, verbose=False)\n",
    "        metric_val = shuffled_mean_df.loc[mean_df['dataset'] == 'test'][metric].values\n",
    "        shuffled_all_metric.append(metric_val)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_all_metric = np.array(shuffled_all_metric)\n",
    "shuffled_all_metric = shuffled_all_metric.flatten()\n",
    "\n",
    "mean_metric_shuffled_data = np.mean(shuffled_all_metric)\n",
    "std_error_shuffled_data = np.std(shuffled_all_metric, ddof=1) / np.sqrt(np.size(shuffled_all_metric))\n",
    "\n",
    "print(f'{metric} Score:{mean_metric_shuffled_data}+-{std_error_shuffled_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde6edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric = pd.DataFrame(zip(all_metric, shuffled_all_metric), columns=['Real Data','Randomized Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.violinplot(data=df_metric)\n",
    "g.set_ylabel(f'Mean {metric} Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "real_scores, random_scores = all_metric, shuffled_all_metric\n",
    "\n",
    "# Calculate the p-value using a paired t-test\n",
    "_, p_value = ttest_rel(real_scores, random_scores)\n",
    "\n",
    "# Check if the p-value is below the significance level (e.g., 0.05)\n",
    "significance_level = 0.05\n",
    "if p_value < significance_level:\n",
    "    print(\"The model's performance on real data is significantly better than random.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the model's performance on real data and random.\")\n",
    "\n",
    "# Quantify the improvement using effect size measures (e.g., Cohen's d)\n",
    "mean_real = np.mean(real_scores)\n",
    "mean_random = np.mean(random_scores)\n",
    "std_real = np.std(real_scores, ddof=1)\n",
    "effect_size = (mean_real - mean_random) / std_real\n",
    "\n",
    "print(\"Effect size (Cohen's d):\", effect_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fdfa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = best_raw_rable[['GT','predicted','correct','affiliative_level','filenames','dataset']].copy()\n",
    "\n",
    "df['rat_number'] = df['filenames'].apply(extract_ratnum_from_file_name)\n",
    "df['day_number'] = df['filenames'].apply(extract_daynum_from_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513cbf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Percentage of time each rat was in the test or train datasets\n",
    "rat_dataset_percentage = df.groupby('rat_number')['dataset'].value_counts(normalize=True) * 100\n",
    "print(\"Percentage of time each rat was in the test or train datasets:\")\n",
    "print(rat_dataset_percentage)\n",
    "\n",
    "# Percentage of time each rat was correctly predicted in the test dataset\n",
    "test_correct_percentage = df[(df['dataset'] == 'test') & (df['correct'])].groupby('rat_number')['correct'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage of time each rat was correctly predicted in the test dataset:\")\n",
    "print(test_correct_percentage)\n",
    "\n",
    "# Percentage of time each session was in the test or train datasets\n",
    "session_dataset_percentage = df.groupby('filenames')['dataset'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage of time each session was in the test or train datasets:\")\n",
    "print(session_dataset_percentage)\n",
    "\n",
    "# Percentage of times each session was correctly predicted in the test dataset\n",
    "test_session_correct_percentage = df[(df['dataset'] == 'test') & (df['correct'])].groupby('filenames')['correct'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage of times each session was correctly predicted in the test dataset:\")\n",
    "print(test_session_correct_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the dataframe is already loaded and named 'df'\n",
    "\n",
    "total_count = len(df)  # Total count of samples in the dataset\n",
    "\n",
    "# Percentage of time each session was in the test or train datasets\n",
    "session_dataset_percentage = (df.groupby('filenames')['dataset'].value_counts() / total_count) * 100\n",
    "print(\"\\nPercentage of time each session was in the test or train datasets:\")\n",
    "print(session_dataset_percentage)\n",
    "\n",
    "# Percentage of times each session was correctly predicted in the test dataset\n",
    "test_session_correct_percentage = (df[(df['dataset'] == 'test') & (df['correct'])].groupby('filenames')['correct'].value_counts() / total_count) * 100\n",
    "print(\"\\nPercentage of times each session was correctly predicted in the test dataset:\")\n",
    "print(test_session_correct_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d676a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4d83663",
   "metadata": {},
   "source": [
    "## Feature selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb7094",
   "metadata": {},
   "source": [
    "Feature selection steps:\n",
    "\n",
    "1. Find top 2 candidates\n",
    "2. Train model and get results\n",
    "3. Add next candiate and repeat step 2.\n",
    "4. If the resutls improved, add this featrue, if not, advance to the next candidate\n",
    "\n",
    "Model trainig steps:\n",
    "\n",
    "1. Split data\n",
    "\n",
    "3. Impute missing values\n",
    "    a. MissForest\n",
    "    b. IterativeImputer\n",
    "    \n",
    "4. rescale data (or not)\n",
    "\n",
    "5. Train model/s\n",
    "    a. svm\n",
    "    b. logistic regression\n",
    "    c. random forest classifier\n",
    "    d. knn classifier\n",
    "    \n",
    "6. Evaluate model/s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112191f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer_class = MissForest\n",
    "classifier_class = RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([('imputer', imputer_class()), ('scaler', StandardScaler()),('classifier', classifier_class())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3946e1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99014a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ee48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_test['dataset'] = 'test'\n",
    "df_results_train['dataset'] = 'train'\n",
    "df_results_all = pd.concat([df_results_train,df_results_test])\n",
    "df_results_all['rat_num'] = df_results_all['files'].apply(extract_ratnum_from_file_name)\n",
    "df_results_all['day_num'] = df_results_all['files'].apply(extract_day_from_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f406ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=df_results_all, x='day_num', y='affiliative_level', hue='GT', col='dataset')\n",
    "sns.lmplot(data=df_results_all, x='day_num', y='affiliative_level', hue='predicted', col='dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e57721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bdc3f2a",
   "metadata": {},
   "source": [
    "## Dimensionality reduction using t-SNE and visualization|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccccb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create the figure and axes\n",
    "# fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# # add the plots for each dataframe\n",
    "# df_rats = df_results_all.groupby('rat_num')\n",
    "# for ratnum , df_r in df_rats:\n",
    "#     sns.regplot(x='day_num', y='affiliative_level', data=df_r, fit_reg=True, ci=None, ax=ax, label=ratnum)\n",
    "# ax.set(ylabel='affiliative_level', xlabel='rat_num')\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67650f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = df_train.drop(['sociability','dataset'], axis = 1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e1c54d",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f9caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pairplot(title, X,y,y_pred):\n",
    "    df_X = pd.DataFrame(X)\n",
    "\n",
    "\n",
    "    df_X = df_X.rename(columns={num:cname for num, cname in enumerate(col_names)})\n",
    "    df_X['GT'] = y\n",
    "    df_X['predicted'] = y_pred\n",
    "    df_X['correct'] = df_X['predicted'] == df_X['GT']\n",
    "\n",
    "    g = sns.pairplot(data=df_X.drop(['predicted','correct'], axis=1), hue='GT')\n",
    "    g.fig.suptitle(f\"{title} Data GT sociability\", y=1.05)\n",
    "\n",
    "    g = sns.pairplot(data=df_X.drop(['GT','correct'], axis=1), hue='predicted')\n",
    "    g.fig.suptitle(f\"{title} Data GT predicted\", y=1.05)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1123aba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X = X_test\n",
    "y = y_test.values\n",
    "y_pred = y_test_pred\n",
    "title = 'Test - Unimputed'\n",
    "plot_pairplot(title, X,y,y_pred)\n",
    "\n",
    "\n",
    "X = X_train\n",
    "y = y_train.values\n",
    "y_pred = y_train_pred\n",
    "title = 'Train - Unimputed'\n",
    "plot_pairplot(title, X,y,y_pred)\n",
    "\n",
    "X = X_test_imp\n",
    "y = y_test.values\n",
    "y_pred = y_test_pred\n",
    "title = 'Test - Imputed'\n",
    "plot_pairplot(title, X,y,y_pred)\n",
    "\n",
    "X = X_train_imp\n",
    "y = y_train.values\n",
    "y_pred = y_train_pred\n",
    "title = 'Train - Imputed'\n",
    "plot_pairplot(title, X,y,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebada199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b3b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e679a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b52ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test_imp\n",
    "\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=3).fit_transform(X)\n",
    "\n",
    "df_results_test[['tsne1','tsne2']] = X_embedded\n",
    "sns.pairplot(df_results_test[['GT','tsne1','tsne2']], hue='GT')\n",
    "sns.pairplot(df_results_test[['predicted','tsne1','tsne2']], hue='predicted')\n",
    "\n",
    "\n",
    "X = X_train_imp\n",
    "\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=3).fit_transform(X)\n",
    "\n",
    "df_results_train[['tsne1','tsne2']] = X_embedded\n",
    "sns.pairplot(df_results_train[['GT','tsne1','tsne2']], hue='GT')\n",
    "sns.pairplot(df_results_train[['predicted','tsne1','tsne2']], hue='predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4768b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = X_test_imp\n",
    "\n",
    "X_embedded = TSNE(n_components=1, learning_rate='auto',\n",
    "                  init='random', perplexity=2).fit_transform(X)\n",
    "\n",
    "# df_results_test[['tsne1','tsne2']] = X_embedded\n",
    "# sns.pairplot(df_results_test[['GT','tsne1','tsne2']], hue='GT')\n",
    "# sns.pairplot(df_results_test[['predicted','tsne1','tsne2']], hue='predicted')\n",
    "\n",
    "df_results_test[['tsne1']] = X_embedded\n",
    "df_results_test['affiliative_level_logit'] = df_results_test['affiliative_level'].apply(logit)\n",
    "sns.lmplot(data=df_results_test, x = 'affiliative_level_logit' ,y = 'tsne1', hue='GT')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eedda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_imp\n",
    "\n",
    "X_embedded = TSNE(n_components=1, learning_rate='auto',\n",
    "                  init='random', perplexity=1).fit_transform(X)\n",
    "\n",
    "# df_results_test[['tsne1','tsne2']] = X_embedded\n",
    "# sns.pairplot(df_results_test[['GT','tsne1','tsne2']], hue='GT')\n",
    "# sns.pairplot(df_results_test[['predicted','tsne1','tsne2']], hue='predicted')\n",
    "\n",
    "df_results_train[['tsne1']] = X_embedded\n",
    "df_results_train['affiliative_level_logit'] = df_results_train['affiliative_level'].apply(logit)\n",
    "sns.(data=df_results_train, x = 'affiliative_level_logit' ,y = 'tsne1', hue='GT')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_imp\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=3).fit_transform(X)\n",
    "\n",
    "df_results_train[['tsne1','tsne2']] = X_embedded\n",
    "\n",
    "sns.pairplot(df_results_train[['GT','tsne1','tsne2']], hue='GT')\n",
    "\n",
    "sns.pairplot(df_results_train[['predicted','tsne1','tsne2']], hue='predicted')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc5eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data (train and test)\n",
    "\n",
    "\n",
    "X = np.append(X_train_imp, X_test_imp, axis=0)\n",
    "y = np.append(y_train, y_test)\n",
    "y_pred = np.append(y_train_pred, y_test_pred)\n",
    "\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=10, n_iter=5000).fit_transform(X)\n",
    "\n",
    "\n",
    "df_res_tsne = pd.DataFrame(y, columns=['GT'])\n",
    "df_res_tsne['dataset'] = np.append(np.full(y_train.shape, \"train\"), np.full(y_test.shape, \"test\"))\n",
    "df_res_tsne['predicted'] = y_pred\n",
    "df_res_tsne['correct'] = df_res_tsne['predicted'] == df_res_tsne['GT']\n",
    "df_res_tsne[['tsne1','tsne2']] = X_embedded\n",
    "#Seaborn pair plot\n",
    "\n",
    "sns.pairplot(df_res_tsne[['GT','tsne1','tsne2']], hue='GT')\n",
    "\n",
    "sns.pairplot(df_res_tsne[['predicted','tsne1','tsne2']], hue='predicted')\n",
    "\n",
    "# # Display the plots\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_tsne.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3faf52a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d4ad369",
   "metadata": {},
   "source": [
    "### Impute the missing values of all the dataset before training and testing\n",
    "\n",
    "Performing imputation before splitting the dataset can potentially lead to data leakage and overly optimistic evaluation results. It's generally recommended to split the dataset into training and testing sets before applying any data preprocessing steps, including imputation.\n",
    "\n",
    "Data leakage can occur when information from the testing set is inadvertently used during the imputation process. This can lead to overfitting and unrealistic evaluation results because the imputation is informed by the target variable in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e0e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer = IterativeImputer(max_iter=100)\n",
    "imputer_class = MissForest\n",
    "\n",
    "# model_class = RandomForestClassifier\n",
    "# model = LogisticRegression(max_iter=10000)\n",
    "# model_class = svm.SVC\n",
    "model_class = tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9783059",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[:,:3], y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7f830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute data before splitting it into train and test\n",
    "\n",
    "imputer = imputer_class()\n",
    "model = model_class()\n",
    "\n",
    "imputed_data = imputer.fit_transform(X_train)\n",
    "\n",
    "# # Splitting the imputed data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(imputed_data,\n",
    "#                                                     labels,\n",
    "#                                                     test_size=0.3,\n",
    "#                                                     stratify=labels)\n",
    "\n",
    "\n",
    "# Creating and training the model (using Logistic Regression as an example)\n",
    "\n",
    "\n",
    "model.fit(imputed_data, y_train)\n",
    "\n",
    "\n",
    "# Making predictions on the training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Calculating accuracy scores\n",
    "# train_accuracy = accuracy_score(y_train.values, y_train_pred)\n",
    "# test_accuracy = accuracy_score(y_test.values, y_test_pred)\n",
    "ret_test = eval_model(y_test, y_test_pred)\n",
    "ret_train = eval_model(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "\n",
    "df_results_test = pd.DataFrame(y_test, columns={'GT'})\n",
    "df_results_test['predicted'] = y_test_pred\n",
    "df_results_test['correct'] = df_results_test['GT'] == df_results_test['predicted']\n",
    "print(df_results_test)\n",
    "\n",
    "\n",
    "print('Test')\n",
    "[print(f'{metric}:{values}') for metric, values in ret_test.items()]\n",
    "print('Train')\n",
    "[print(f'{metric}:{values}') for metric, values in ret_train.items()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae68078",
   "metadata": {},
   "source": [
    "### Imputing the data after each splitting\n",
    "\n",
    "The imputaion quality drops when performing on smaller sub sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a9dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600cd62d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Splitting the imputed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.3,\n",
    "                                                    stratify=labels)\n",
    "\n",
    "imputer = imputer_class()\n",
    "model = model_class()\n",
    "\n",
    "imputed_data_train = imputer.fit_transform(X_train)\n",
    "imputed_data_test = imputer.fit_transform(X_test)\n",
    "\n",
    "# Converting the imputed data back to a DataFrame\n",
    "imputed_data_train = pd.DataFrame(imputed_data_train, columns=X_train.columns)\n",
    "imputed_data_test = pd.DataFrame(imputed_data_test, columns=X_test.columns)\n",
    "\n",
    "\n",
    "\n",
    "# Creating and training the model (using Logistic Regression as an example)\n",
    "# model = LogisticRegression()\n",
    "model.fit(imputed_data_train, y_train)\n",
    "\n",
    "# Making predictions on the training and testing sets\n",
    "y_train_pred = model.predict(imputed_data_train)\n",
    "y_test_pred = model.predict(imputed_data_test)\n",
    "\n",
    "# Calculating accuracy scores\n",
    "# train_accuracy = accuracy_score(y_train.values, y_train_pred)\n",
    "# test_accuracy = accuracy_score(y_test.values, y_test_pred)\n",
    "ret_test = eval_model(y_test, y_test_pred)\n",
    "ret_train = eval_model(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "df_results_test = pd.DataFrame(y_test, columns={'GT'})\n",
    "df_results_test['predicted'] = y_test_pred\n",
    "df_results_test['correct'] = df_results_test['GT'] == df_results_test['predicted']\n",
    "print(df_results_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Test')\n",
    "[print(f'{metric}:{values}') for metric, values in ret_test.items()]\n",
    "print('Train')\n",
    "[print(f'{metric}:{values}') for metric, values in ret_train.items()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835e3cef",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "Since the data plitting have significant effect on the model performance, cross validation is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_fold(X_train, X_test, y_train, y_test):\n",
    "        # Splitting the imputed data into training and testing sets\n",
    "\n",
    "\n",
    "    imputer = imputer_class(max_iter=500)\n",
    "    model = model_class()\n",
    "    \n",
    "    imputed_data_train = imputer.fit_transform(X_train)\n",
    "    imputed_data_test = imputer.fit_transform(X_test)\n",
    "\n",
    "    # Converting the imputed data back to a DataFrame\n",
    "#     imputed_data_train = pd.DataFrame(imputed_data_train, columns=X_train.columns)\n",
    "#     imputed_data_test = pd.DataFrame(imputed_data_test, columns=X_test.columns)\n",
    "    \n",
    "    # debug\n",
    "#     print(imputed_data_test.shape)\n",
    "#     print(imputed_data_test.shape)\n",
    "\n",
    "\n",
    "    # Creating and training the model (using Logistic Regression as an example)\n",
    "    \n",
    "    model.fit(imputed_data_train, y_train)\n",
    "\n",
    "    # Making predictions on the training and testing sets\n",
    "    y_train_pred = model.predict(imputed_data_train)\n",
    "    y_test_pred = model.predict(imputed_data_test)\n",
    "    \n",
    "    return y_train, y_train_pred, y_test, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e56b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c3c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cv = KFold(n_splits=3, shuffle=True)\n",
    "# cv = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "\n",
    "cv_scores_train = []\n",
    "cv_scores_test = []\n",
    "# for train_index, test_index in cv.split(data, rat_numbers):\n",
    "for train_index, test_index in cv.split(data):\n",
    "    X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    y_train, y_train_pred, y_test, y_test_pred = one_fold(X_train, X_test, y_train, y_test)\n",
    "    scores_train = eval_model(y_train, y_train_pred)\n",
    "    scores_test = eval_model(y_test, y_test_pred)\n",
    "    cv_scores_train.append(scores_train)\n",
    "    cv_scores_test.append(scores_test)\n",
    "# Summarize\n",
    "ret_train = pd.DataFrame(cv_scores_train).mean()\n",
    "ret_test = pd.DataFrame(cv_scores_test).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b013f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test')\n",
    "print(ret_test)\n",
    "\n",
    "print('Train')\n",
    "print(ret_train)\n",
    "\n",
    "# print('Test')\n",
    "# [[print(f'{metric}:{values}') for metric, values in ret.items()] for ret in cv_scores_test]\n",
    "# print('Train')\n",
    "# [[print(f'{metric}:{values}') for metric, values in ret.items()] for ret in cv_scores_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c461ad",
   "metadata": {},
   "outputs": [],
   "source": [
    " pd.DataFrame(cv_scores_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553787b7",
   "metadata": {},
   "source": [
    "### Impute the data and see how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe0ee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imputer = imputer_class(1000)\n",
    "imputed_data = imputer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8314c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_num = np.array(labels=='affiliative', dtype=float)*0.1\n",
    "labels_num = labels_num[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_conc = np.concatenate((data.values, labels_num), axis=1)\n",
    "plt.imshow(data_conc, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e4900",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data_concat = np.concatenate((imputed_data, labels_num), axis=1)\n",
    "plt.imshow(imputed_data_concat, cmap='hot', interpolation='nearest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996fe44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X = imputed_data\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=15).fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7b900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "strings = filenames\n",
    "label_encoder = LabelEncoder()\n",
    "converted_numbers = label_encoder.fit_transform(strings)\n",
    "\n",
    "strings = filenames\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "converted_numbers\n",
    "\n",
    "\n",
    "rat_numbers = [extract_ratnum_from_file_name(filename) for filename in filenames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4c44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(X_embedded)\n",
    "\n",
    "df_tsne['labels'] = labels\n",
    "df_tsne['filenames'] = converted_numbers\n",
    "df_tsne['rat_number'] = rat_numbers\n",
    "# df_tsne['pred'] = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72180729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dummies and store it in a variable\n",
    "dummies = pd.get_dummies(df_tsne.labels)\n",
    " \n",
    "# Concatenate the dummies to original dataframe\n",
    "merged = pd.concat([df_tsne, dummies], axis='columns')\n",
    " \n",
    "# drop the values\n",
    "merged.drop(['labels'], axis='columns')\n",
    "\n",
    "merged = merged[['rat_number','affiliative','aversive']].groupby('rat_number').sum()\n",
    "merged['sum'] = merged['affiliative'] + merged['aversive']\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c70675",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_tsne, x=0, y=1, hue='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5013d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sns.catplot(data=df_tsne, x=0, y=1, hue='rat_number', row='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d216b27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sns.catplot(data=df_tsne, x=0, y=1, hue='filenames', row='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac03970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548ec31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "all_pos = {}\n",
    "colors = []\n",
    "for r, df_r in df_tsne.groupby('rat_number'):\n",
    "\n",
    "\n",
    "    f_names = df_r['filenames'].values\n",
    "    pos_ = df_r[[0,1]].values\n",
    "    pos_2 = {(r,f): p for p, f in zip(pos_,f_names)}\n",
    "    mean_pos = np.mean(pos_,axis=0)\n",
    "    pos_2[r] = mean_pos\n",
    "    \n",
    "    labels_sub = np.array(['b']* len(df_r))\n",
    "    labels_sub[np.where(np.array(df_r['labels']=='affiliative'))] = 'g'\n",
    "    \n",
    "    all_pos.update(pos_2)\n",
    "    [G.add_edge(r, (r,f)) for f in f_names]\n",
    "    colors = np.append(colors, labels_sub)\n",
    "# colors = np.array(colors).flatten()\n",
    "fig = plt.figure(figsize=(40,80))\n",
    "# nx.draw_networkx(G,all_pos)\n",
    "nx.draw_networkx(G,all_pos, edge_color=colors, font_size=50,width=5)\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d63fbef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# G = nx.Graph()\n",
    "# all_pos = {}\n",
    "# df_g = df_tsne.groupby('rat_number')\n",
    "# n_groups = len(df_g)\n",
    "# fig = plt.figure(figsize=(40,80))\n",
    "# for n, (r, df_r) in enumerate(df_g):\n",
    "#     G = nx.Graph()\n",
    "#     plt.subplot(n_groups,1,n+1)\n",
    "\n",
    "#     f_names = df_r['filenames'].values\n",
    "#     pos_ = df_r[[0,1]].values\n",
    "#     labels_sub = np.array(['b']* len(df_r))\n",
    "#     labels_sub[np.where(np.array(df_r['labels']=='affiliative'))] = 'g'\n",
    "# #     labels_sub = np.append(labels_sub, 'y', axis=None)\n",
    "    \n",
    "    \n",
    "#     pos_2 = {(r,f): p for p, f in zip(pos_,f_names)}\n",
    "#     mean_pos = np.mean(pos_,axis=0)\n",
    "    \n",
    "#     pos_2[r] = mean_pos\n",
    "    \n",
    "#     all_pos.update(pos_2)\n",
    "#     [G.add_edge(r, (r,f)) for f in f_names]\n",
    "    \n",
    "\n",
    "#     nx.draw_networkx(G,pos_2, node_size=2000, font_size=50, edge_color=labels_sub)\n",
    "#     ax = plt.gca()\n",
    "#     ax.margins(0.08)\n",
    "# #     plt.axis(\"off\")\n",
    "#     plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
