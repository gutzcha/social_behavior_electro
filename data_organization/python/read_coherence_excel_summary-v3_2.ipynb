{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "c2cd4625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af63149",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "477dacf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Project- Electro\\\\social_behavior_electro\\\\data_organization\\\\python'"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b69cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = osp.join('..','..','analysis')\n",
    "path_to_aversive_enc_pre = osp.join(folder_path,\n",
    "                            'Population analysis results for LFP Coherence - Encounter-PreEncounter_4-12_30-80_aversive.xlsx')\n",
    "path_to_affiliative_enc_pre = osp.join(folder_path,\n",
    "                               'Population analysis results for LFP Coherence - Encounter-PreEncounter_4-12_30-80_affiliative.xlsx')\n",
    "\n",
    "path_to_aversive_post_pre = osp.join(folder_path,\n",
    "                            'Population analysis results for LFP Coherence - PostEncounter-PreEncounter_4-12_30-80_aversive.xlsx')\n",
    "path_to_affiliative_post_pre = osp.join(folder_path,\n",
    "                               'Population analysis results for LFP Coherence - PostEncounter-PreEncounter_4-12_30-80_affiliative.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "path_to_affiliative_lfp = osp.join(folder_path,\n",
    "                            'lfp_rawdata_affiliative.xlsx') \n",
    "\n",
    "path_to_aversive_lfp = osp.join(folder_path,\n",
    "                            'lfp_rawdata_aversive.xlsx')\n",
    "\n",
    "path_to_affiliative_spikes = osp.join(folder_path,\n",
    "                            'spikes_rawdata_affiliative.xlsx') \n",
    "\n",
    "path_to_aversive_spikes = osp.join(folder_path,\n",
    "                            'spikes_rawdata_aversive.xlsx')\n",
    "\n",
    "\n",
    "path_to_common_areas = osp.join(folder_path, 'common_area_pairs_w5_sessions.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00eb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aff_lfp_raw = pd.read_excel(path_to_affiliative_lfp ,header=[0,1,2], index_col=[0])\n",
    "df_avv_lfp_raw = pd.read_excel(path_to_aversive_lfp ,header=[0,1,2], index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aff_spikes_raw = pd.read_excel(path_to_affiliative_spikes ,header=[0,1], index_col=[0])\n",
    "df_avv_spikes_raw = pd.read_excel(path_to_aversive_spikes ,header=[0,1], index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4fabc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aff_enc_pre = pd.read_excel(path_to_affiliative_enc_pre,None)\n",
    "df_avv_enc_pre = pd.read_excel(path_to_aversive_enc_pre,None)\n",
    "\n",
    "df_aff_post_pre = pd.read_excel(path_to_affiliative_post_pre,None)\n",
    "df_avv_post_pre = pd.read_excel(path_to_aversive_post_pre,None)\n",
    "\n",
    "\n",
    "df_files_avv_bad = pd.read_excel(path_to_aversive_enc_pre,'Uploaded files', header=None)\n",
    "df_files_aff_bad = pd.read_excel(path_to_affiliative_enc_pre,'Uploaded files', header=None)\n",
    "\n",
    "df_areas = pd.read_excel(path_to_common_areas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba353773",
   "metadata": {},
   "source": [
    "## Clean and combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "dc6e2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def fix_file_paths(df_files):\n",
    "    \n",
    "    df_files = df_files.rename(columns={df_files.columns[1]:'timestamps', df_files.columns[0]:'lfp', })\n",
    "    \n",
    "    return df_files\n",
    "df_files_avv = fix_file_paths(df_files_avv_bad)['lfp']\n",
    "df_files_aff = fix_file_paths(df_files_aff_bad)['lfp']\n",
    "\n",
    "\n",
    "def extract_ratnum_from_file_name(filename):\n",
    "    filename = filename.replace(' ','')\n",
    "    match = re.search(r\"rat(\\d{1,2})\", filename.lower())\n",
    "\n",
    "    if match:\n",
    "        number = int(match.group(1))       \n",
    "    else:\n",
    "        number = -1\n",
    "    return number\n",
    "\n",
    "def extract_daynum_from_file_name(filename):\n",
    "    filename = filename.replace(' ','')\n",
    "    match = re.search(r\"day(\\d{1,2})\", filename.lower())\n",
    "    if match:\n",
    "        number = int(match.group(1))       \n",
    "    else:\n",
    "        number = -1\n",
    "    return number\n",
    "# rat_numbers = [extract_ratnum_from_file_name(filename) for filename in filenames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_num_to_name_map = {\n",
    "    '111': 'MeD',\n",
    "    '2': 'MePV',\n",
    "    '13':'CeA',\n",
    "    '112': 'BMA',\n",
    "    '14': 'AA',\n",
    "    '16': 'EA',\n",
    "    '12': 'STIA',\n",
    "    '15': 'VP'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def reformat_columns(df):\n",
    "    # combine column names and remove levels\n",
    "    df_new = pd.DataFrame()\n",
    "    areas = df.columns.get_level_values('area').unique()\n",
    "    freqs = df.columns.get_level_values('freq').unique()\n",
    "    stages = df.columns.get_level_values('stage').unique()\n",
    "\n",
    "    df_new.index = df.index\n",
    "    for area in areas:\n",
    "        for freq in freqs:\n",
    "            for stage in stages:\n",
    "\n",
    "                col_name = (area, freq, stage)\n",
    "                \n",
    "                area_name = area_num_to_name_map[area]\n",
    "                if \"diffduring\" in stage.lower():\n",
    "                    new_stage = 'enc_pre_lfp'\n",
    "                elif\"diffafter\" in stage.lower():\n",
    "                    new_stage = 'post_pre_lfp'\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                new_freq = freq.replace('-','_') + 'Hz'\n",
    "                \n",
    "                \n",
    "                new_col_name = '_'.join((area_name, new_freq, new_stage))\n",
    "\n",
    "                df_new[new_col_name] = df[col_name]\n",
    "    return df_new\n",
    "\n",
    "\n",
    "SPIKE_DATA_TYPE_SRC = 'diff'\n",
    "SPIKE_DATA_TYPE_TARGET = 'spike_diff'\n",
    "\n",
    "SPIKE_DATA_TYPE_SRC = 'ratio'\n",
    "SPIKE_DATA_TYPE_TARGET = 'spike_ratio'\n",
    "\n",
    "def reformat_columns_spikes(df):\n",
    "    # combine column names and remove levels\n",
    "    df_new = pd.DataFrame()\n",
    "    areas = df.columns.get_level_values('area').unique()\n",
    "    stages = df.columns.get_level_values('stage').unique()\n",
    "\n",
    "    df_new.index = df.index\n",
    "    for area in areas:\n",
    "        for stage in stages:\n",
    "\n",
    "            col_name = (area, stage)\n",
    "\n",
    "            area_name = area_num_to_name_map[area]\n",
    "#             if \"diffduring\" in stage.lower():\n",
    "#                 new_stage = 'enc_pre_spike_diff'\n",
    "#             elif\"diffafter\" in stage.lower():\n",
    "#                 new_stage = 'post_pre_spike_diff'\n",
    "            if SPIKE_DATA_TYPE_SRC+\"during\" in stage.lower():\n",
    "                new_stage = 'enc_pre_' + SPIKE_DATA_TYPE_TARGET\n",
    "            elif SPIKE_DATA_TYPE_SRC+\"after\" in stage.lower():\n",
    "                new_stage = 'post_pre_spike_ratio'\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            new_col_name = '_'.join((area_name, new_stage))\n",
    "\n",
    "            df_new[new_col_name] = df[col_name]\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff2369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aff_lfp = reformat_columns(df_aff_lfp_raw)\n",
    "\n",
    "df_avv_lfp = reformat_columns(df_avv_lfp_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aff_spikes = reformat_columns_spikes(df_aff_spikes_raw)\n",
    "df_avv_spikes = reformat_columns_spikes(df_avv_spikes_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff9154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f02ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add variables befor concatination\n",
    "df_aff_enc_pre = pd.read_excel(path_to_affiliative_enc_pre,None)\n",
    "# df_aff_enc_pre['sociability'] = 'affiliative'\n",
    "# df_aff_enc_pre['stage'] = 'enc_pre'\n",
    "\n",
    "df_aff_post_pre = pd.read_excel(path_to_affiliative_post_pre,None)\n",
    "# df_aff_post_pre['sociability'] = 'affiliative'\n",
    "# df_aff_post_pre['stage'] = 'post_pre'\n",
    "\n",
    "\n",
    "df_avv_enc_pre = pd.read_excel(path_to_aversive_enc_pre,None)\n",
    "# df_avv_enc_pre['sociability'] = 'aversive'\n",
    "# df_avv_enc_pre['stage'] = 'enc_pre'\n",
    "\n",
    "\n",
    "df_avv_post_pre = pd.read_excel(path_to_aversive_post_pre,None)\n",
    "# df_avv_post_pre['sociability'] = 'aversive'\n",
    "# df_avv_post_pre['stage'] = 'post_pre'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6936c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_keywords = {'First': '4_18Hz', 'Second':'30_80Hz'}\n",
    "stage_keywords = {'During':'enc_pre', 'After':'post_pre'}\n",
    "substrings_to_remove = ['During', 'After','Before', 'First', 'Second']\n",
    "ignore_keyworks = ['Norm', 'files']\n",
    "\n",
    "def remove_columns_with_fewer_values(df, N=5):\n",
    "    # Get the count of non-null values in each column\n",
    "    column_counts = df.count()\n",
    "\n",
    "    # Filter columns based on count condition\n",
    "    columns_to_remove = column_counts[column_counts < N].index\n",
    "\n",
    "    # Drop the columns from the DataFrame\n",
    "    updated_df = df.drop(columns=columns_to_remove)\n",
    "\n",
    "    return updated_df\n",
    "\n",
    "def reformat_dict_to_table(df_dict, file_df):\n",
    "# def reformat_dict_to_table(df_dict, freq_keywords, stage_keywords, ignore_keyworks, substrings_to_remove):\n",
    "    ret_df_list = []\n",
    "    for sheet, df in df_dict.items():\n",
    "        df = df.copy()\n",
    "        # ignore first and last sheets that contain a summary and list of file names\n",
    "        if any(substring in sheet for substring in ignore_keyworks): \n",
    "            continue\n",
    "            \n",
    "        if isinstance(df, str):\n",
    "            print(df)\n",
    "            continue\n",
    "            \n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "        \n",
    "         # Remove all substrings to keep just the area name\n",
    "        area_name = sheet\n",
    "        for sub_string_to_remove in substrings_to_remove:\n",
    "            area_name = area_name.replace(sub_string_to_remove,'')\n",
    "        \n",
    "        for fk, freq in  freq_keywords.items():\n",
    "            if fk in sheet:\n",
    "                this_freq = freq_keywords[fk]\n",
    "            for sk in stage_keywords.keys():\n",
    "                if sk in sheet:\n",
    "                    this_stage = stage_keywords[sk]\n",
    "        \n",
    "                # Rename all column names to match\n",
    "        \n",
    "        for col in df.columns:\n",
    "            area_pair = [col, area_name]\n",
    "            area_pair.sort()\n",
    "            \n",
    "            df.rename(columns={col:f'{area_pair[0]}_{area_pair[1]}_{this_freq}_{this_stage}'}, inplace=True)\n",
    "        \n",
    "        \n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "        df = remove_columns_with_fewer_values(df)\n",
    "        ret_df_list.append(df)\n",
    "        \n",
    "    df_ret = pd.concat(ret_df_list,axis=1)\n",
    "    df_ret['files'] = file_df.values\n",
    "    return df_ret\n",
    "                \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "3d1eac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_aff_enc_pre_rectified = reformat_dict_to_table(df_aff_enc_pre, df_files_aff)\n",
    "# df_aff_enc_post_rectified = reformat_dict_to_table(df_aff_post_pre, df_files_aff)\n",
    "# df_aff = pd.concat([df_aff_enc_pre_rectified.drop('files', axis=1), df_aff_enc_post_rectified], axis=1)\n",
    "# df_aff = pd.concat([df_aff, df_aff_lfp.reset_index(drop=True)], axis=1)\n",
    "# df_aff['sociability'] = 'affiliative'\n",
    "\n",
    "\n",
    "\n",
    "# df_avv_enc_pre_rectified = reformat_dict_to_table(df_avv_enc_pre, df_files_avv)\n",
    "# df_avv_enc_post_rectified = reformat_dict_to_table(df_avv_post_pre, df_files_avv)\n",
    "# df_avv = pd.concat([df_avv_enc_pre_rectified.drop('files', axis=1), df_avv_enc_post_rectified], axis=1)\n",
    "# df_avv = pd.concat([df_avv, df_avv_lfp.reset_index(drop=True)], axis=1)\n",
    "# df_avv['sociability'] = 'aversive'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aff_enc_pre_rectified = reformat_dict_to_table(df_aff_enc_pre, df_files_aff)\n",
    "df_aff_enc_post_rectified = reformat_dict_to_table(df_aff_post_pre, df_files_aff)\n",
    "df_aff = pd.concat([\n",
    "    df_aff_enc_pre_rectified.set_index('files'),\n",
    "    df_aff_enc_post_rectified.set_index('files'),\n",
    "    df_aff_lfp,\n",
    "    df_aff_spikes\n",
    "],\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "df_aff['sociability'] = 'affiliative'\n",
    "\n",
    "\n",
    "df_avv_enc_pre_rectified = reformat_dict_to_table(df_avv_enc_pre, df_files_avv)\n",
    "df_avv_enc_post_rectified = reformat_dict_to_table(df_avv_post_pre, df_files_avv)\n",
    "df_avv = pd.concat([\n",
    "    df_avv_enc_pre_rectified.set_index('files'),\n",
    "    df_avv_enc_post_rectified.set_index('files'),\n",
    "    df_avv_lfp,\n",
    "    df_avv_spikes\n",
    "], axis=1)\n",
    "\n",
    "df_avv['sociability'] = 'aversive'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecfb8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d54a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avv_cols = df_avv.columns\n",
    "df_aff_cols = df_aff.columns\n",
    "common_cols = set(df_aff_cols).intersection(set(df_avv_cols))\n",
    "df_aff = df_aff[common_cols]\n",
    "df_avv = df_avv[common_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837bf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0563f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([df_aff, df_avv])\n",
    "filenames = all_df.index.to_list()\n",
    "all_df['rat_number'] = pd.Categorical([extract_ratnum_from_file_name(r) for r in filenames])\n",
    "all_df['day_number'] = pd.Categorical([extract_daynum_from_file_name(r) for r in filenames])\n",
    "# all_df = all_df.set_index('files')\n",
    "# all_df.to_excel(osp.join(folder_path, 'combined_data.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c68f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df['sociability'] = np.random.permutation(all_df['sociability'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c548a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b84959",
   "metadata": {},
   "source": [
    "## Preliminary feature selection via statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedcd099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, StratifiedGroupKFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "############################\n",
    "import sklearn.neighbors._base\n",
    "import sys\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "from missingpy import MissForest\n",
    "############################\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.special import logit\n",
    "\n",
    "\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b656778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix column names\n",
    "# change 4-12 to 4-18\n",
    "def fix_range_name(s):\n",
    "    return s.replace('4_12','4_18')\n",
    "# if lfp is in the name, put it at the beggining\n",
    "# if spike_ratio is in the name, put it at the beginning\n",
    "def move_suffix_to_apx(s, sfx):\n",
    "    if sfx in s:\n",
    "        s = s.replace('_'+sfx, '')\n",
    "        s = sfx+'_'+s\n",
    "    return s\n",
    "\n",
    "# if there is not lfp or skipe ration in the name, add coherence at the befinning\n",
    "def add_cohherence(s):\n",
    "    if 'lfp' not in s and 'spike' not in s and ('enc' in s or 'post' in s):\n",
    "        s = 'coherence_' + s\n",
    "    return s\n",
    "\n",
    "# add number to catergorical variables to ensure that they are first\n",
    "def add_number(s):\n",
    "    if 'sociability' in s:\n",
    "        s = '0_' + s\n",
    "        \n",
    "    elif 'rat_number' in s:\n",
    "        s = '1_' + s\n",
    "        \n",
    "    elif 'day_number' in s:\n",
    "        s = '2_' + s\n",
    "    return s\n",
    "\n",
    "def add_all_fix_functions(s):\n",
    "    s = fix_range_name(s)\n",
    "    s = move_suffix_to_apx(s, 'lfp')\n",
    "    s = move_suffix_to_apx(s, SPIKE_DATA_TYPE_TARGET)\n",
    "    s = add_cohherence(s)\n",
    "#     s = add_number(s)\n",
    "    return s\n",
    "\n",
    "all_df.rename(columns={c:add_all_fix_functions(c) for c in all_df.columns} , inplace=True)\n",
    "all_df = all_df.reindex(sorted(all_df.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "780c3f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267a324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars  = ['sociability','rat_number','day_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422da86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "def plot_correlation_heat_map(df):\n",
    "    corr_data = df.corr()\n",
    "    a4_dims = (11.7, 8.27)\n",
    "    fig, ax = plt.subplots(figsize=a4_dims)\n",
    "    g = sns.heatmap(corr_data, ax=ax)\n",
    "    return\n",
    "\n",
    "def mark_nans_with_x(correlation_matrix, ax):\n",
    "#     correlation_matrix = correlation_matrix.drop(cat_vars, axis = 1)\n",
    "    # Step 3: Extract variables with all NaN values\n",
    "    nan_variables = correlation_matrix.columns[correlation_matrix.isnull().all()]\n",
    "\n",
    "    # Step 4: Mark relevant squares with NaN values in the heat map\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(len(correlation_matrix.columns)):\n",
    "            if correlation_matrix.iloc[i, j] != 1.0 and pd.isnull(correlation_matrix.iloc[i, j]):\n",
    "                ax.text(j + 0.5, i + 0.5, 'X', ha='center', va='center', color='red', fontsize=16)\n",
    "    return\n",
    "    \n",
    "# def plot_correlation_heat_map_sidebyside(df):\n",
    "#     a4_dims = (11.7, 8.27)\n",
    "\n",
    "#     df1 = df.loc[df['sociability']=='aversive']\n",
    "#     df2 = df.loc[df['sociability']=='affiliative']\n",
    "#     fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=a4_dims)\n",
    "\n",
    "#     corr_data1 = df1.corr()\n",
    "#     print(corr_data1.columns)\n",
    "#     g = sns.heatmap(corr_data1, ax=ax1, vmin=-1, vmax=1)\n",
    "\n",
    "#     corr_data2 = df2.corr()\n",
    "#     g = sns.heatmap(corr_data2, ax=ax2, vmin=-1, vmax=1)\n",
    "\n",
    "#     plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.7, hspace=None)\n",
    "#     mark_nans_with_x(corr_data1, ax1)\n",
    "#     mark_nans_with_x(corr_data2, ax2)\n",
    "    \n",
    "\n",
    "#     return\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def plot_correlation_heat_map_sidebyside(df):\n",
    "    # Convert numeric columns to float if needed\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].astype(float)\n",
    "\n",
    "    # Step 1: Split the dataframe based on the value of \"sociability\"\n",
    "    df_low_sociability = df[df[\"sociability\"] == \"affiliative\"]\n",
    "    df_high_sociability = df[df[\"sociability\"] == \"aversive\"]\n",
    "\n",
    "    # Step 2: Extract correlation matrices\n",
    "    corr_low_sociability = df_low_sociability.corr()\n",
    "    corr_high_sociability = df_high_sociability.corr()\n",
    "\n",
    "    # Step 3: Create heat map plot side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    sns.heatmap(corr_low_sociability, annot=False, cmap='coolwarm', vmin=-1, vmax=1, ax=ax1)\n",
    "    sns.heatmap(corr_high_sociability, annot=False, cmap='coolwarm', vmin=-1, vmax=1, ax=ax2)\n",
    "\n",
    "    # Step 4: Mark relevant squares with NaN values in the heat maps\n",
    "    for ax, corr_matrix in zip([ax1, ax2], [corr_low_sociability, corr_high_sociability]):\n",
    "        nan_matrix = corr_matrix.isnull().astype(int)\n",
    "        for i in range(len(corr_matrix.index)):\n",
    "            for j in range(len(corr_matrix.columns)):\n",
    "                if nan_matrix.iloc[i, j] == 1:\n",
    "                    ax.text(j + 0.5, i + 0.5, 'X', ha='center', va='center', color='red', fontsize=16)\n",
    "\n",
    "    # Step 5: Extract and mark cells with significant difference\n",
    "    for i in range(len(corr_low_sociability.index)):\n",
    "        for j in range(len(corr_low_sociability.columns)):\n",
    "            if i != j:\n",
    "#                 low_val = corr_low_sociability.iloc[i, j]\n",
    "#                 high_val = corr_high_sociability.iloc[i, j]\n",
    "                \n",
    "                low_col = pd.to_numeric(df_low_sociability.iloc[:, i], errors='coerce')\n",
    "                high_col = pd.to_numeric(df_high_sociability.iloc[:, j], errors='coerce')\n",
    "                \n",
    "                len_high= np.sum(~np.isnan(high_col.values))\n",
    "                len_low= np.sum(~np.isnan(low_col.values))\n",
    "                \n",
    "                if len_high<MIN_N_SESSIONS or len_low<MIN_N_SESSIONS:\n",
    "                    p_value = 1\n",
    "#                     print(f'len_high: {len_high}, len_high: {len_low}')\n",
    "                else:\n",
    "#                     _, p_value = mannwhitneyu(low_col, high_col, alternative='two-sided')\n",
    "                    p_value = ttest_ind(low_col, high_col, nan_policy='omit').pvalue\n",
    "                if p_value < 0.05:\n",
    "                    ax1.text(j + 0.5, i + 0.5, 'O', ha='center', va='center', color='blue', fontsize=16)\n",
    "                    ax2.text(j + 0.5, i + 0.5, 'O', ha='center', va='center', color='blue', fontsize=16)\n",
    "#                 print(f'i,j:({i},{j})')\n",
    "    \n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.7, hspace=None)\n",
    "    \n",
    "\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311da094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spike_data = all_df[[c for c in all_df.columns if 'spike' in c or c in cat_vars]]\n",
    "df_lfp_data = all_df[[c for c in all_df.columns if 'lfp' in c or c in cat_vars]]\n",
    "df_coherence_data = all_df[[c for c in all_df.columns if 'coherence' in c or c in cat_vars]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "c65db0d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_correlation_heat_map(all_df)\n",
    "# plot_correlation_heat_map(df_spike_data)\n",
    "# plot_correlation_heat_map(df_lfp_data)\n",
    "# plot_correlation_heat_map(df_coherence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "2392f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spike_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "5554a9c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_correlation_heat_map_sidebyside(df_spike_data)\n",
    "# plot_correlation_heat_map_sidebyside(df_lfp_data)\n",
    "# plot_correlation_heat_map_sidebyside(df_coherence_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "9bcf7799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_coherence_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract values\n",
    "# # all_df = all_df.sample(frac = 1)\n",
    "# df_values = all_df.drop('sociability',axis=1).values\n",
    "# y = all_df['sociability'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "e9e38af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN EDA HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "53ec006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03579ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d613b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def transform_var(func_handle, X_train,X_test, method='reuse', func_parameters=None):\n",
    "    if func_parameters is None:\n",
    "        func_train = func_handle()\n",
    "    else:\n",
    "        func_train = func_handle(**func_parameters)\n",
    "            \n",
    "    X_train_trans = func_train.fit_transform(X_train)\n",
    "    \n",
    "    if method == 'reuse':\n",
    "        X_test_trans = func_train.transform(X_test)\n",
    "        func_test = func_train\n",
    "    # create new instance and refit on test data\n",
    "    elif method == 'new':\n",
    "        if func_parameters is None:\n",
    "            func_test = func_handle()\n",
    "        else:\n",
    "            func_test = func_handle(**func_parameters)\n",
    "        X_test_trans = func_test.fit_transform(X_test)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input value. The value must be either 'reuse' or 'new'.\")\n",
    "    return X_train_trans, X_test_trans, func_train, func_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18344cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(y_test, y_test_pred):\n",
    "#     print(f'y:{y_test}')\n",
    "#     print(f'y_pred:{y_test_pred}')\n",
    "    # Calculating evaluation metrics on the testing set\n",
    "    ret_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    ret_precision = precision_score(y_test, y_test_pred, pos_label=\"affiliative\")\n",
    "    ret_recall = recall_score(y_test, y_test_pred,pos_label=\"affiliative\")\n",
    "    ret_f1 = f1_score(y_test, y_test_pred,pos_label=\"affiliative\")\n",
    "#     ret_roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "    ret = {\n",
    "        'accuracy':np.round(ret_accuracy, 3),\n",
    "        'precision':np.round(ret_precision, 3),\n",
    "        'recall':np.round(ret_recall, 3),\n",
    "        'f1':np.round(ret_f1, 3),\n",
    "    }\n",
    "    \n",
    "\n",
    "    return ret\n",
    "\n",
    "def sum_model_results(y, y_pred, \n",
    "                      confidence_levels, affiliative,\n",
    "                      filenames,\n",
    "                      dataset):\n",
    "    df_results = pd.DataFrame()\n",
    "    df_results['GT'] = y\n",
    "    df_results['predicted'] = y_pred\n",
    "    df_results['correct'] = df_results['GT'] == df_results['predicted']\n",
    "    df_results['confidence'] = confidence_levels\n",
    "    df_results['affiliative_level'] = affiliative\n",
    "    df_results['filenames'] = filenames\n",
    "\n",
    "    return df_results\n",
    "\n",
    "def sum_all_results(y_train, y_train_pred, y_test, y_test_pred, **kwargs):\n",
    "    \n",
    "    ret_train = eval_model(y_train,y_train_pred)\n",
    "    ret_test = eval_model(y_test,y_test_pred)\n",
    "    df_res_all = pd.DataFrame.from_dict([ret_test, ret_train])\n",
    "    df_res_all.index = ['test','train']\n",
    "    \n",
    "    for param, vals in kwargs.items():\n",
    "        try:\n",
    "            df_res_all[param] = [vals]*2\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "    return df_res_all\n",
    "\n",
    "\n",
    "def train_eval_model(config_dict,\n",
    "                     X_train, y_train, X_test, y_test,\n",
    "                     train_file_names, test_file_names,\n",
    "                     method_name, imputer_name, scaler_name, classifier_name,\n",
    "                     k_fold, ind, iteration):\n",
    "    \n",
    "#     print('I am here')\n",
    "    scaler = config_dict['scaler'][scaler_name]\n",
    "    imputer_class = config_dict['imputer'][imputer_name]\n",
    "    \n",
    "    if ('imputer_param' in config_dict) and (imputer_name in config_dict['imputer_param']):\n",
    "        imputer_param = config_dict['imputer_param'][imputer_name]\n",
    "    else:\n",
    "        imputer_param = None\n",
    "    \n",
    "    classifier_class = config_dict['model'][classifier_name]\n",
    "\n",
    "    \n",
    "    # impute\n",
    "    \n",
    "    X_train_imputed, X_test_imputed, imputer_train, imputer_test = transform_var(\n",
    "        imputer_class, X_train, X_test, method_name, imputer_param);\n",
    "\n",
    "    # ==================== impute each class seperatly =================\n",
    "#     X_train_aff = X_train[y_train=='affiliative']\n",
    "#     X_test_aff = X_test[y_test=='affiliative']\n",
    "#     X_train_avv = X_train[y_train=='aversive']\n",
    "#     X_test_avv = X_test[y_test=='aversive']\n",
    "    \n",
    "#     print(f'size of X_train_aff :{X_train_aff.shape}')\n",
    "#     print(f'size of X_test_aff :{X_test_aff.shape}')\n",
    "#     print(f'size of X_train_avv :{X_train_avv.shape}')\n",
    "#     print(f'size of X_test_avv :{X_test_avv.shape}')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     X_train_aff_imputed, X_test_aff_imputed, imputer_train, imputer_test = transform_var(\n",
    "#         imputer_class, X_train_aff, X_test_aff, method_name, imputer_param);\n",
    "    \n",
    "#     X_train_avv_imputed, X_test_avv_imputed, imputer_train, imputer_test = transform_var(\n",
    "#         imputer_class, X_train_avv, X_test_avv, method_name, imputer_param);\n",
    "    \n",
    "#     X_train_imputed = np.concatenate((X_train_aff_imputed,X_train_avv_imputed), axis=0)\n",
    "#     X_test_imputed = np.concatenate((X_test_aff_imputed,X_test_avv_imputed), axis=0)\n",
    "    # ==================== impute each class seperatly (end) =================\n",
    "\n",
    "\n",
    "    # scale\n",
    "    X_train_scaled, X_test_scaled, scaler_train, scaler_test= transform_var(\n",
    "        imputer_class, X_train_imputed, X_test_imputed, method_name)\n",
    "\n",
    "    # train classifier\n",
    "    if ('model_param' in config_dict) and (classifier_name in config_dict['model_param']):\n",
    "        model = classifier_class(**config_dict['model_param'][classifier_name])\n",
    "    else: \n",
    "        model = classifier_class()\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # predict\n",
    "\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    y_test_pred_prob = model.predict_proba(X_test_scaled)\n",
    "    y_train_pred_prob = model.predict_proba(X_train_scaled)\n",
    "\n",
    "    test_confidence_levels = y_test_pred_prob.max(axis=1)\n",
    "    train_confidence_levels = y_train_pred_prob.max(axis=1)\n",
    "\n",
    "    test_affiliative = y_test_pred_prob[:,0]\n",
    "    train_affiliative = y_train_pred_prob[:,0]\n",
    "\n",
    "    sum_results = sum_all_results(\n",
    "        y_train, y_train_pred, y_test, y_test_pred,\n",
    "        method = method_name,\n",
    "        imputer= imputer_name,\n",
    "        scaler = scaler_name, \n",
    "        model = classifier_name,\n",
    "        k_fold = k_fold,\n",
    "        ind = ind,\n",
    "        trained_model = model,\n",
    "        trained_imputer = imputer_train,\n",
    "        trained_scalar = scaler_train,\n",
    "        iteration = iteration\n",
    "        \n",
    "    )\n",
    "\n",
    "    df_results_train = sum_model_results(\n",
    "        y_train, y_train_pred,train_confidence_levels, train_affiliative,train_file_names ,dataset='train')\n",
    "    df_results_train['dataset'] = 'train'\n",
    "    df_results_test = sum_model_results(y_test, y_test_pred,test_confidence_levels, test_affiliative, test_file_names ,dataset='test')\n",
    "    df_results_test['dataset'] = 'test'\n",
    "    df_results_all = pd.concat([df_results_test,df_results_train])\n",
    "    df_results_all\n",
    "    \n",
    "    df_results_all['method'] = method_name\n",
    "    df_results_all['imputer'] = imputer_name\n",
    "    df_results_all['scaler'] = scaler_name\n",
    "    df_results_all['model'] = classifier_name\n",
    "    df_results_all['k_fold'] = k_fold\n",
    "    df_results_all['ind'] = ind\n",
    "    df_results_all['iteration'] = iteration\n",
    "    \n",
    "\n",
    "    return sum_results, df_results_all, model, imputer_train\n",
    "\n",
    "def model_selection(config_dict, X, y, subject_id, filenames, n_iterations,verbose=True):\n",
    "    all_results_summary = []\n",
    "    all_raw_results = []\n",
    "    errors_log = []\n",
    "    models= []\n",
    "    imputers = []\n",
    "    for iteration in tqdm(range(n_iterations), disable=n_iterations<10):          \n",
    "        for k_fold in config_dict['k_fold']:\n",
    "#             print(f'k_fold: {k_fold}')\n",
    "            stratified_group_kfold = StratifiedGroupKFold(n_splits=k_fold, shuffle=True)\n",
    "            for ind, (train_index, test_index) in enumerate(stratified_group_kfold.split(X, y, groups=subject_id)):\n",
    "#             stratified_group_kfold = StratifiedKFold(n_splits=k_fold, shuffle=True)        \n",
    "#             for ind, (train_index, test_index) in enumerate(stratified_group_kfold.split(X, y)):\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                train_file_names, test_file_names = filenames[train_index], filenames[test_index]\n",
    "                # mamke sure that there is nan columns:\n",
    "                X_train_nan_flag = np.any(np.all(np.isnan(X_train), axis=0))\n",
    "                X_test_nan_flag = np.any(np.all(np.isnan(X_test), axis=0))\n",
    "                if X_train_nan_flag or X_test_nan_flag:\n",
    "#                     if (X_train_nan_flag):\n",
    "#                         print(f'Detected nan columns: {np.isnan(X_train)}')\n",
    "#                     if (X_test_nan_flag):\n",
    "#                         print(f'Detected nan columns: {np.isnan(X_test)}')\n",
    "                                                       \n",
    "                                                       \n",
    "                    continue\n",
    "\n",
    "                for method_name in config_dict['method']:\n",
    "                    for imputer_name in config_dict['imputer'].keys():\n",
    "                        for scaler_name in config_dict['scaler'].keys():\n",
    "                            for classifier_name in config_dict['model'].keys():\n",
    "\n",
    "#                                 try:\n",
    "                                sum_results, raw_results, model, imputer = train_eval_model(\n",
    "                                    config_dict,\n",
    "                                    X_train, y_train, X_test, y_test,\n",
    "                                    train_file_names, test_file_names,\n",
    "                                    method_name,\n",
    "                                    imputer_name,\n",
    "                                    scaler_name,\n",
    "                                    classifier_name,\n",
    "                                    k_fold, ind, iteration)\n",
    "                                all_results_summary.append(sum_results)\n",
    "                                all_raw_results.append(raw_results)\n",
    "#                                     models.append(model)\n",
    "#                                     imputers.append(imputer)\n",
    "#                                 except Exception as e:\n",
    "#                                     traceback.print_exc()\n",
    "#                                     error_df = dict()\n",
    "#                                     error_df['method'] = method_name\n",
    "#                                     error_df['imputer'] = imputer_name\n",
    "#                                     error_df['scaler'] = scaler_name\n",
    "#                                     error_df['model'] = classifier_name\n",
    "#                                     error_df['error'] = e\n",
    "#                                     error_df['iteration'] = iteration\n",
    "#                                     errors_log.append(error_df)\n",
    "#                                     continue\n",
    "                                \n",
    "    return all_results_summary, all_raw_results, errors_log,models, imputers\n",
    "                        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imputer_class = MissForest\n",
    "# classifier_class = RandomForestClassifier\n",
    "# method = 'reuse'\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_values, y, test_size=0.33)\n",
    "# # impute\n",
    "# X_train_imputed, X_test_imputed, imputer_train, imputer_test = transform_var(imputer_class, X_train, X_test, method)\n",
    "\n",
    "# # scale\n",
    "# X_train_scaled, X_test_scaled, scaler_train, scaler_test= transform_var(imputer_class, X_train_imputed, X_test_imputed, method)\n",
    "\n",
    "# # train classifier\n",
    "# model = classifier_class()\n",
    "# model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # predict\n",
    "# y_train_pred = model.predict(X_train_scaled)\n",
    "# y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# y_test_pred_prob = model.predict_proba(X_test_scaled)\n",
    "# y_train_pred_prob = model.predict_proba(X_train_scaled)\n",
    "\n",
    "# test_confidence_levels = y_test_pred_prob.max(axis=1)\n",
    "# train_confidence_levels = y_train_pred_prob.max(axis=1)\n",
    "\n",
    "# test_affiliative = y_test_pred_prob[:,0]\n",
    "# train_affiliative = y_train_pred_prob[:,0]\n",
    "\n",
    "# sum_results = sum_all_results(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "# df_results_train = sum_model_results(y_train, y_train_pred,train_confidence_levels, train_affiliative, dataset='train')\n",
    "# df_results_train['dataset'] = 'train'\n",
    "# df_results_test = sum_model_results(y_test, y_test_pred,test_confidence_levels, test_affiliative ,dataset='test')\n",
    "# df_results_test['dataset'] = 'test'\n",
    "# df_results_all = pd.concat([df_results_test,df_results_train])\n",
    "# df_results_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d6e37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# decomposer = PCA(n_components=2)\n",
    "# # decomposer = TSNE(n_components=2, perplexity=10)\n",
    "\n",
    "\n",
    "# X_train_decomp = decomposer.fit_transform(X_train_standard)\n",
    "\n",
    "\n",
    "# # decomposer = PCA(n_components=2)\n",
    "# # X_test_decomp = decomposer.fit_transform(X_test_standard)\n",
    "# X_test_decomp = decomposer.transform(X_test_standard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4c6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_train = pd.DataFrame()\n",
    "# res_train[['comp1','comp2']] = X_train_decomp\n",
    "# res_train['dataset'] = 'train'\n",
    "# res_train['GT'] = y_train\n",
    "\n",
    "# res_test = pd.DataFrame()\n",
    "# res_test[['comp1','comp2']] = X_test_decomp\n",
    "# res_test['dataset'] = 'test'\n",
    "# res_test['GT'] = y_test\n",
    "# res_all = pd.concat([res_train,res_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "e748820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.catplot(data=res_all, x='comp1', y='comp2', col='dataset', hue='GT')\n",
    "# g.set(xticklabels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "6a809f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad295ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_N_SESSIONS = 5\n",
    "\n",
    "def variable_significance_table(df):\n",
    "    significance_table = pd.DataFrame(columns=[\"Variable\", \"Statistic\", \"p-value\"])\n",
    "\n",
    "    predictor = df[\"sociability\"]\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column != \"sociability\":\n",
    "            \n",
    "            variable = df[column].values\n",
    "            \n",
    "            variable_aff = variable[np.where(predictor=='affiliative')]\n",
    "            variable_avv = variable[np.where(predictor=='aversive')]\n",
    "            \n",
    "            \n",
    "            variable_aff = variable_aff[~pd.isnull(variable_aff)]\n",
    "            variable_avv = variable_avv[~pd.isnull(variable_avv)]\n",
    "            \n",
    "            if len(variable_aff)<MIN_N_SESSIONS or len(variable_avv)< MIN_N_SESSIONS:\n",
    "                continue\n",
    "\n",
    "            statistic, p_value = mannwhitneyu(variable_avv, variable_aff)\n",
    "\n",
    "            significance_table = significance_table.append(\n",
    "                {\"Variable\": column, \"Statistic\": statistic, \"p-value\": p_value},\n",
    "                ignore_index=True\n",
    "            )\n",
    "    significance_table = significance_table.sort_values('p-value')\n",
    "    return significance_table\n",
    "\n",
    "\n",
    "sig_table_all = variable_significance_table(df=all_df)\n",
    "\n",
    "# sig_table_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sig_table_all.head(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2fa0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e35ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sig_cut_off = 0.1\n",
    "# sig_table = sig_table_all\n",
    "# sig_table_filt = sig_table.loc[sig_table['p-value']<sig_cut_off]\n",
    "# n_var_start = 3\n",
    "# n_params = len(sig_table_filt)\n",
    "# all_vars = list(sig_table_filt.Variable.values)\n",
    "\n",
    "# current_vars = all_vars[:n_var_start]\n",
    "# vars_to_test = all_vars[n_var_start:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77557946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_vars + [vars_to_test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c81f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_per_param(var_names, all_df,config_dict, n_iterations, verbose=True):\n",
    "\n",
    "    X = all_df[var_names].values\n",
    "\n",
    "    samples_to_remove = ~np.all(np.isnan(X),axis=1)\n",
    "    y = all_df.iloc[samples_to_remove]['sociability'].values\n",
    "    filenames = all_df.iloc[samples_to_remove].index.values\n",
    "    subject_id = all_df.iloc[samples_to_remove]['rat_number'].values\n",
    "    X = all_df.iloc[samples_to_remove][var_names].values\n",
    "    \n",
    "\n",
    "    ret, raw_results, errors_log,models, imputers = model_selection(\n",
    "        config_dict=config_dict,\n",
    "        X=X, y=y,\n",
    "        subject_id=subject_id,\n",
    "        filenames=filenames,\n",
    "        n_iterations=n_iterations,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    raw_results_df = pd.concat(raw_results)\n",
    "    \n",
    "    ret_df = pd.concat(ret)\n",
    "    ret_df = ret_df.reset_index().rename(columns={'index':'dataset'})\n",
    "#     mean_df = ret_df.drop(['trained_model','trained_imputer','trained_scalar','ind'],axis=1).groupby(['dataset','method', 'imputer', 'scaler', 'model', 'k_fold']).mean().reset_index()\n",
    "    mean_df = ret_df.drop(['trained_model','trained_imputer','trained_scalar','ind','iteration'],axis=1).groupby(\n",
    "        ['dataset','method', 'imputer', 'scaler', 'model', 'k_fold']).mean().reset_index()\n",
    "\n",
    "    # Add a new column 'ind' that counts the number of rows for each group\n",
    "    mean_df['ind'] = ret_df.drop(['trained_model','trained_imputer','trained_scalar','ind','iteration'],axis=1).groupby(['dataset','method', 'imputer', 'scaler', 'model', 'k_fold']).size().reset_index(name='ind')['ind']\n",
    "\n",
    "    return mean_df, raw_results_df, ret_df, errors_log, models, imputers\n",
    "\n",
    "def select_parameters(config_dict, df,sig_table=None,sig_cut_off=0.1, verbose=False, n_iterations=1):\n",
    "   \n",
    "    if sig_table is None:\n",
    "        sig_table = variable_significance_table(df)\n",
    "    \n",
    "    all_errors_log = []\n",
    "    best_f1 = 0\n",
    "    best_sum_table = []\n",
    "    best_raw_rable = []\n",
    "    best_ret_df = []\n",
    "    best_model = []\n",
    "    best_imputer = []\n",
    "    best_params = dict()\n",
    "    n_var_start = 1\n",
    "    sig_table_filt = sig_table.loc[sig_table['p-value']<sig_cut_off]\n",
    "    \n",
    "    n_params = len(sig_table_filt)\n",
    "    all_vars = list(sig_table_filt.Variable.values)\n",
    "    \n",
    "#     current_vars = all_vars[:n_var_start]\n",
    "    current_vars = []\n",
    "    vars_to_test = all_vars[n_var_start:]\n",
    "    \n",
    "    for n_var, new_var in enumerate(vars_to_test):\n",
    "        var_names = current_vars + [new_var]\n",
    "        try:\n",
    "            if verbose:\n",
    "                print(f\"============ New param: {new_var} ({n_var+1} of {len(vars_to_test)}) ============\")\n",
    "            \n",
    "            mean_df, raw_results_df, ret_df, errors_log, models, imputers = run_per_param(\n",
    "                var_names,df,config_dict, n_iterations, verbose)\n",
    "            this_best_sum_df = mean_df.loc[mean_df['dataset']=='test'].sort_values('f1', ascending=False).iloc[0]\n",
    "            all_errors_log.append(errors_log)\n",
    "            this_best_f1 = mean_df.loc[mean_df['dataset']=='test']['f1'].max()\n",
    "            best_ind = mean_df.loc[mean_df['dataset']=='test']['f1'].argmax()\n",
    "            print(best_f1)\n",
    "            if this_best_f1 > best_f1:\n",
    "                current_vars.append(new_var)\n",
    "                if verbose:\n",
    "                    print(f'*********New best f1 was found: {this_best_f1}************')\n",
    "                    print(f'Adding: {new_var}')\n",
    "                    print(f'Current vars: {current_vars}')\n",
    "                    print(f'method:{this_best_sum_df.method}')\n",
    "                    print(f'imputer:{this_best_sum_df.imputer}')\n",
    "                    print(f'model:{this_best_sum_df.model}')\n",
    "                    print(f'k_fold:{this_best_sum_df.k_fold}')\n",
    "\n",
    "                best_f1 = this_best_f1\n",
    "                best_sum_table = mean_df\n",
    "                best_ret_df = ret_df\n",
    "                best_raw_rable = raw_results_df.loc[\n",
    "                    (raw_results_df['method'] == this_best_sum_df.method)&\n",
    "                    (raw_results_df['imputer'] == this_best_sum_df.imputer)&\n",
    "                    (raw_results_df['scaler'] == this_best_sum_df.scaler)&\n",
    "                    (raw_results_df['model'] == this_best_sum_df.model)&\n",
    "                    (raw_results_df['k_fold'] == this_best_sum_df.k_fold)\n",
    "                ]\n",
    "    #                 best_model = models[best_ind]\n",
    "    #                 best_imputer = imputers[best_ind]\n",
    "                best_params = {\n",
    "                    'method': this_best_sum_df.method,\n",
    "                    'imputer': this_best_sum_df.imputer,\n",
    "                    'scaler': this_best_sum_df.scaler,\n",
    "                    'model': this_best_sum_df.model,\n",
    "                    'k_fold': this_best_sum_df.k_fold\n",
    "                }\n",
    "            else:\n",
    "                print(f'Variable {new_var} did not improve the model')\n",
    "        except :\n",
    "            print('=================Error - start ==================')\n",
    "            traceback.print_exc()\n",
    "            print('=================Error - end ==================')\n",
    "            continue\n",
    "    print('*************=============ALL DONE============*******************')\n",
    "    return best_f1, best_sum_table,best_raw_rable, current_vars,best_ret_df, all_errors_log, best_model, best_imputer, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run again\n",
    "\n",
    "\n",
    "    \n",
    "def run_model_again(df, list_of_variables, config_dict,verbose=False, n_total_iterations=100,n_iterations=1):\n",
    "\n",
    "    all_f1 = []\n",
    "\n",
    "    for n in tqdm(range(n_total_iterations)):\n",
    "        try:\n",
    "            mean_df, raw_results_df, ret_df, errors_log, models, imputers = run_per_param(\n",
    "                list_of_variables,\n",
    "                df,\n",
    "                config_dict=config_dict_best,\n",
    "                verbose=verbose,\n",
    "                n_iterations=n_iterations)\n",
    "            \n",
    "            f1_val = mean_df.loc[mean_df['dataset'] == 'test']['f1'].values\n",
    "            all_f1.append(f1_val)\n",
    "        except:\n",
    "            continue\n",
    "    all_f1 = np.array(all_f1)\n",
    "    all_f1 = all_f1.flatten()\n",
    "\n",
    "    mean_f1_real_data = np.mean(all_f1)\n",
    "    std_error_f1_real_data = np.std(all_f1, ddof=1) / np.sqrt(np.size(all_f1))\n",
    "\n",
    "    print(f'F1 Score:{mean_f1_real_data}\\u00B12{std_error_f1_real_data}')\n",
    "    return all_f1\n",
    "\n",
    "def get_config_dict(config_dict, best_params):\n",
    "    for pname, param in best_params.items():\n",
    "        if pname == 'method':\n",
    "            continue\n",
    "        if pname == 'k_fold':\n",
    "            config_dict_best[pname] = [param]\n",
    "    #         config_dict_best[pname] = [3]\n",
    "        else:\n",
    "            config_dict_best[pname] = {param:config_dict[pname][param]}\n",
    "    return config_dict_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4293223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def select_parameters_v2(config_dict, sig_table, all_df, varbose=False, n_iterations=1):\n",
    "   \n",
    "#     all_errors_log = []\n",
    "#     best_f1 = 0\n",
    "#     best_sum_table = []\n",
    "#     best_raw_rable = []\n",
    "#     best_ret_df = []\n",
    "#     best_model = []\n",
    "#     best_imputer = []\n",
    "#     best_params = dict()\n",
    "#     n_var_start = 1\n",
    "#     sig_cut_off = 0.1\n",
    "#     sig_table_filt = sig_table.loc[sig_table['p-value']<sig_cut_off]\n",
    "    \n",
    "#     n_params = len(sig_table_filt)\n",
    "#     all_vars = list(sig_table_filt.Variable.values)\n",
    "    \n",
    "#     current_vars = all_vars[:n_var_start]\n",
    "#     vars_to_test = all_vars[n_var_start:]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for new_var in vars_to_test:\n",
    "#         var_names = current_vars + [new_var]\n",
    "#         try:\n",
    "#             print(f\"============ New param: {new_var} ============\")\n",
    "#             mean_df, raw_results_df, ret_df, errors_log, models, imputers = run_per_param(\n",
    "#                 var_names,all_df,config_dict, n_iterations, varbose)\n",
    "#             this_best_sum_df = mean_df.loc[mean_df['dataset']=='test'].sort_values('f1', ascending=False).iloc[0]\n",
    "#             all_errors_log.append(errors_log)\n",
    "#             this_best_f1 = mean_df.loc[mean_df['dataset']=='test']['f1'].max()\n",
    "#             best_ind = mean_df.loc[mean_df['dataset']=='test']['f1'].argmax()\n",
    "#             print(best_f1)\n",
    "#             if this_best_f1 > best_f1:\n",
    "#                 current_vars.append(new_var)\n",
    "            \n",
    "#                 print(f'*********New best f1 was found: {this_best_f1}************')\n",
    "#                 print(f'Adding: {new_var}')\n",
    "#                 print(f'Current vars: {current_vars}')\n",
    "#                 print(f'method:{this_best_sum_df.method}')\n",
    "#                 print(f'imputer:{this_best_sum_df.imputer}')\n",
    "#                 print(f'model:{this_best_sum_df.model}')\n",
    "#                 print(f'k_fold:{this_best_sum_df.k_fold}')\n",
    "\n",
    "#                 best_f1 = this_best_f1\n",
    "#                 best_sum_table = mean_df\n",
    "#                 best_ret_df = ret_df\n",
    "#                 best_raw_rable = raw_results_df.loc[\n",
    "#                     (raw_results_df['method'] == this_best_sum_df.method)&\n",
    "#                     (raw_results_df['imputer'] == this_best_sum_df.imputer)&\n",
    "#                     (raw_results_df['scaler'] == this_best_sum_df.scaler)&\n",
    "#                     (raw_results_df['model'] == this_best_sum_df.model)&\n",
    "#                     (raw_results_df['k_fold'] == this_best_sum_df.k_fold)\n",
    "#                 ]\n",
    "# #                 best_model = models[best_ind]\n",
    "# #                 best_imputer = imputers[best_ind]\n",
    "#                 best_params = {\n",
    "#                     'method': this_best_sum_df.method,\n",
    "#                     'imputer': this_best_sum_df.imputer,\n",
    "#                     'scaler': this_best_sum_df.scaler,\n",
    "#                     'model': this_best_sum_df.model,\n",
    "#                     'k_fold': this_best_sum_df.k_fold\n",
    "#                 }\n",
    "#             else:\n",
    "#                 print(f'Variable {new_var} did not improve the model')\n",
    "#         except :\n",
    "#             print('=================Error - start ==================')\n",
    "#             traceback.print_exc()\n",
    "#             print('=================Error - end ==================')\n",
    "#             continue\n",
    "#     print('*************=============ALL DONE============*******************')\n",
    "#     return best_f1, best_sum_table,best_raw_rable, current_vars,best_ret_df, all_errors_log, best_model, best_imputer, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start with top 3 variables\n",
    "# top_3_vars = sig_table_all.Variable[:3].values\n",
    "# X = all_df[top_3_vars].values\n",
    "\n",
    "# samples_to_remove = ~np.all(np.isnan(X),axis=1)\n",
    "# y = all_df.iloc[samples_to_remove]['sociability'].values\n",
    "# subject_id = all_df.iloc[samples_to_remove]['rat_number'].values\n",
    "# X = all_df.iloc[samples_to_remove][top_3_vars].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict_test = {\n",
    "    'method': ['new'],\n",
    "    'k_fold': [5],\n",
    "    'imputer': {\n",
    "        'iterative_imputer': IterativeImputer,\n",
    "    },\n",
    "    'imputer_param': {\n",
    "        'iterative_imputer': {'max_iter':1000},\n",
    "    },\n",
    "    'scaler':{\n",
    "        'standard_scaler': StandardScaler,\n",
    "    },\n",
    "   'model':{\n",
    "    'random_forest':RandomForestClassifier,\n",
    "    'logistic_regression': LogisticRegression,\n",
    "#     'svm': svm.SVC,\n",
    "#     'naive_basian': GaussianNB,\n",
    "#     'knn_classifier': KNeighborsClassifier,\n",
    "   },\n",
    "    'model_param':{\n",
    "    'svm':{'probability': True},\n",
    "    'knn_classifier':{'n_neighbors': 5}\n",
    "    }\n",
    "}\n",
    "\n",
    "config_dict = {\n",
    "    'method': ['reuse'],\n",
    "#     'method': ['reuse', 'new'],\n",
    "#     'k_fold': [3,4,5],\n",
    "    'k_fold': [4,5],\n",
    "    'imputer': {\n",
    "#         'missforest':MissForest,\n",
    "        'iterative_imputer': IterativeImputer,\n",
    "#         'knn_imputer': KNNImputer        \n",
    "    },\n",
    "    'imputer_param': {\n",
    "        'iterative_imputer': {'max_iter':1000},\n",
    "    },\n",
    "    'scaler':{\n",
    "        'standard_scaler': StandardScaler,\n",
    "#         'no_scaler': lambda x: x,\n",
    "    },\n",
    "    'model':{\n",
    "        'random_forest':RandomForestClassifier,\n",
    "        'logistic_regression': LogisticRegression,\n",
    "#         'svm': svm.SVC,\n",
    "#         'naive_basian': GaussianNB,\n",
    "#         'knn_classifier': KNeighborsClassifier,\n",
    "    },\n",
    "    'model_param':{\n",
    "        'svm':{'probability': True},\n",
    "        'knn_classifier':{'n_neighbors': 5}\n",
    "    }\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "7d48ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with warnings.catch_warnings(record=True):\n",
    "#     best_f1, best_sum_table, best_raw_rable, current_vars, best_ret_df, all_errors_log, best_model, best_imputer, best_params = select_parameters(\n",
    "#         config_dict = config_dict,\n",
    "#         sig_table = None,\n",
    "#         verbose = True,\n",
    "#         df = df_spike_data,\n",
    "#         sig_cut_off =np.inf ,\n",
    "#         n_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "bdd2711c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ New param: lfp_lfp_AA_30_80Hz_enc_pre (1 of 3) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "*********New best f1 was found: 0.5596************\n",
      "Adding: lfp_lfp_AA_30_80Hz_enc_pre\n",
      "Current vars: ['lfp_lfp_AA_30_80Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:logistic_regression\n",
      "k_fold:4\n",
      "============ New param: lfp_lfp_AA_4_18Hz_enc_pre (2 of 3) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5596\n",
      "*********New best f1 was found: 0.6317************\n",
      "Adding: lfp_lfp_AA_4_18Hz_enc_pre\n",
      "Current vars: ['lfp_lfp_AA_30_80Hz_enc_pre', 'lfp_lfp_AA_4_18Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:4\n",
      "============ New param: lfp_lfp_CeA_30_80Hz_enc_pre (3 of 3) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:12<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6317\n",
      "*********New best f1 was found: 0.6590869565217392************\n",
      "Adding: lfp_lfp_CeA_30_80Hz_enc_pre\n",
      "Current vars: ['lfp_lfp_AA_30_80Hz_enc_pre', 'lfp_lfp_AA_4_18Hz_enc_pre', 'lfp_lfp_CeA_30_80Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:logistic_regression\n",
      "k_fold:5\n",
      "*************=============ALL DONE============*******************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings(record=True):\n",
    "    best_f1, best_sum_table, best_raw_rable, current_vars, best_ret_df, all_errors_log, best_model, best_imputer, best_params = select_parameters(\n",
    "        config_dict = config_dict,\n",
    "        sig_table = None,\n",
    "        verbose = True,\n",
    "        df = df_lfp_data,\n",
    "        sig_cut_off =0.05,\n",
    "        n_iterations=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "e7617c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████████████████████████▉                                      | 53/100 [00:20<00:18,  2.59it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:38<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:0.5721466666666667±20.009052296395593221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f1_score_lfp = run_model_again(\n",
    "    df=df_lfp_data,\n",
    "    list_of_variables=current_vars,\n",
    "    config_dict = config_dict_best,\n",
    "    n_total_iterations=100,\n",
    "    verbose=False,\n",
    "    n_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "90be7f43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ New param: coherence_coherence_AA_MeD_4_18Hz_enc_pre (1 of 6) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "*********New best f1 was found: 0.65998************\n",
      "Adding: coherence_coherence_AA_MeD_4_18Hz_enc_pre\n",
      "Current vars: ['coherence_coherence_AA_MeD_4_18Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:5\n",
      "============ New param: coherence_coherence_CeA_MeD_4_18Hz_enc_pre (2 of 6) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:10<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65998\n",
      "Variable coherence_coherence_CeA_MeD_4_18Hz_enc_pre did not improve the model\n",
      "============ New param: coherence_coherence_AA_MeD_30_80Hz_enc_pre (3 of 6) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65998\n",
      "Variable coherence_coherence_AA_MeD_30_80Hz_enc_pre did not improve the model\n",
      "============ New param: coherence_coherence_CeA_MeD_4_18Hz_post_pre (4 of 6) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65998\n",
      "Variable coherence_coherence_CeA_MeD_4_18Hz_post_pre did not improve the model\n",
      "============ New param: coherence_coherence_CeA_STIA_30_80Hz_enc_pre (5 of 6) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65998\n",
      "Variable coherence_coherence_CeA_STIA_30_80Hz_enc_pre did not improve the model\n",
      "============ New param: coherence_coherence_CeA_MeD_30_80Hz_enc_pre (6 of 6) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65998\n",
      "Variable coherence_coherence_CeA_MeD_30_80Hz_enc_pre did not improve the model\n",
      "*************=============ALL DONE============*******************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings(record=True):\n",
    "    best_f1, best_sum_table, best_raw_rable, current_vars, best_ret_df, all_errors_log, best_model, best_imputer, best_params = select_parameters(\n",
    "        config_dict = config_dict,\n",
    "        sig_table = None,\n",
    "        verbose = True,\n",
    "        df = df_coherence_data,\n",
    "        sig_cut_off =0.1,\n",
    "        n_iterations=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "2e01617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████                                                                              | 5/100 [00:01<00:27,  3.44it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "  6%|████▉                                                                             | 6/100 [00:01<00:27,  3.40it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  8%|██████▌                                                                           | 8/100 [00:02<00:27,  3.36it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  9%|███████▍                                                                          | 9/100 [00:02<00:27,  3.35it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      " 13%|██████████▌                                                                      | 13/100 [00:03<00:25,  3.36it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 18%|██████████████▌                                                                  | 18/100 [00:05<00:24,  3.35it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 21%|█████████████████                                                                | 21/100 [00:06<00:23,  3.43it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 22%|█████████████████▊                                                               | 22/100 [00:06<00:22,  3.40it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 23%|██████████████████▋                                                              | 23/100 [00:06<00:22,  3.43it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 28%|██████████████████████▋                                                          | 28/100 [00:08<00:21,  3.43it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 34%|███████████████████████████▌                                                     | 34/100 [00:10<00:19,  3.38it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 35%|████████████████████████████▎                                                    | 35/100 [00:10<00:19,  3.42it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 37%|█████████████████████████████▉                                                   | 37/100 [00:10<00:18,  3.37it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 40%|████████████████████████████████▍                                                | 40/100 [00:11<00:17,  3.37it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 41%|█████████████████████████████████▏                                               | 41/100 [00:12<00:17,  3.38it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 51%|█████████████████████████████████████████▎                                       | 51/100 [00:15<00:14,  3.39it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 56%|█████████████████████████████████████████████▎                                   | 56/100 [00:16<00:12,  3.39it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|██████████████████████████████████████████████▏                                  | 57/100 [00:16<00:12,  3.36it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 60%|████████████████████████████████████████████████▌                                | 60/100 [00:17<00:11,  3.37it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 71%|█████████████████████████████████████████████████████████▌                       | 71/100 [00:20<00:08,  3.41it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 77%|██████████████████████████████████████████████████████████████▎                  | 77/100 [00:22<00:06,  3.41it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 84%|████████████████████████████████████████████████████████████████████             | 84/100 [00:24<00:04,  3.41it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▋       | 91/100 [00:26<00:02,  3.39it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      " 93%|███████████████████████████████████████████████████████████████████████████▎     | 93/100 [00:27<00:02,  3.36it/s]C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:29<00:00,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:0.6075966666666667±20.012431253195253057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f1_score_coherence = run_model_again(\n",
    "    df=df_coherence_data,\n",
    "    list_of_variables=current_vars,\n",
    "    config_dict = config_dict_best,\n",
    "    n_total_iterations=100,\n",
    "    verbose=False,\n",
    "    n_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "c0c5b903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ New param: lfp_lfp_AA_30_80Hz_enc_pre (0 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "*********New best f1 was found: 0.5743666666666667************\n",
      "Adding: lfp_lfp_AA_30_80Hz_enc_pre\n",
      "Current vars: ['lfp_lfp_AA_30_80Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:logistic_regression\n",
      "k_fold:3\n",
      "============ New param: coherence_coherence_CeA_MeD_30_80Hz_post_pre (1 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5743666666666667\n",
      "Variable coherence_coherence_CeA_MeD_30_80Hz_post_pre did not improve the model\n",
      "============ New param: coherence_coherence_AA_MeD_4_18Hz_enc_pre (2 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5743666666666667\n",
      "*********New best f1 was found: 0.6436333333333334************\n",
      "Adding: coherence_coherence_AA_MeD_4_18Hz_enc_pre\n",
      "Current vars: ['lfp_lfp_AA_30_80Hz_enc_pre', 'coherence_coherence_AA_MeD_4_18Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:3\n",
      "============ New param: lfp_lfp_AA_4_18Hz_enc_pre (3 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6436333333333334\n",
      "*********New best f1 was found: 0.6911333333333334************\n",
      "Adding: lfp_lfp_AA_4_18Hz_enc_pre\n",
      "Current vars: ['lfp_lfp_AA_30_80Hz_enc_pre', 'coherence_coherence_AA_MeD_4_18Hz_enc_pre', 'lfp_lfp_AA_4_18Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:3\n",
      "============ New param: coherence_coherence_CeA_MeD_4_18Hz_enc_pre (4 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6911333333333334\n",
      "Variable coherence_coherence_CeA_MeD_4_18Hz_enc_pre did not improve the model\n",
      "============ New param: coherence_coherence_AA_MeD_30_80Hz_enc_pre (5 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6911333333333334\n",
      "Variable coherence_coherence_AA_MeD_30_80Hz_enc_pre did not improve the model\n",
      "============ New param: spike_ratio_spike_ratio_STIA_enc_pre (6 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6911333333333334\n",
      "Variable spike_ratio_spike_ratio_STIA_enc_pre did not improve the model\n",
      "============ New param: lfp_lfp_CeA_30_80Hz_enc_pre (7 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:06<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6911333333333334\n",
      "Variable lfp_lfp_CeA_30_80Hz_enc_pre did not improve the model\n",
      "============ New param: coherence_coherence_CeA_MeD_4_18Hz_post_pre (8 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:04<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6911333333333334\n",
      "Variable coherence_coherence_CeA_MeD_4_18Hz_post_pre did not improve the model\n",
      "============ New param: coherence_coherence_CeA_STIA_30_80Hz_enc_pre (9 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:04<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6911333333333334\n",
      "Variable coherence_coherence_CeA_STIA_30_80Hz_enc_pre did not improve the model\n",
      "============ New param: lfp_lfp_MePV_4_18Hz_post_pre (10 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6911333333333334\n",
      "Variable lfp_lfp_MePV_4_18Hz_post_pre did not improve the model\n",
      "============ New param: lfp_lfp_CeA_4_18Hz_enc_pre (11 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6911333333333334\n",
      "Variable lfp_lfp_CeA_4_18Hz_enc_pre did not improve the model\n",
      "============ New param: lfp_lfp_MeD_30_80Hz_enc_pre (12 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:04<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6911333333333334\n",
      "Variable lfp_lfp_MeD_30_80Hz_enc_pre did not improve the model\n",
      "============ New param: coherence_coherence_CeA_MeD_30_80Hz_enc_pre (13 of 14) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:06<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6911333333333334\n",
      "Variable coherence_coherence_CeA_MeD_30_80Hz_enc_pre did not improve the model\n",
      "*************=============ALL DONE============*******************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 7718.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:nan±2nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:262: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings(record=True):\n",
    "    best_f1, best_sum_table, best_raw_rable, current_vars, best_ret_df, all_errors_log, best_model, best_imputer, best_params = select_parameters(\n",
    "        config_dict = config_dict,\n",
    "        sig_table = None,\n",
    "        verbose = True,\n",
    "        df = all_df,\n",
    "        sig_cut_off =0.1,\n",
    "        n_iterations=10)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "a201b737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.46it/s]\u001b[A\n",
      "  1%|▊                                                                                 | 1/100 [00:00<00:30,  3.27it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.51it/s]\u001b[A\n",
      "  2%|█▋                                                                                | 2/100 [00:00<00:28,  3.41it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.51it/s]\u001b[A\n",
      "  3%|██▍                                                                               | 3/100 [00:00<00:28,  3.37it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.51it/s]\u001b[A\n",
      "  4%|███▎                                                                              | 4/100 [00:01<00:29,  3.28it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      "  5%|████                                                                              | 5/100 [00:01<00:29,  3.24it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      "  6%|████▉                                                                             | 6/100 [00:01<00:28,  3.27it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.50it/s]\u001b[A\n",
      "  7%|█████▋                                                                            | 7/100 [00:02<00:28,  3.23it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      "  8%|██████▌                                                                           | 8/100 [00:02<00:28,  3.19it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.40it/s]\u001b[A\n",
      "  9%|███████▍                                                                          | 9/100 [00:02<00:28,  3.23it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 10%|████████                                                                         | 10/100 [00:03<00:28,  3.21it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[AC:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 11%|████████▉                                                                        | 11/100 [00:03<00:27,  3.24it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.51it/s]\u001b[A\n",
      " 12%|█████████▋                                                                       | 12/100 [00:03<00:27,  3.22it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 13%|██████████▌                                                                      | 13/100 [00:04<00:27,  3.20it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 14%|███████████▎                                                                     | 14/100 [00:04<00:26,  3.19it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 15%|████████████▏                                                                    | 15/100 [00:04<00:26,  3.18it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 16%|████████████▉                                                                    | 16/100 [00:04<00:26,  3.17it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[AC:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.40it/s]\u001b[A\n",
      " 17%|█████████████▊                                                                   | 17/100 [00:05<00:25,  3.20it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 18%|██████████████▌                                                                  | 18/100 [00:05<00:25,  3.19it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 19%|███████████████▍                                                                 | 19/100 [00:05<00:25,  3.18it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 20%|████████████████▏                                                                | 20/100 [00:06<00:25,  3.17it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 21%|█████████████████                                                                | 21/100 [00:06<00:24,  3.22it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.51it/s]\u001b[A\n",
      " 22%|█████████████████▊                                                               | 22/100 [00:06<00:24,  3.20it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[AC:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 23%|██████████████████▋                                                              | 23/100 [00:07<00:24,  3.19it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 24%|███████████████████▍                                                             | 24/100 [00:07<00:23,  3.18it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 25%|████████████████████▎                                                            | 25/100 [00:07<00:23,  3.20it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.29it/s]\u001b[A\n",
      " 26%|█████████████████████                                                            | 26/100 [00:08<00:23,  3.20it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.37it/s]\u001b[A\n",
      " 27%|█████████████████████▊                                                           | 27/100 [00:08<00:22,  3.21it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 28%|██████████████████████▋                                                          | 28/100 [00:08<00:22,  3.18it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 29%|███████████████████████▍                                                         | 29/100 [00:09<00:22,  3.17it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.23it/s]\u001b[A\n",
      " 30%|████████████████████████▎                                                        | 30/100 [00:09<00:22,  3.16it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.22it/s]\u001b[A\n",
      " 31%|█████████████████████████                                                        | 31/100 [00:09<00:21,  3.18it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 32%|█████████████████████████▉                                                       | 32/100 [00:10<00:21,  3.13it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.33it/s]\u001b[A\n",
      " 33%|██████████████████████████▋                                                      | 33/100 [00:10<00:21,  3.14it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 34%|███████████████████████████▌                                                     | 34/100 [00:10<00:20,  3.14it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.30it/s]\u001b[A\n",
      " 35%|████████████████████████████▎                                                    | 35/100 [00:10<00:20,  3.16it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.22it/s]\u001b[A\n",
      " 36%|█████████████████████████████▏                                                   | 36/100 [00:11<00:20,  3.15it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 37%|█████████████████████████████▉                                                   | 37/100 [00:11<00:19,  3.15it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.33it/s]\u001b[A\n",
      " 38%|██████████████████████████████▊                                                  | 38/100 [00:11<00:19,  3.16it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 39%|███████████████████████████████▌                                                 | 39/100 [00:12<00:19,  3.16it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[AC:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.33it/s]\u001b[A\n",
      " 40%|████████████████████████████████▍                                                | 40/100 [00:12<00:19,  3.12it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.27it/s]\u001b[A\n",
      " 41%|█████████████████████████████████▏                                               | 41/100 [00:12<00:18,  3.13it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 42%|██████████████████████████████████                                               | 42/100 [00:13<00:18,  3.14it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.24it/s]\u001b[A\n",
      " 43%|██████████████████████████████████▊                                              | 43/100 [00:13<00:18,  3.11it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 44%|███████████████████████████████████▋                                             | 44/100 [00:13<00:17,  3.13it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 45%|████████████████████████████████████▍                                            | 45/100 [00:14<00:17,  3.14it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.33it/s]\u001b[A\n",
      " 46%|█████████████████████████████████████▎                                           | 46/100 [00:14<00:17,  3.14it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 47%|██████████████████████████████████████                                           | 47/100 [00:14<00:16,  3.15it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.31it/s]\u001b[A\n",
      " 48%|██████████████████████████████████████▉                                          | 48/100 [00:15<00:16,  3.17it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.29it/s]\u001b[A\n",
      " 49%|███████████████████████████████████████▋                                         | 49/100 [00:15<00:16,  3.15it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 50%|████████████████████████████████████████▌                                        | 50/100 [00:15<00:16,  3.12it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.24it/s]\u001b[A\n",
      " 51%|█████████████████████████████████████████▎                                       | 51/100 [00:16<00:15,  3.13it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.35it/s]\u001b[A\n",
      " 52%|██████████████████████████████████████████                                       | 52/100 [00:16<00:15,  3.16it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 53%|██████████████████████████████████████████▉                                      | 53/100 [00:16<00:14,  3.14it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 54%|███████████████████████████████████████████▋                                     | 54/100 [00:16<00:14,  3.15it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 55%|████████████████████████████████████████████▌                                    | 55/100 [00:17<00:14,  3.15it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 56%|█████████████████████████████████████████████▎                                   | 56/100 [00:17<00:13,  3.15it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 57%|██████████████████████████████████████████████▏                                  | 57/100 [00:17<00:13,  3.16it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 58%|██████████████████████████████████████████████▉                                  | 58/100 [00:18<00:13,  3.16it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 59%|███████████████████████████████████████████████▊                                 | 59/100 [00:18<00:13,  3.14it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 60%|████████████████████████████████████████████████▌                                | 60/100 [00:18<00:12,  3.14it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████▍                               | 61/100 [00:19<00:12,  3.10it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 62%|██████████████████████████████████████████████████▏                              | 62/100 [00:19<00:12,  3.12it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 63%|███████████████████████████████████████████████████                              | 63/100 [00:19<00:11,  3.13it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.10it/s]\u001b[A\n",
      " 64%|███████████████████████████████████████████████████▊                             | 64/100 [00:20<00:11,  3.12it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 65%|████████████████████████████████████████████████████▋                            | 65/100 [00:20<00:11,  3.09it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 66%|█████████████████████████████████████████████████████▍                           | 66/100 [00:20<00:10,  3.11it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 67%|██████████████████████████████████████████████████████▎                          | 67/100 [00:21<00:10,  3.08it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[AC:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 68%|███████████████████████████████████████████████████████                          | 68/100 [00:21<00:10,  3.08it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 69%|███████████████████████████████████████████████████████▉                         | 69/100 [00:21<00:09,  3.11it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 70%|████████████████████████████████████████████████████████▋                        | 70/100 [00:22<00:09,  3.08it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.25it/s]\u001b[A\n",
      " 71%|█████████████████████████████████████████████████████████▌                       | 71/100 [00:22<00:09,  3.10it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 72%|██████████████████████████████████████████████████████████▎                      | 72/100 [00:22<00:09,  3.09it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 73%|███████████████████████████████████████████████████████████▏                     | 73/100 [00:23<00:08,  3.11it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 74%|███████████████████████████████████████████████████████████▉                     | 74/100 [00:23<00:08,  3.08it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 75%|████████████████████████████████████████████████████████████▊                    | 75/100 [00:23<00:08,  3.10it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 76%|█████████████████████████████████████████████████████████████▌                   | 76/100 [00:24<00:07,  3.10it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 77%|██████████████████████████████████████████████████████████████▎                  | 77/100 [00:24<00:07,  3.12it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.30it/s]\u001b[A\n",
      " 78%|███████████████████████████████████████████████████████████████▏                 | 78/100 [00:24<00:07,  3.10it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.39it/s]\u001b[A\n",
      " 79%|███████████████████████████████████████████████████████████████▉                 | 79/100 [00:25<00:06,  3.12it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.22it/s]\u001b[A\n",
      " 80%|████████████████████████████████████████████████████████████████▊                | 80/100 [00:25<00:06,  3.12it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 81%|█████████████████████████████████████████████████████████████████▌               | 81/100 [00:25<00:06,  3.09it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 82%|██████████████████████████████████████████████████████████████████▍              | 82/100 [00:26<00:05,  3.11it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 83%|███████████████████████████████████████████████████████████████████▏             | 83/100 [00:26<00:05,  3.08it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.31it/s]\u001b[A\n",
      " 84%|████████████████████████████████████████████████████████████████████             | 84/100 [00:26<00:05,  3.08it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.18it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████████████████████████████████████████▊            | 85/100 [00:26<00:04,  3.09it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.97it/s]\u001b[A\n",
      " 86%|█████████████████████████████████████████████████████████████████████▋           | 86/100 [00:27<00:04,  3.03it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.12it/s]\u001b[A\n",
      " 87%|██████████████████████████████████████████████████████████████████████▍          | 87/100 [00:27<00:04,  3.03it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.12it/s]\u001b[A\n",
      " 88%|███████████████████████████████████████████████████████████████████████▎         | 88/100 [00:27<00:03,  3.04it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.23it/s]\u001b[A\n",
      " 89%|████████████████████████████████████████████████████████████████████████         | 89/100 [00:28<00:03,  3.06it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.22it/s]\u001b[A\n",
      " 90%|████████████████████████████████████████████████████████████████████████▉        | 90/100 [00:28<00:03,  3.05it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[AC:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.13it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▋       | 91/100 [00:28<00:02,  3.05it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.17it/s]\u001b[A\n",
      " 92%|██████████████████████████████████████████████████████████████████████████▌      | 92/100 [00:29<00:02,  3.04it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.29it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████████████████████████████████████████▎     | 93/100 [00:29<00:02,  3.09it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.21it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████████████████████████████████████████▏    | 94/100 [00:29<00:01,  3.10it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████████████████████████████████████████▉    | 95/100 [00:30<00:01,  3.07it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████████████████████████████████████████▊   | 96/100 [00:30<00:01,  3.10it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 97%|██████████████████████████████████████████████████████████████████████████████▌  | 97/100 [00:30<00:00,  3.07it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.32it/s]\u001b[A\n",
      " 98%|███████████████████████████████████████████████████████████████████████████████▍ | 98/100 [00:31<00:00,  3.08it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.16it/s]\u001b[A\n",
      " 99%|████████████████████████████████████████████████████████████████████████████████▏| 99/100 [00:31<00:00,  3.10it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.14it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:0.6767466666666668±20.009559650426341415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f1_score_all = run_model_again(\n",
    "    df=all_df,\n",
    "    list_of_variables=current_vars,\n",
    "    config_dict = config_dict_best,\n",
    "    n_total_iterations=100,\n",
    "    verbose=False,\n",
    "    n_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7acc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ New param: lfp_lfp_AA_30_80Hz_enc_pre (1 of 13) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "*********New best f1 was found: 0.54506************\n",
      "Adding: lfp_lfp_AA_30_80Hz_enc_pre\n",
      "Current vars: ['lfp_lfp_AA_30_80Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:logistic_regression\n",
      "k_fold:5\n",
      "============ New param: coherence_coherence_CeA_MeD_30_80Hz_post_pre (2 of 13) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:10<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54506\n",
      "*********New best f1 was found: 0.5677391304347826************\n",
      "Adding: coherence_coherence_CeA_MeD_30_80Hz_post_pre\n",
      "Current vars: ['lfp_lfp_AA_30_80Hz_enc_pre', 'coherence_coherence_CeA_MeD_30_80Hz_post_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:5\n",
      "============ New param: coherence_coherence_AA_MeD_4_18Hz_enc_pre (3 of 13) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:13<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5677391304347826\n",
      "*********New best f1 was found: 0.6478918918918919************\n",
      "Adding: coherence_coherence_AA_MeD_4_18Hz_enc_pre\n",
      "Current vars: ['lfp_lfp_AA_30_80Hz_enc_pre', 'coherence_coherence_CeA_MeD_30_80Hz_post_pre', 'coherence_coherence_AA_MeD_4_18Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:4\n",
      "============ New param: lfp_lfp_AA_4_18Hz_enc_pre (4 of 13) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6478918918918919\n",
      "Variable lfp_lfp_AA_4_18Hz_enc_pre did not improve the model\n",
      "============ New param: coherence_coherence_CeA_MeD_4_18Hz_enc_pre (5 of 13) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:17<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6478918918918919\n",
      "*********New best f1 was found: 0.6937837837837838************\n",
      "Adding: coherence_coherence_CeA_MeD_4_18Hz_enc_pre\n",
      "Current vars: ['lfp_lfp_AA_30_80Hz_enc_pre', 'coherence_coherence_CeA_MeD_30_80Hz_post_pre', 'coherence_coherence_AA_MeD_4_18Hz_enc_pre', 'coherence_coherence_CeA_MeD_4_18Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:4\n",
      "============ New param: coherence_coherence_AA_MeD_30_80Hz_enc_pre (6 of 13) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:23<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6937837837837838\n",
      "*********New best f1 was found: 0.7042105263157895************\n",
      "Adding: coherence_coherence_AA_MeD_30_80Hz_enc_pre\n",
      "Current vars: ['lfp_lfp_AA_30_80Hz_enc_pre', 'coherence_coherence_CeA_MeD_30_80Hz_post_pre', 'coherence_coherence_AA_MeD_4_18Hz_enc_pre', 'coherence_coherence_CeA_MeD_4_18Hz_enc_pre', 'coherence_coherence_AA_MeD_30_80Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:4\n",
      "============ New param: lfp_lfp_CeA_30_80Hz_enc_pre (7 of 13) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:41<00:00,  4.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7042105263157895\n",
      "Variable lfp_lfp_CeA_30_80Hz_enc_pre did not improve the model\n",
      "============ New param: coherence_coherence_CeA_MeD_4_18Hz_post_pre (8 of 13) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:53<00:00,  5.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7042105263157895\n",
      "Variable coherence_coherence_CeA_MeD_4_18Hz_post_pre did not improve the model\n",
      "============ New param: coherence_coherence_CeA_STIA_30_80Hz_enc_pre (9 of 13) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:02<00:00,  6.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7042105263157895\n",
      "*********New best f1 was found: 0.7487560975609756************\n",
      "Adding: coherence_coherence_CeA_STIA_30_80Hz_enc_pre\n",
      "Current vars: ['lfp_lfp_AA_30_80Hz_enc_pre', 'coherence_coherence_CeA_MeD_30_80Hz_post_pre', 'coherence_coherence_AA_MeD_4_18Hz_enc_pre', 'coherence_coherence_CeA_MeD_4_18Hz_enc_pre', 'coherence_coherence_AA_MeD_30_80Hz_enc_pre', 'coherence_coherence_CeA_STIA_30_80Hz_enc_pre']\n",
      "method:reuse\n",
      "imputer:iterative_imputer\n",
      "model:random_forest\n",
      "k_fold:5\n",
      "============ New param: lfp_lfp_MePV_4_18Hz_post_pre (10 of 13) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:27<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7487560975609756\n",
      "Variable lfp_lfp_MePV_4_18Hz_post_pre did not improve the model\n",
      "============ New param: lfp_lfp_CeA_4_18Hz_enc_pre (11 of 13) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:57<00:00, 11.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7487560975609756\n",
      "Variable lfp_lfp_CeA_4_18Hz_enc_pre did not improve the model\n",
      "============ New param: lfp_lfp_MeD_30_80Hz_enc_pre (12 of 13) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [02:31<00:00, 15.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7487560975609756\n",
      "Variable lfp_lfp_MeD_30_80Hz_enc_pre did not improve the model\n",
      "============ New param: coherence_coherence_CeA_MeD_30_80Hz_enc_pre (13 of 13) ============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [02:25<01:36, 24.19s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-488-784469c02afa>\", line 65, in select_parameters\n",
      "    mean_df, raw_results_df, ret_df, errors_log, models, imputers = run_per_param(\n",
      "  File \"<ipython-input-488-784469c02afa>\", line 12, in run_per_param\n",
      "    ret, raw_results, errors_log,models, imputers = model_selection(\n",
      "  File \"<ipython-input-487-72a582770058>\", line 196, in model_selection\n",
      "    sum_results, raw_results, model, imputer = train_eval_model(\n",
      "  File \"<ipython-input-487-72a582770058>\", line 69, in train_eval_model\n",
      "    X_train_imputed, X_test_imputed, imputer_train, imputer_test = transform_var(\n",
      "  File \"<ipython-input-432-2c56d5ecd25d>\", line 7, in transform_var\n",
      "    X_train_trans = func_train.fit_transform(X_train)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py\", line 678, in fit_transform\n",
      "    Xt, estimator = self._impute_one_feature(\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py\", line 331, in _impute_one_feature\n",
      "    estimator.fit(X_train, y_train)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_bayes.py\", line 291, in fit\n",
      "    coef_, rmse_ = self._update_coef_(\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_bayes.py\", line 391, in _update_coef_\n",
      "    rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)\n",
      "  File \"<__array_function__ internals>\", line 5, in dot\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================Error - start ==================\n",
      "=================Error - end ==================\n",
      "*************=============ALL DONE============*******************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "df_coherence_lfp = all_df[[c for c in all_df.columns if 'spike' not in c]]\n",
    "\n",
    "with warnings.catch_warnings(record=True):\n",
    "    best_f1, best_sum_table, best_raw_rable, current_vars, best_ret_df, all_errors_log, best_model, best_imputer, best_params = select_parameters(\n",
    "        config_dict = config_dict,\n",
    "        sig_table = None,\n",
    "        verbose = True,\n",
    "        df = df_coherence_lfp,\n",
    "        sig_cut_off =0.1,\n",
    "        n_iterations=10)\n",
    "    \n",
    "f1_score_coherence_lfp = run_model_again(\n",
    "    df=df_coherence_lfp,\n",
    "    list_of_variables=current_vars,\n",
    "    config_dict = config_dict_best,\n",
    "    n_total_iterations=100,\n",
    "    verbose=False,\n",
    "    n_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "19e02704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with warnings.catch_warnings(record=True):\n",
    "#     best_f1, best_sum_table, best_raw_rable, current_vars, best_ret_df, all_errors_log, best_model, best_imputer, best_params = select_parameters(\n",
    "#         config_dict = config_dict,\n",
    "#         sig_table = sig_table_all,\n",
    "#         all_df = all_df, n_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a695b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bcc3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_ret_df.loc[\n",
    "#     (best_ret_df['imputer'] == 'iterative_imputer')&\n",
    "#     (best_ret_df['model'] == 'random_forest')&\n",
    "#     (best_ret_df['kfold'] == 5)&\n",
    "#     (best_ret_df['dataset'] == 'test')]\n",
    "\n",
    "# best_raw_rable.loc[\n",
    "#        (best_raw_rable['imputer'] == 'iterative_imputer')&\n",
    "#         (best_raw_rable['model'] == 'random_forest')&\n",
    "#         (best_raw_rable['kfold'] == 5)]\n",
    "# best_raw_rable.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f040fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_raw_rable.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f24c0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_sum_table.to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5880134",
   "metadata": {},
   "source": [
    "### Best params selected:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ff2134d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AA_30_80Hz_enc_pre_lfp',\n",
       " 'CeA_MeD_30_80Hz_post_pre',\n",
       " 'AA_MeD_4_18Hz_enc_pre',\n",
       " 'CeA_MeD_4_18Hz_enc_pre',\n",
       " 'AA_MeD_30_80Hz_enc_pre']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e569f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'reuse',\n",
       " 'imputer': 'iterative_imputer',\n",
       " 'scaler': 'standard_scaler',\n",
       " 'model': 'random_forest',\n",
       " 'k_fold': 3}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4847ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict_best = config_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98a08c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pname, param in best_params.items():\n",
    "#     if pname == 'method':\n",
    "#         continue\n",
    "#     if pname == 'k_fold':\n",
    "#         config_dict_best[pname] = [param]\n",
    "# #         config_dict_best[pname] = [3]\n",
    "#     else:\n",
    "#         config_dict_best[pname] = {param:config_dict[pname][param]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bd6c42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': ['reuse'],\n",
       " 'k_fold': [3],\n",
       " 'imputer': {'iterative_imputer': sklearn.impute._iterative.IterativeImputer},\n",
       " 'imputer_param': {'iterative_imputer': {'max_iter': 1000}},\n",
       " 'scaler': {'standard_scaler': sklearn.preprocessing._data.StandardScaler},\n",
       " 'model': {'random_forest': sklearn.ensemble._forest.RandomForestClassifier},\n",
       " 'model_param': {'svm': {'probability': True},\n",
       "  'knn_classifier': {'n_neighbors': 5}}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69952dbd",
   "metadata": {},
   "source": [
    "### Recreate sucsess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc956355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_dict_best = {'method': ['reuse'],\n",
    "#  'k_fold': [4],\n",
    "#  'imputer': {'imputer': sklearn.impute._iterative.IterativeImputer},\n",
    "#  'imputer_param': {'iterative_imputer': {'max_iter': 10000}},\n",
    "#  'scaler': {'scaler': sklearn.preprocessing._data.StandardScaler},\n",
    "#  'model': {'model': LogisticRegression},\n",
    "#  'model_param': {'svm': {'probability': True},\n",
    "#   'knn_classifier': {'n_neighbors': 5}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "429f5f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_df, raw_results_df, ret_df, errors_log, models, imputers = run_per_param(\n",
    "#     current_vars, all_df, config_dict=config_dict_best,n_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b49ba9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f277de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_results_df.loc[raw_results_df['dataset'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a901d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_shuffled = all_df.copy()\n",
    "all_df_shuffled['sociability'] = np.random.permutation(all_df_shuffled['sociability'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a576d0a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:713: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:713: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n"
     ]
    }
   ],
   "source": [
    "n_total_iterations = 100\n",
    "n_iterations = 1\n",
    "\n",
    "all_f1 = []\n",
    "\n",
    "for n in range(n_total_iterations):\n",
    "    try:\n",
    "        mean_df, raw_results_df, ret_df, errors_log, models, imputers = run_per_param(\n",
    "            current_vars, all_df, config_dict=config_dict_best, n_iterations=n_iterations)\n",
    "        f1_val = mean_df.loc[mean_df['dataset'] == 'test']['f1'].values\n",
    "        all_f1.append(f1_val)\n",
    "    except:\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ef74acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de74639c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa58f52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:0.7052966666666669+-0.00902592586444392\n"
     ]
    }
   ],
   "source": [
    "# all_f1 = np.array(all_f1)\n",
    "# all_f1 = all_f1.flatten()\n",
    "\n",
    "# mean_f1_real_data = np.mean(all_f1)\n",
    "# std_error_f1_real_data = np.std(all_f1, ddof=1) / np.sqrt(np.size(all_f1))\n",
    "\n",
    "# print(f'F1 Score:{mean_f1_real_data}+-{std_error_f1_real_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe56b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.max(all_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b363d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ccf87035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# all_f1.sort()\n",
    "# all_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70dc058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(range(len(all_f1)),all_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a2930e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(all_f1,bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b5497e3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:713: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n",
      "Iteration number: 0 of 1\n"
     ]
    }
   ],
   "source": [
    "n_total_iterations = 100\n",
    "n_iterations = 1\n",
    "\n",
    "shuffled_all_f1 = []\n",
    "\n",
    "for n in range(n_total_iterations):\n",
    "    all_df_shuffled = all_df.copy()\n",
    "    try:\n",
    "        all_df_shuffled['sociability'] = np.random.permutation(all_df_shuffled['sociability'].values)\n",
    "        shuffled_mean_df, shuffled_raw_results_df, shuffled_ret_df, shuffled_errors_log, shuffled_models, shuffled_imputers = run_per_param(\n",
    "            current_vars, all_df_shuffled, config_dict=config_dict_best, n_iterations=n_iterations, verbose=False)\n",
    "        f1_val = shuffled_mean_df.loc[mean_df['dataset'] == 'test']['f1'].values\n",
    "        shuffled_all_f1.append(f1_val)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83e7850f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:0.5479600000000001+-0.012580632443656373\n"
     ]
    }
   ],
   "source": [
    "shuffled_all_f1 = np.array(shuffled_all_f1)\n",
    "shuffled_all_f1 = shuffled_all_f1.flatten()\n",
    "\n",
    "mean_f1_shuffled_data = np.mean(shuffled_all_f1)\n",
    "std_error_shuffled_data = np.std(shuffled_all_f1, ddof=1) / np.sqrt(np.size(shuffled_all_f1))\n",
    "\n",
    "print(f'F1 Score:{mean_f1_shuffled_data}+-{std_error_shuffled_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "453cd629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "345e147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_all_f1 = np.array(shuffled_all_f1).flatten()\n",
    "# plt.hist(all_f1,bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b45b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_f1 = all_f1.flatten()\n",
    "# all_f1_shuffled = all_f1_shuffled.flatten()\n",
    "# shuffled_all_f1 = shuffled_all_f1.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "97755f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_all_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dde6edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1 = pd.DataFrame(zip(all_f1, shuffled_all_f1), columns=['Real Data','Randomized Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6073b250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Mean F1 Score')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.violinplot(data=df_f1)\n",
    "g.set_ylabel('Mean F1 Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "real_scores, random_scores = all_f1, shuffled_all_f1\n",
    "\n",
    "# Calculate the p-value using a paired t-test\n",
    "_, p_value = ttest_rel(real_scores, random_scores)\n",
    "\n",
    "# Check if the p-value is below the significance level (e.g., 0.05)\n",
    "significance_level = 0.05\n",
    "if p_value < significance_level:\n",
    "    print(\"The model's performance on real data is significantly better than random.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the model's performance on real data and random.\")\n",
    "\n",
    "# Quantify the improvement using effect size measures (e.g., Cohen's d)\n",
    "mean_real = np.mean(real_scores)\n",
    "mean_random = np.mean(random_scores)\n",
    "std_real = np.std(real_scores, ddof=1)\n",
    "effect_size = (mean_real - mean_random) / std_real\n",
    "\n",
    "print(\"Effect size (Cohen's d):\", effect_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fdfa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = best_raw_rable[['GT','predicted','correct','affiliative_level','filenames','dataset']].copy()\n",
    "\n",
    "df['rat_number'] = df['filenames'].apply(extract_ratnum_from_file_name)\n",
    "df['day_number'] = df['filenames'].apply(extract_daynum_from_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513cbf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Percentage of time each rat was in the test or train datasets\n",
    "rat_dataset_percentage = df.groupby('rat_number')['dataset'].value_counts(normalize=True) * 100\n",
    "print(\"Percentage of time each rat was in the test or train datasets:\")\n",
    "print(rat_dataset_percentage)\n",
    "\n",
    "# Percentage of time each rat was correctly predicted in the test dataset\n",
    "test_correct_percentage = df[(df['dataset'] == 'test') & (df['correct'])].groupby('rat_number')['correct'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage of time each rat was correctly predicted in the test dataset:\")\n",
    "print(test_correct_percentage)\n",
    "\n",
    "# Percentage of time each session was in the test or train datasets\n",
    "session_dataset_percentage = df.groupby('filenames')['dataset'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage of time each session was in the test or train datasets:\")\n",
    "print(session_dataset_percentage)\n",
    "\n",
    "# Percentage of times each session was correctly predicted in the test dataset\n",
    "test_session_correct_percentage = df[(df['dataset'] == 'test') & (df['correct'])].groupby('filenames')['correct'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage of times each session was correctly predicted in the test dataset:\")\n",
    "print(test_session_correct_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the dataframe is already loaded and named 'df'\n",
    "\n",
    "total_count = len(df)  # Total count of samples in the dataset\n",
    "\n",
    "# Percentage of time each session was in the test or train datasets\n",
    "session_dataset_percentage = (df.groupby('filenames')['dataset'].value_counts() / total_count) * 100\n",
    "print(\"\\nPercentage of time each session was in the test or train datasets:\")\n",
    "print(session_dataset_percentage)\n",
    "\n",
    "# Percentage of times each session was correctly predicted in the test dataset\n",
    "test_session_correct_percentage = (df[(df['dataset'] == 'test') & (df['correct'])].groupby('filenames')['correct'].value_counts() / total_count) * 100\n",
    "print(\"\\nPercentage of times each session was correctly predicted in the test dataset:\")\n",
    "print(test_session_correct_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d676a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4d83663",
   "metadata": {},
   "source": [
    "## Feature selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb7094",
   "metadata": {},
   "source": [
    "Feature selection steps:\n",
    "\n",
    "1. Find top 2 candidates\n",
    "2. Train model and get results\n",
    "3. Add next candiate and repeat step 2.\n",
    "4. If the resutls improved, add this featrue, if not, advance to the next candidate\n",
    "\n",
    "Model trainig steps:\n",
    "\n",
    "1. Split data\n",
    "\n",
    "3. Impute missing values\n",
    "    a. MissForest\n",
    "    b. IterativeImputer\n",
    "    \n",
    "4. rescale data (or not)\n",
    "\n",
    "5. Train model/s\n",
    "    a. svm\n",
    "    b. logistic regression\n",
    "    c. random forest classifier\n",
    "    d. knn classifier\n",
    "    \n",
    "6. Evaluate model/s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112191f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_class = MissForest\n",
    "classifier_class = RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([('imputer', imputer_class()), ('scaler', StandardScaler()),('classifier', classifier_class())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3946e1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99014a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ee48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_test['dataset'] = 'test'\n",
    "df_results_train['dataset'] = 'train'\n",
    "df_results_all = pd.concat([df_results_train,df_results_test])\n",
    "df_results_all['rat_num'] = df_results_all['files'].apply(extract_ratnum_from_file_name)\n",
    "df_results_all['day_num'] = df_results_all['files'].apply(extract_day_from_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f406ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=df_results_all, x='day_num', y='affiliative_level', hue='GT', col='dataset')\n",
    "sns.lmplot(data=df_results_all, x='day_num', y='affiliative_level', hue='predicted', col='dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e57721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bdc3f2a",
   "metadata": {},
   "source": [
    "## Dimensionality reduction using t-SNE and visualization|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccccb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create the figure and axes\n",
    "# fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# # add the plots for each dataframe\n",
    "# df_rats = df_results_all.groupby('rat_num')\n",
    "# for ratnum , df_r in df_rats:\n",
    "#     sns.regplot(x='day_num', y='affiliative_level', data=df_r, fit_reg=True, ci=None, ax=ax, label=ratnum)\n",
    "# ax.set(ylabel='affiliative_level', xlabel='rat_num')\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67650f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = df_train.drop(['sociability','dataset'], axis = 1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e1c54d",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f9caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pairplot(title, X,y,y_pred):\n",
    "    df_X = pd.DataFrame(X)\n",
    "\n",
    "\n",
    "    df_X = df_X.rename(columns={num:cname for num, cname in enumerate(col_names)})\n",
    "    df_X['GT'] = y\n",
    "    df_X['predicted'] = y_pred\n",
    "    df_X['correct'] = df_X['predicted'] == df_X['GT']\n",
    "\n",
    "    g = sns.pairplot(data=df_X.drop(['predicted','correct'], axis=1), hue='GT')\n",
    "    g.fig.suptitle(f\"{title} Data GT sociability\", y=1.05)\n",
    "\n",
    "    g = sns.pairplot(data=df_X.drop(['GT','correct'], axis=1), hue='predicted')\n",
    "    g.fig.suptitle(f\"{title} Data GT predicted\", y=1.05)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1123aba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X = X_test\n",
    "y = y_test.values\n",
    "y_pred = y_test_pred\n",
    "title = 'Test - Unimputed'\n",
    "plot_pairplot(title, X,y,y_pred)\n",
    "\n",
    "\n",
    "X = X_train\n",
    "y = y_train.values\n",
    "y_pred = y_train_pred\n",
    "title = 'Train - Unimputed'\n",
    "plot_pairplot(title, X,y,y_pred)\n",
    "\n",
    "X = X_test_imp\n",
    "y = y_test.values\n",
    "y_pred = y_test_pred\n",
    "title = 'Test - Imputed'\n",
    "plot_pairplot(title, X,y,y_pred)\n",
    "\n",
    "X = X_train_imp\n",
    "y = y_train.values\n",
    "y_pred = y_train_pred\n",
    "title = 'Train - Imputed'\n",
    "plot_pairplot(title, X,y,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebada199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b3b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e679a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b52ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test_imp\n",
    "\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=3).fit_transform(X)\n",
    "\n",
    "df_results_test[['tsne1','tsne2']] = X_embedded\n",
    "sns.pairplot(df_results_test[['GT','tsne1','tsne2']], hue='GT')\n",
    "sns.pairplot(df_results_test[['predicted','tsne1','tsne2']], hue='predicted')\n",
    "\n",
    "\n",
    "X = X_train_imp\n",
    "\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=3).fit_transform(X)\n",
    "\n",
    "df_results_train[['tsne1','tsne2']] = X_embedded\n",
    "sns.pairplot(df_results_train[['GT','tsne1','tsne2']], hue='GT')\n",
    "sns.pairplot(df_results_train[['predicted','tsne1','tsne2']], hue='predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4768b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = X_test_imp\n",
    "\n",
    "X_embedded = TSNE(n_components=1, learning_rate='auto',\n",
    "                  init='random', perplexity=2).fit_transform(X)\n",
    "\n",
    "# df_results_test[['tsne1','tsne2']] = X_embedded\n",
    "# sns.pairplot(df_results_test[['GT','tsne1','tsne2']], hue='GT')\n",
    "# sns.pairplot(df_results_test[['predicted','tsne1','tsne2']], hue='predicted')\n",
    "\n",
    "df_results_test[['tsne1']] = X_embedded\n",
    "df_results_test['affiliative_level_logit'] = df_results_test['affiliative_level'].apply(logit)\n",
    "sns.lmplot(data=df_results_test, x = 'affiliative_level_logit' ,y = 'tsne1', hue='GT')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eedda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_imp\n",
    "\n",
    "X_embedded = TSNE(n_components=1, learning_rate='auto',\n",
    "                  init='random', perplexity=1).fit_transform(X)\n",
    "\n",
    "# df_results_test[['tsne1','tsne2']] = X_embedded\n",
    "# sns.pairplot(df_results_test[['GT','tsne1','tsne2']], hue='GT')\n",
    "# sns.pairplot(df_results_test[['predicted','tsne1','tsne2']], hue='predicted')\n",
    "\n",
    "df_results_train[['tsne1']] = X_embedded\n",
    "df_results_train['affiliative_level_logit'] = df_results_train['affiliative_level'].apply(logit)\n",
    "sns.(data=df_results_train, x = 'affiliative_level_logit' ,y = 'tsne1', hue='GT')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_imp\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=3).fit_transform(X)\n",
    "\n",
    "df_results_train[['tsne1','tsne2']] = X_embedded\n",
    "\n",
    "sns.pairplot(df_results_train[['GT','tsne1','tsne2']], hue='GT')\n",
    "\n",
    "sns.pairplot(df_results_train[['predicted','tsne1','tsne2']], hue='predicted')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc5eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data (train and test)\n",
    "\n",
    "\n",
    "X = np.append(X_train_imp, X_test_imp, axis=0)\n",
    "y = np.append(y_train, y_test)\n",
    "y_pred = np.append(y_train_pred, y_test_pred)\n",
    "\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=10, n_iter=5000).fit_transform(X)\n",
    "\n",
    "\n",
    "df_res_tsne = pd.DataFrame(y, columns=['GT'])\n",
    "df_res_tsne['dataset'] = np.append(np.full(y_train.shape, \"train\"), np.full(y_test.shape, \"test\"))\n",
    "df_res_tsne['predicted'] = y_pred\n",
    "df_res_tsne['correct'] = df_res_tsne['predicted'] == df_res_tsne['GT']\n",
    "df_res_tsne[['tsne1','tsne2']] = X_embedded\n",
    "#Seaborn pair plot\n",
    "\n",
    "sns.pairplot(df_res_tsne[['GT','tsne1','tsne2']], hue='GT')\n",
    "\n",
    "sns.pairplot(df_res_tsne[['predicted','tsne1','tsne2']], hue='predicted')\n",
    "\n",
    "# # Display the plots\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_tsne.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3faf52a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d4ad369",
   "metadata": {},
   "source": [
    "### Impute the missing values of all the dataset before training and testing\n",
    "\n",
    "Performing imputation before splitting the dataset can potentially lead to data leakage and overly optimistic evaluation results. It's generally recommended to split the dataset into training and testing sets before applying any data preprocessing steps, including imputation.\n",
    "\n",
    "Data leakage can occur when information from the testing set is inadvertently used during the imputation process. This can lead to overfitting and unrealistic evaluation results because the imputation is informed by the target variable in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e0e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer = IterativeImputer(max_iter=100)\n",
    "imputer_class = MissForest\n",
    "\n",
    "# model_class = RandomForestClassifier\n",
    "# model = LogisticRegression(max_iter=10000)\n",
    "# model_class = svm.SVC\n",
    "model_class = tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9783059",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[:,:3], y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7f830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute data before splitting it into train and test\n",
    "\n",
    "imputer = imputer_class()\n",
    "model = model_class()\n",
    "\n",
    "imputed_data = imputer.fit_transform(X_train)\n",
    "\n",
    "# # Splitting the imputed data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(imputed_data,\n",
    "#                                                     labels,\n",
    "#                                                     test_size=0.3,\n",
    "#                                                     stratify=labels)\n",
    "\n",
    "\n",
    "# Creating and training the model (using Logistic Regression as an example)\n",
    "\n",
    "\n",
    "model.fit(imputed_data, y_train)\n",
    "\n",
    "\n",
    "# Making predictions on the training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Calculating accuracy scores\n",
    "# train_accuracy = accuracy_score(y_train.values, y_train_pred)\n",
    "# test_accuracy = accuracy_score(y_test.values, y_test_pred)\n",
    "ret_test = eval_model(y_test, y_test_pred)\n",
    "ret_train = eval_model(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "\n",
    "df_results_test = pd.DataFrame(y_test, columns={'GT'})\n",
    "df_results_test['predicted'] = y_test_pred\n",
    "df_results_test['correct'] = df_results_test['GT'] == df_results_test['predicted']\n",
    "print(df_results_test)\n",
    "\n",
    "\n",
    "print('Test')\n",
    "[print(f'{metric}:{values}') for metric, values in ret_test.items()]\n",
    "print('Train')\n",
    "[print(f'{metric}:{values}') for metric, values in ret_train.items()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae68078",
   "metadata": {},
   "source": [
    "### Imputing the data after each splitting\n",
    "\n",
    "The imputaion quality drops when performing on smaller sub sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a9dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600cd62d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Splitting the imputed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,\n",
    "                                                    labels,\n",
    "                                                    test_size=0.3,\n",
    "                                                    stratify=labels)\n",
    "\n",
    "imputer = imputer_class()\n",
    "model = model_class()\n",
    "\n",
    "imputed_data_train = imputer.fit_transform(X_train)\n",
    "imputed_data_test = imputer.fit_transform(X_test)\n",
    "\n",
    "# Converting the imputed data back to a DataFrame\n",
    "imputed_data_train = pd.DataFrame(imputed_data_train, columns=X_train.columns)\n",
    "imputed_data_test = pd.DataFrame(imputed_data_test, columns=X_test.columns)\n",
    "\n",
    "\n",
    "\n",
    "# Creating and training the model (using Logistic Regression as an example)\n",
    "# model = LogisticRegression()\n",
    "model.fit(imputed_data_train, y_train)\n",
    "\n",
    "# Making predictions on the training and testing sets\n",
    "y_train_pred = model.predict(imputed_data_train)\n",
    "y_test_pred = model.predict(imputed_data_test)\n",
    "\n",
    "# Calculating accuracy scores\n",
    "# train_accuracy = accuracy_score(y_train.values, y_train_pred)\n",
    "# test_accuracy = accuracy_score(y_test.values, y_test_pred)\n",
    "ret_test = eval_model(y_test, y_test_pred)\n",
    "ret_train = eval_model(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "df_results_test = pd.DataFrame(y_test, columns={'GT'})\n",
    "df_results_test['predicted'] = y_test_pred\n",
    "df_results_test['correct'] = df_results_test['GT'] == df_results_test['predicted']\n",
    "print(df_results_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Test')\n",
    "[print(f'{metric}:{values}') for metric, values in ret_test.items()]\n",
    "print('Train')\n",
    "[print(f'{metric}:{values}') for metric, values in ret_train.items()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835e3cef",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "Since the data plitting have significant effect on the model performance, cross validation is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_fold(X_train, X_test, y_train, y_test):\n",
    "        # Splitting the imputed data into training and testing sets\n",
    "\n",
    "\n",
    "    imputer = imputer_class(max_iter=500)\n",
    "    model = model_class()\n",
    "    \n",
    "    imputed_data_train = imputer.fit_transform(X_train)\n",
    "    imputed_data_test = imputer.fit_transform(X_test)\n",
    "\n",
    "    # Converting the imputed data back to a DataFrame\n",
    "#     imputed_data_train = pd.DataFrame(imputed_data_train, columns=X_train.columns)\n",
    "#     imputed_data_test = pd.DataFrame(imputed_data_test, columns=X_test.columns)\n",
    "    \n",
    "    # debug\n",
    "#     print(imputed_data_test.shape)\n",
    "#     print(imputed_data_test.shape)\n",
    "\n",
    "\n",
    "    # Creating and training the model (using Logistic Regression as an example)\n",
    "    \n",
    "    model.fit(imputed_data_train, y_train)\n",
    "\n",
    "    # Making predictions on the training and testing sets\n",
    "    y_train_pred = model.predict(imputed_data_train)\n",
    "    y_test_pred = model.predict(imputed_data_test)\n",
    "    \n",
    "    return y_train, y_train_pred, y_test, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e56b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c3c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cv = KFold(n_splits=3, shuffle=True)\n",
    "# cv = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "\n",
    "cv_scores_train = []\n",
    "cv_scores_test = []\n",
    "# for train_index, test_index in cv.split(data, rat_numbers):\n",
    "for train_index, test_index in cv.split(data):\n",
    "    X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    y_train, y_train_pred, y_test, y_test_pred = one_fold(X_train, X_test, y_train, y_test)\n",
    "    scores_train = eval_model(y_train, y_train_pred)\n",
    "    scores_test = eval_model(y_test, y_test_pred)\n",
    "    cv_scores_train.append(scores_train)\n",
    "    cv_scores_test.append(scores_test)\n",
    "# Summarize\n",
    "ret_train = pd.DataFrame(cv_scores_train).mean()\n",
    "ret_test = pd.DataFrame(cv_scores_test).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b013f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test')\n",
    "print(ret_test)\n",
    "\n",
    "print('Train')\n",
    "print(ret_train)\n",
    "\n",
    "# print('Test')\n",
    "# [[print(f'{metric}:{values}') for metric, values in ret.items()] for ret in cv_scores_test]\n",
    "# print('Train')\n",
    "# [[print(f'{metric}:{values}') for metric, values in ret.items()] for ret in cv_scores_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c461ad",
   "metadata": {},
   "outputs": [],
   "source": [
    " pd.DataFrame(cv_scores_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553787b7",
   "metadata": {},
   "source": [
    "### Impute the data and see how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe0ee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imputer = imputer_class(1000)\n",
    "imputed_data = imputer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8314c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_num = np.array(labels=='affiliative', dtype=float)*0.1\n",
    "labels_num = labels_num[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_conc = np.concatenate((data.values, labels_num), axis=1)\n",
    "plt.imshow(data_conc, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e4900",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data_concat = np.concatenate((imputed_data, labels_num), axis=1)\n",
    "plt.imshow(imputed_data_concat, cmap='hot', interpolation='nearest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996fe44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X = imputed_data\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=15).fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7b900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "strings = filenames\n",
    "label_encoder = LabelEncoder()\n",
    "converted_numbers = label_encoder.fit_transform(strings)\n",
    "\n",
    "strings = filenames\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "converted_numbers\n",
    "\n",
    "\n",
    "rat_numbers = [extract_ratnum_from_file_name(filename) for filename in filenames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4c44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(X_embedded)\n",
    "\n",
    "df_tsne['labels'] = labels\n",
    "df_tsne['filenames'] = converted_numbers\n",
    "df_tsne['rat_number'] = rat_numbers\n",
    "# df_tsne['pred'] = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72180729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dummies and store it in a variable\n",
    "dummies = pd.get_dummies(df_tsne.labels)\n",
    " \n",
    "# Concatenate the dummies to original dataframe\n",
    "merged = pd.concat([df_tsne, dummies], axis='columns')\n",
    " \n",
    "# drop the values\n",
    "merged.drop(['labels'], axis='columns')\n",
    "\n",
    "merged = merged[['rat_number','affiliative','aversive']].groupby('rat_number').sum()\n",
    "merged['sum'] = merged['affiliative'] + merged['aversive']\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c70675",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_tsne, x=0, y=1, hue='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5013d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sns.catplot(data=df_tsne, x=0, y=1, hue='rat_number', row='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d216b27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sns.catplot(data=df_tsne, x=0, y=1, hue='filenames', row='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac03970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548ec31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "all_pos = {}\n",
    "colors = []\n",
    "for r, df_r in df_tsne.groupby('rat_number'):\n",
    "\n",
    "\n",
    "    f_names = df_r['filenames'].values\n",
    "    pos_ = df_r[[0,1]].values\n",
    "    pos_2 = {(r,f): p for p, f in zip(pos_,f_names)}\n",
    "    mean_pos = np.mean(pos_,axis=0)\n",
    "    pos_2[r] = mean_pos\n",
    "    \n",
    "    labels_sub = np.array(['b']* len(df_r))\n",
    "    labels_sub[np.where(np.array(df_r['labels']=='affiliative'))] = 'g'\n",
    "    \n",
    "    all_pos.update(pos_2)\n",
    "    [G.add_edge(r, (r,f)) for f in f_names]\n",
    "    colors = np.append(colors, labels_sub)\n",
    "# colors = np.array(colors).flatten()\n",
    "fig = plt.figure(figsize=(40,80))\n",
    "# nx.draw_networkx(G,all_pos)\n",
    "nx.draw_networkx(G,all_pos, edge_color=colors, font_size=50,width=5)\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d63fbef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# G = nx.Graph()\n",
    "# all_pos = {}\n",
    "# df_g = df_tsne.groupby('rat_number')\n",
    "# n_groups = len(df_g)\n",
    "# fig = plt.figure(figsize=(40,80))\n",
    "# for n, (r, df_r) in enumerate(df_g):\n",
    "#     G = nx.Graph()\n",
    "#     plt.subplot(n_groups,1,n+1)\n",
    "\n",
    "#     f_names = df_r['filenames'].values\n",
    "#     pos_ = df_r[[0,1]].values\n",
    "#     labels_sub = np.array(['b']* len(df_r))\n",
    "#     labels_sub[np.where(np.array(df_r['labels']=='affiliative'))] = 'g'\n",
    "# #     labels_sub = np.append(labels_sub, 'y', axis=None)\n",
    "    \n",
    "    \n",
    "#     pos_2 = {(r,f): p for p, f in zip(pos_,f_names)}\n",
    "#     mean_pos = np.mean(pos_,axis=0)\n",
    "    \n",
    "#     pos_2[r] = mean_pos\n",
    "    \n",
    "#     all_pos.update(pos_2)\n",
    "#     [G.add_edge(r, (r,f)) for f in f_names]\n",
    "    \n",
    "\n",
    "#     nx.draw_networkx(G,pos_2, node_size=2000, font_size=50, edge_color=labels_sub)\n",
    "#     ax = plt.gca()\n",
    "#     ax.margins(0.08)\n",
    "# #     plt.axis(\"off\")\n",
    "#     plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
